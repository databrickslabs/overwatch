<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Cloud Infrastructure on Overwatch</title>
    <link>https://databrickslabs.github.io/overwatch/deployoverwatch/cloudinfra/</link>
    <description>Recent content in Cloud Infrastructure on Overwatch</description>
    <generator>Hugo</generator>
    <language>en</language>
    <lastBuildDate>Thu, 07 Sep 2023 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://databrickslabs.github.io/overwatch/deployoverwatch/cloudinfra/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>GCP</title>
      <link>https://databrickslabs.github.io/overwatch/deployoverwatch/cloudinfra/gcp/</link>
      <pubDate>Thu, 07 Sep 2023 00:00:00 +0000</pubDate>
      <guid>https://databrickslabs.github.io/overwatch/deployoverwatch/cloudinfra/gcp/</guid>
      <description>&lt;h2 id=&#34;configuring-overwatch-on-gcp---databricks&#34;&gt;Configuring Overwatch on GCP - Databricks&lt;/h2&gt;&#xA;&lt;p&gt;Reach out to your Databricks representative to help you with these tasks as needed.&lt;/p&gt;&#xA;&lt;p&gt;There are two primary sources of data that need to be configured:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://docs.gcp.databricks.com/administration-guide/account-settings/audit-logs.html&#34;&gt;Audit Logs-GCP&lt;/a&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;These will be delivered to the configured bucket. These buckets are configured on a per-workspace basis&#xA;and can be delivered to the same target bucket, just ensure that the prefixes are different to avoid collisions.&#xA;We don&amp;rsquo;t want multiple workspaces delivering into the same prefix. The audit logs contain data for every interaction&#xA;within the environment and are used to track the state of various objects through time along with which accounts&#xA;interacted with them. This data is relatively small and delivery occurs infrequently which is why it&amp;rsquo;s&#xA;rarely of any consequence to deliver audit logs to buckets even outside of the control plane region.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Cluster Logs - Crucial to get the most out of Overwatch&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Cluster logs delivery location is configured in the cluster spec &amp;ndash;&amp;gt; Advanced Options &amp;ndash;&amp;gt; Logging. These logs can&#xA;get quite large and they are stored in a very inefficient format for query and long-term storage. As such, it&amp;rsquo;s&#xA;crucial to store these logs in the same region as the worker nodes for best results. Additionally, using dedicated&#xA;buckets provides more flexibility when configuring TTL (time-to-live) to minimize long-term, unnecessary costs.&#xA;It&amp;rsquo;s not recommended to store these on DBFS directly (dbfs mount points are ok).&lt;/li&gt;&#xA;&lt;li&gt;Best Practice - Multi-Workspace &amp;ndash; When multiple workspaces are using Overwatch within a single region it&amp;rsquo;s best to&#xA;ensure that each are going to their own prefix, even if sharing a bucket. This greatly reduces Overwatch scan times&#xA;as the log files build up.&lt;/li&gt;&#xA;&lt;li&gt;To enable Cluster Logs on Multiworkspace - follow this &lt;a href=&#34;https://databrickslabs.github.io/overwatch/deployoverwatch/ucedeployment/cloudstorageaccessrequirements/gcp//#cluster-logging-locations-setup&#34;&gt;link&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;div class=&#34;notices note&#34; &gt;&lt;p&gt;&lt;strong&gt;GCP &amp;ndash; Remote Cluster Logs&lt;/strong&gt; - Databricks on GCP, does not support mounted/GCS bucket locations. Customers must&#xA;provide DBFS root path as a target for log delivery.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Azure</title>
      <link>https://databrickslabs.github.io/overwatch/deployoverwatch/cloudinfra/azure/</link>
      <pubDate>Mon, 12 Dec 2022 11:29:59 -0500</pubDate>
      <guid>https://databrickslabs.github.io/overwatch/deployoverwatch/cloudinfra/azure/</guid>
      <description>&lt;h2 id=&#34;fast-travel&#34;&gt;Fast Travel&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#configuring-overwatch-on-azure-databricks&#34;&gt;Configuring Overwatch on Azure Databricks&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#reference-architecture&#34;&gt;Reference Architecture&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#audit-log-delivery-via-event-hub&#34;&gt;Configuring Audit Log Delivery Through Event Hub&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#setting-up-storage-accounts&#34;&gt;Setting up Storage Accounts&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://docs.databricks.com/data/data-sources/azure/adls-gen2/azure-datalake-gen2-sp-access.html&#34;&gt;Mount Storage Accounts&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;configuring-overwatch-on-azure-databricks&#34;&gt;Configuring Overwatch on Azure Databricks&lt;/h2&gt;&#xA;&lt;p&gt;Reach out to your Databricks representative to help you with these tasks as needed.&#xA;&lt;!-- raw HTML omitted --&gt;&#xA;To get started, we suggest you deploy a single workspace end to end so that you can figure out the steps involved and you&#xA;can then apply these for the other workspaces to be deployed.&lt;/p&gt;</description>
    </item>
    <item>
      <title>AWS</title>
      <link>https://databrickslabs.github.io/overwatch/deployoverwatch/cloudinfra/aws/</link>
      <pubDate>Mon, 12 Dec 2022 11:29:56 -0500</pubDate>
      <guid>https://databrickslabs.github.io/overwatch/deployoverwatch/cloudinfra/aws/</guid>
      <description>&lt;h2 id=&#34;configuring-overwatch-on-aws---databricks&#34;&gt;Configuring Overwatch on AWS - Databricks&lt;/h2&gt;&#xA;&lt;p&gt;Reach out to your Customer Success Engineer (CSE) to help you with these tasks as needed.&lt;/p&gt;&#xA;&lt;p&gt;There are two primary sources of data that need to be configured:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://docs.databricks.com/administration-guide/account-settings/audit-logs.html&#34;&gt;Audit Logs-AWS&lt;/a&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;The audit logs contain data for every interaction&#xA;within the environment and are used to track the state of various objects through time along with which accounts&#xA;interacted with them. This data is relatively small and delivery occurs infrequently which is why it&amp;rsquo;s&#xA;rarely of any consequence to deliver audit logs to buckets even outside of the control plane region.&lt;/li&gt;&#xA;&lt;li&gt;For ingesting this data, you have the option of using System tables (&lt;strong&gt;RECOMMENDED&lt;/strong&gt;) or you can configure the audit&#xA;logs to be delivered to the configured bucket. These buckets are configured on a per-workspace basis and can be&#xA;delivered to the same target bucket, just ensure that the prefixes are different to avoid collisions.&#xA;We don&amp;rsquo;t want multiple workspaces delivering into the same prefix.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Cluster Logs - Crucial to get the most out of Overwatch&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Cluster logs delivery location is configured in the cluster spec &amp;ndash;&amp;gt; Advanced Options &amp;ndash;&amp;gt; Logging. These logs can&#xA;get quite large and they are stored in a very inefficient format for query and long-term storage. As such, it&amp;rsquo;s&#xA;&lt;strong&gt;crucial&lt;/strong&gt; to store these logs in the same region as the worker nodes for best results. Additionally, using dedicated&#xA;buckets provides more flexibility when configuring TTL (time-to-live) to minimize long-term, unnecessary costs.&#xA;It&amp;rsquo;s not recommended to store these on DBFS directly.&lt;/li&gt;&#xA;&lt;li&gt;Best Practice - Multi-Workspace &amp;ndash; When multiple workspaces are using Overwatch within a single region it&amp;rsquo;s best to&#xA;ensure that each are going to their own prefix, even if sharing a storage account. This greatly reduces Overwatch scan times&#xA;as the log files build up. If scan times get too long, the TTL can be reduced or additional storage accounts can&#xA;be created to increase read IOPS throughput (rarely necessary) intra-region.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;reference-architecture&#34;&gt;Reference Architecture&lt;/h2&gt;&#xA;&lt;p&gt;As of 0.7.1 Overwatch can be deployed on a single workspace and retrieve data from one or more workspaces. For more details&#xA;on requirements see &lt;a href=&#34;https://databrickslabs.github.io/overwatch/deployoverwatch//#multi-workspace-monitoring---considerations&#34;&gt;Multi-Workspace Consideration&lt;/a&gt;.&#xA;There are many cases where some workspaces should be able to monitor many workspaces and others should only monitor&#xA;themselves. Additionally, co-location of the output data and who should be able to access what data also comes into play,&#xA;this reference architecture can accommodate all of these needs. To learn more about the details walk through the&#xA;&lt;a href=&#34;https://databrickslabs.github.io/overwatch/deployoverwatch/&#34;&gt;deployment steps&lt;/a&gt;&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
