<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Welcome To OverWatch on Overwatch</title>
    <link>https://databrickslabs.github.io/overwatch/</link>
    <description>Recent content in Welcome To OverWatch on Overwatch</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Thu, 20 May 2021 21:27:44 -0400</lastBuildDate><atom:link href="https://databrickslabs.github.io/overwatch/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Definitions</title>
      <link>https://databrickslabs.github.io/overwatch/dataengineer/definitions/</link>
      <pubDate>Mon, 11 Jan 2021 12:21:19 -0500</pubDate>
      
      <guid>https://databrickslabs.github.io/overwatch/dataengineer/definitions/</guid>
      <description>Consumption Layer  Column Descriptions   ETL Tables  Bronze Silver Gold    Consumption Layer &amp;ldquo;Tables&amp;rdquo; (Views) All end users should be hitting consumer tables first. Digging into lower layers gets significantly more complex. Below is the data model for the consumption layer. The consumption layer is often in a stand-alone database apart from the ETL tables to minimize clutter and confusion. These entities in this layer are actually not tables at all (with a few minor exceptions such as lookup tables) but rather views.</description>
    </item>
    
    <item>
      <title>ETL Process</title>
      <link>https://databrickslabs.github.io/overwatch/dataengineer/etl_process/</link>
      <pubDate>Mon, 11 Jan 2021 12:21:46 -0500</pubDate>
      
      <guid>https://databrickslabs.github.io/overwatch/dataengineer/etl_process/</guid>
      <description>Data Ingestion and Resume Process The specificities can vary slightly between cloud provider but the general methodology is the exact same. Specific differences will be discussed in Cloud-Specific Variations section.
Each module is responsible for building certain entities at each layer, bronze, silver, gold, and presentation. The mapping between module and gold entity can be found in the Modules section. Each module is also tracked individually in the primary tracking tabe, Pipeline_Report.</description>
    </item>
    
    <item>
      <title>Configuration</title>
      <link>https://databrickslabs.github.io/overwatch/gettingstarted/configuration/</link>
      <pubDate>Wed, 28 Oct 2020 11:55:12 -0400</pubDate>
      
      <guid>https://databrickslabs.github.io/overwatch/gettingstarted/configuration/</guid>
      <description>Configuration Basics The Overwatch configuration can be created as a case class of OverwatchParams or as a json string passed into the main class com.databricks.labs.overwatch.BatchRunner. When passed in as a json string, it is serialized into an instance of OverwatchParams. This provides strong validation on the input parameters and strong typing for additional validation options. All configs attempt to be defaulted to a reasonable default where possible. If a default is set, passing in a value will overwrite the default.</description>
    </item>
    
    <item>
      <title>Upgrade</title>
      <link>https://databrickslabs.github.io/overwatch/dataengineer/upgrade/</link>
      <pubDate>Thu, 20 May 2021 21:27:44 -0400</pubDate>
      
      <guid>https://databrickslabs.github.io/overwatch/dataengineer/upgrade/</guid>
      <description>Sometimes upgrading from one version to the next requires a schema change. In these cases, the CHANGELOG will be explicit. Upgrades MUST be executed WITH the new binary and before the pipeline is executed. The general upgrade process is:
 Adjust any Overwatch configurations and get the updated compactString from JsonUtils Import the upgrade package Use the compactString of parameters to instantiate the workspace Call the upgrade function for the version to which you&amp;rsquo;re upgrading.</description>
    </item>
    
    <item>
      <title>Modules</title>
      <link>https://databrickslabs.github.io/overwatch/gettingstarted/modules/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://databrickslabs.github.io/overwatch/gettingstarted/modules/</guid>
      <description>Modules Modules are the method by which Overwatch is segmented; currently, the modules available include:
 audit clusters clusterEvents pools jobs accounts notebooks sparkEvents  The default is to use all modules so if none are specified in the configuration, all modules will be enabled. Currently, under normal, daily operations, there no significant cost to any of these modules. It&amp;rsquo;s likely best to leave them all turned on unless there&amp;rsquo;s a specific reason not to.</description>
    </item>
    
    <item>
      <title>Advanced Topics</title>
      <link>https://databrickslabs.github.io/overwatch/gettingstarted/advancedtopics/</link>
      <pubDate>Wed, 28 Oct 2020 13:43:52 -0400</pubDate>
      
      <guid>https://databrickslabs.github.io/overwatch/gettingstarted/advancedtopics/</guid>
      <description>Quick Reference  Interacting With Overwatch and its State Joining With Slow Changing Dimensions (SCD)  Interacting With Overwatch and its State Use your production notebook (or equivallent) to instantiate your Overatch Configs. From there use JsonUtils.compactString to get the condensed parameters for your workspace.
NOTE you cannot use the escaped string, it needs to be the compactString.
Below is an example of getting the compact string. You may also refer to the example runner notebook for your cloud provider.</description>
    </item>
    
    <item>
      <title>Azure</title>
      <link>https://databrickslabs.github.io/overwatch/environmentsetup/azure/</link>
      <pubDate>Wed, 28 Oct 2020 09:12:32 -0400</pubDate>
      
      <guid>https://databrickslabs.github.io/overwatch/environmentsetup/azure/</guid>
      <description>Configuring Overwatch on Azure Databricks Reference Architecture Configuring Audit Log Delivery Through Event Hub Setting up Storage Accounts Mount Storage Accounts  Configuring Overwatch on Azure Databricks Reach out to your Customer Success Engineer (CSE) to help you with these tasks as needed. To get started, the Basic Deployment configuration. As more modules are enabled, additional environment configuration may be required in addition to the Basic Deployment.
There are two primary sources of data that need to be configured:</description>
    </item>
    
    <item>
      <title>AWS</title>
      <link>https://databrickslabs.github.io/overwatch/environmentsetup/aws/</link>
      <pubDate>Wed, 28 Oct 2020 09:12:28 -0400</pubDate>
      
      <guid>https://databrickslabs.github.io/overwatch/environmentsetup/aws/</guid>
      <description>Configuring Overwatch on AWS - Databricks Reach out to your Customer Success Engineer (CSE) to help you with these tasks as needed. To get started, the Basic Deployment configuration. As more modules are enabled, additional environment configuration may be required in addition to the Basic Deployment.
There are two primary sources of data that need to be configured:
 Audit Logs  These will be delivered to the configured bucket. These buckets are configured on a per-workspace basis and can be delivered to the same target bucket, just ensure that the prefixes are different to avoid collisions.</description>
    </item>
    
  </channel>
</rss>
