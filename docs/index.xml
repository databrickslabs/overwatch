<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Welcome To OverWatch on Overwatch</title>
    <link>https://databrickslabs.github.io/overwatch/</link>
    <description>Recent content in Welcome To OverWatch on Overwatch</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Tue, 18 Apr 2023 11:28:39 -0500</lastBuildDate><atom:link href="https://databrickslabs.github.io/overwatch/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>AWS</title>
      <link>https://databrickslabs.github.io/overwatch/deployoverwatch/ucedeployment/cloudstorageaccessrequirements/aws/</link>
      <pubDate>Tue, 18 Apr 2023 11:28:39 -0500</pubDate>
      
      <guid>https://databrickslabs.github.io/overwatch/deployoverwatch/ucedeployment/cloudstorageaccessrequirements/aws/</guid>
      <description>Under Construction &amp;ndash; we will be improving these docs shortly
AWS IAM/Policy required to set up for Storage Credentials {&amp;#34;Version&amp;#34;: &amp;#34;2012-10-17&amp;#34;,&amp;#34;Statement&amp;#34;: [{&amp;#34;Action&amp;#34;: [&amp;#34;s3:GetObject&amp;#34;,&amp;#34;s3:GetObjectVersion&amp;#34;,&amp;#34;s3:PutObject&amp;#34;,&amp;#34;s3:PutObjectAcl&amp;#34;,&amp;#34;s3:DeleteObject&amp;#34;,&amp;#34;s3:ListBucket&amp;#34;,&amp;#34;s3:GetBucketLocation&amp;#34;,&amp;#34;s3:GetLifecycleConfiguration&amp;#34;,&amp;#34;s3:PutLifecycleConfiguration&amp;#34;],&amp;#34;Resource&amp;#34;: [&amp;#34;arn:aws:s3:::&amp;lt;BUCKET-NAME&amp;gt;/*&amp;#34;,&amp;#34;arn:aws:s3:::&amp;lt; BUCKET-NAME&amp;gt; &amp;#34;],&amp;#34;Effect&amp;#34;: &amp;#34;Allow&amp;#34;},{&amp;#34;Action&amp;#34;: [&amp;#34;kms:Decrypt&amp;#34;,&amp;#34;kms:Encrypt&amp;#34;,&amp;#34;kms:GenerateDataKey*&amp;#34;],&amp;#34;Resource&amp;#34;: [&amp;#34;arn:aws:kms:&amp;lt;KMS_KEY&amp;gt;&amp;#34;],&amp;#34;Effect&amp;#34;: &amp;#34;Allow&amp;#34;},{&amp;#34;Action&amp;#34;: [&amp;#34;sts:AssumeRole&amp;#34;],&amp;#34;Resource&amp;#34;: [&amp;#34;arn:aws:iam::&amp;lt;AWS-ACCOUNT-ID&amp;gt;:role/&amp;lt;THIS-IAM-ROLE&amp;gt;&amp;#34;],&amp;#34;Effect&amp;#34;: &amp;#34;Allow&amp;#34;}]} Trust Relation {&amp;#34;Version&amp;#34;: &amp;#34;2012-10-17&amp;#34;,&amp;#34;Statement&amp;#34;: [{&amp;#34;Effect&amp;#34;: &amp;#34;Allow&amp;#34;,&amp;#34;Principal&amp;#34;: {&amp;#34;AWS&amp;#34;: &amp;#34;arn:aws:iam::414351767826:role/unity-catalog-prod-UCMasterRole-14S5ZJVKOTYTL&amp;#34; //DO NOT CHANGE},&amp;#34;Action&amp;#34;: &amp;#34;sts:AssumeRole&amp;#34;,&amp;#34;Condition&amp;#34;: {&amp;#34;StringEquals&amp;#34;: {&amp;#34;sts:ExternalId&amp;#34;: &amp;#34;e6e8162c-a42f-43a0-af86-312058795a14&amp;#34;}}}]} Instance Profile - IAM Role / Policy {&amp;#34;Version&amp;#34;: &amp;#34;2012-10-17&amp;#34;,&amp;#34;Statement&amp;#34;: [{&amp;#34;Sid&amp;#34;: &amp;#34;PermitSelectedBucketsList&amp;#34;,&amp;#34;Effect&amp;#34;: &amp;#34;Allow&amp;#34;,&amp;#34;Action&amp;#34;: [&amp;#34;s3:ListBucket&amp;#34;,&amp;#34;s3:GetObject&amp;#34;,&amp;#34;s3:PutObject&amp;#34;,&amp;#34;s3:GetObject&amp;#34;,&amp;#34;s3:DeleteObject&amp;#34;,&amp;#34;s3:PutObjectAcl&amp;#34;,&amp;#34;s3:GetBucketNotification&amp;#34;,&amp;#34;s3:PutBucketNotification&amp;#34;],&amp;#34;Resource&amp;#34;: [&amp;#34;arn:aws:s3:::&amp;lt;BUCKET-NAME&amp;gt;/*&amp;#34;,&amp;#34;arn:aws:s3:::&amp;lt; BUCKET-NAME&amp;gt; &amp;#34;]},{&amp;#34;Action&amp;#34;: [&amp;#34;kms:Decrypt&amp;#34;,&amp;#34;kms:Encrypt&amp;#34;,&amp;#34;kms:GenerateDataKey*&amp;#34;],&amp;#34;Resource&amp;#34;: [&amp;#34;arn:aws:kms:&amp;lt;KMS_KEY&amp;gt;&amp;#34;],&amp;#34;Effect&amp;#34;: &amp;#34;Allow&amp;#34;},{&amp;#34;Sid&amp;#34;: &amp;#34;DenyAuditLogsBucketCRUD&amp;#34;,&amp;#34;Effect&amp;#34;: &amp;#34;Deny&amp;#34;,&amp;#34;Action&amp;#34;: [&amp;#34;s3:PutObject&amp;#34;,&amp;#34;s3:PutObjectAcl&amp;#34;,&amp;#34;s3:DeleteObject&amp;#34;],&amp;#34;Resource&amp;#34;: [&amp;#34;arn:aws:s3:::&amp;lt;AUDIT-LOG-BUCKET-NAME&amp;gt;/*&amp;#34;]}]} Why Is Delete Required In The Policy</description>
    </item>
    
    <item>
      <title>Cluster Configuration</title>
      <link>https://databrickslabs.github.io/overwatch/deployoverwatch/runningoverwatch/clusterconfig/</link>
      <pubDate>Tue, 13 Dec 2022 17:01:49 -0500</pubDate>
      
      <guid>https://databrickslabs.github.io/overwatch/deployoverwatch/runningoverwatch/clusterconfig/</guid>
      <description>Cluster Requirements DBR 11.3LTS as of 0.7.1.0 Overwatch will likely run on different versions of DBR but is built and tested on 11.3LTS since 0.7.1 Overwatch &amp;lt; 0.7.1 &amp;ndash; DBR 10.4LTS Overwatch &amp;lt; 0.6.1 &amp;ndash; DBR 9.1LTS Using Photon As of 0.7.1.0 Photon is recommended so long as the Overwatch cluster is using DBR 11.3LTS+. Photon does increase the DBU spend but the performance boost often results in the code running significantly more efficiently netting out a benefit.</description>
    </item>
    
    <item>
      <title>Configuration</title>
      <link>https://databrickslabs.github.io/overwatch/deployoverwatch/configureoverwatch/configuration/</link>
      <pubDate>Mon, 12 Dec 2022 11:35:40 -0500</pubDate>
      
      <guid>https://databrickslabs.github.io/overwatch/deployoverwatch/configureoverwatch/configuration/</guid>
      <description>Overwatch Deployment Configuration How it works Overwatch deployment is driven by a configuration file which will ultimately be loaded into the deployment as a csv format. This csv file will contain all the necessary details to perform the deployment. Since CSVs are a bit cantankerous we&amp;rsquo;ve offered two different methods for building the configuration file. If you&amp;rsquo;re good at VSCode or similar text editor and want to edit the CSV directly feel free to do so.</description>
    </item>
    
    <item>
      <title>Azure</title>
      <link>https://databrickslabs.github.io/overwatch/deployoverwatch/ucedeployment/cloudstorageaccessrequirements/azure/</link>
      <pubDate>Tue, 18 Apr 2023 11:28:39 -0500</pubDate>
      
      <guid>https://databrickslabs.github.io/overwatch/deployoverwatch/ucedeployment/cloudstorageaccessrequirements/azure/</guid>
      <description>Creating the Managed Identity Create a Managed Identity to authorize access to the external location. This managed Identity will be configured using a Databricks Storage Credential. Databricks recommends using an Access Connector for Azure Databricks.
After the managed identity is created, it needs to be provisioned read/write access to the storage target for the Overwatch Output (which will ultimately become your external location).
Provisioning the Managed Identity to The Storage If you intend to provision the managed identity to the storage account you need to grant the managed identity</description>
    </item>
    
    <item>
      <title>UC Pre-Requisites</title>
      <link>https://databrickslabs.github.io/overwatch/deployoverwatch/ucedeployment/uceprereqs/</link>
      <pubDate>Tue, 18 Apr 2023 11:28:39 -0500</pubDate>
      
      <guid>https://databrickslabs.github.io/overwatch/deployoverwatch/ucedeployment/uceprereqs/</guid>
      <description>Unity Catalog Prerequisites After all UC Pre-requisites are completed, please continue to Deploy Overwatch section.
This section will walk you through the steps necessary as a prerequisite to deploy Overwatch on Unity Catalog.
Workspace should be UC enabled. Overwatch Pipeline Cluster must be UC enabled (single user and runtime version &amp;gt; 11.3+). UC Storage Requirements Create Storage Credentials to be used by the external locations provisioned with appropriate read/write access to the UC External Location (AWS | GCP | AZURE) with privileges: READ FILES WRITE FILES CREATE EXTERNAL TABLE Create UC External location where Overwatch data is to be stored (AWS | GCP | AZURE).</description>
    </item>
    
    <item>
      <title>Custom Costs</title>
      <link>https://databrickslabs.github.io/overwatch/deployoverwatch/configureoverwatch/customcosts/</link>
      <pubDate>Tue, 13 Dec 2022 14:35:00 -0500</pubDate>
      
      <guid>https://databrickslabs.github.io/overwatch/deployoverwatch/configureoverwatch/customcosts/</guid>
      <description>Fine-Tuning Your Costs Every customer has their own contracts and this means that the costs associated with cloud compute and DBUs may differ between customers. To ensure the costs in Overwatch are as accurate as possible it&amp;rsquo;s important that these costs are configured as accurately as possible.
Configuring Custom Costs There are three essential components to the cost function:
The node type (instanceDetails.Api_Name) and its associated contract price (instanceDetails.Compute_Contract_Price) by Workspace The node type (instanceDetails.</description>
    </item>
    
    <item>
      <title>As A Notebook</title>
      <link>https://databrickslabs.github.io/overwatch/deployoverwatch/runningoverwatch/notebook/</link>
      <pubDate>Mon, 12 Dec 2022 12:04:26 -0500</pubDate>
      
      <guid>https://databrickslabs.github.io/overwatch/deployoverwatch/runningoverwatch/notebook/</guid>
      <description>Deploying Overwatch As A Notebook Notebooks can either be run manually or scheduled to run as a job. While the notebook can be scheduled as a job, it&amp;rsquo;s strongly recommended that Overwatch be run as a JAR instead of a notebook. Notebook execution is great for rapid testing and validation.
This deployment method requires Overwatch Version 0.7.1.0+
Converting Your Config From CSV To Delta AS OF version 0.7.1.1 you may now use a CSV OR a Delta Table OR Delta Path for your config</description>
    </item>
    
    <item>
      <title>Modules / Scopes</title>
      <link>https://databrickslabs.github.io/overwatch/dataengineer/modules/</link>
      <pubDate>Mon, 12 Dec 2022 11:40:34 -0500</pubDate>
      
      <guid>https://databrickslabs.github.io/overwatch/dataengineer/modules/</guid>
      <description>Modules A module is a single workload that builds a target table. More details about all the modules are available in Pipeline Management.
Scopes Scopes are the method by which Overwatch is segmented and a scope will contain all the related modules to build the output from Bronze through to Gold. For example there&amp;rsquo;s one scope called &amp;ldquo;jobs&amp;rdquo; but it contains all the modules for jobs and job runs from bronze through gold as well as the jobruncostpotentialfact gold fact table.</description>
    </item>
    
    <item>
      <title>GCP</title>
      <link>https://databrickslabs.github.io/overwatch/deployoverwatch/ucedeployment/cloudstorageaccessrequirements/gcp/</link>
      <pubDate>Tue, 18 Apr 2023 11:28:39 -0500</pubDate>
      
      <guid>https://databrickslabs.github.io/overwatch/deployoverwatch/ucedeployment/cloudstorageaccessrequirements/gcp/</guid>
      <description>Under Construction &amp;ndash; we will be improving these docs shortly
Service Account Storage Credential External Location Cluster Logging Locations </description>
    </item>
    
    <item>
      <title>UC Configuration Details</title>
      <link>https://databrickslabs.github.io/overwatch/deployoverwatch/ucedeployment/uceconfiguration/</link>
      <pubDate>Tue, 18 Apr 2023 11:28:39 -0500</pubDate>
      
      <guid>https://databrickslabs.github.io/overwatch/deployoverwatch/ucedeployment/uceconfiguration/</guid>
      <description>Configuration changes required for UC Enablement There is no unique configuration process for UC; however, the format for the THREE CONFIGURATIONS below are specific for UC Enablement. For all other configurations, please follow the Configuration
etl_database_name - &amp;lt;catalog_name&amp;gt;.&amp;lt;etl_database_name&amp;gt; consumer_database_name - &amp;lt;catalog_name&amp;gt;.&amp;lt;consumer_database_name&amp;gt; storage_prefix - &amp;lt;UC External Location&amp;gt;/&amp;lt;storage_prefix&amp;gt; </description>
    </item>
    
    <item>
      <title>As A JAR</title>
      <link>https://databrickslabs.github.io/overwatch/deployoverwatch/runningoverwatch/jar/</link>
      <pubDate>Tue, 13 Dec 2022 16:09:01 -0500</pubDate>
      
      <guid>https://databrickslabs.github.io/overwatch/deployoverwatch/runningoverwatch/jar/</guid>
      <description>Deploying Overwatch As A JAR On Databricks Workflows This deployment method requires Overwatch Version 0.7.1.0+
Main Class The main class for job is com.databricks.labs.overwatch.MultiWorkspaceRunner
Dependent Library com.databricks.labs:overwatch_2.12:0.7.x.x
com.microsoft.azure:azure-eventhubs-spark_2.12:2.3.21 (Azure only)
com.microsoft.azure:msal4j:1.10.1 (Azure Only - With AAD Auth For EH)
Parameters As of 0.7.1.1 the config.csv referenced below can be any one of the following
&amp;ldquo;dbfs:/path/to/config.csv&amp;rdquo; &amp;ndash; original config csv approach still works (must end with .csv) &amp;ldquo;dbfs:/path/to/deltaTable&amp;rdquo; &amp;ndash; path to a delta table containing the config &amp;ldquo;myDatabase.</description>
    </item>
    
    <item>
      <title>Security Considerations</title>
      <link>https://databrickslabs.github.io/overwatch/deployoverwatch/configureoverwatch/securityconsiderations/</link>
      <pubDate>Tue, 13 Dec 2022 14:48:09 -0500</pubDate>
      
      <guid>https://databrickslabs.github.io/overwatch/deployoverwatch/configureoverwatch/securityconsiderations/</guid>
      <description>API Access Overwatch utilizes several APIs to normalize the platform data. Overwatch leverages secret scopes and keys to acquire a token that is authorized to access the platform. The account that owns the token (i.e. dapi token) must have read access to the assets you wish to manage. If the token owner is a non-admin account the account must be granted read level access to the assets to be monitored.</description>
    </item>
    
    <item>
      <title>Pipeline_Management</title>
      <link>https://databrickslabs.github.io/overwatch/dataengineer/pipeline_management/</link>
      <pubDate>Mon, 11 Jan 2021 12:21:46 -0500</pubDate>
      
      <guid>https://databrickslabs.github.io/overwatch/dataengineer/pipeline_management/</guid>
      <description>Overwatch Data Promotion Process Overwatch data is promoted from bronze - silver - gold - presentation to ensure data consistency and quality as the data is enriched between the stages. The presentation layer is composed of views that reference the latest schema version of the gold layer. This disconnects the consumption layer from the underlying data structure so that developers can transparently add and alter columns without user disruption. All tables in each layer (except consumption) are suffixed in the ETL database with _layer.</description>
    </item>
    
    <item>
      <title>Migrating To UC</title>
      <link>https://databrickslabs.github.io/overwatch/deployoverwatch/ucedeployment/migratingtouc/</link>
      <pubDate>Tue, 18 Apr 2023 11:28:39 -0500</pubDate>
      
      <guid>https://databrickslabs.github.io/overwatch/deployoverwatch/ucedeployment/migratingtouc/</guid>
      <description>Migrating Existing Deployment From Hive_Metastore To UC Migrating a deployment from Hive Metastore has been made very simple. The steps are
Complete the UC Pre-Requisites Ensure storage is set up correctly Update the Overwatch Configuration appropriately for UC Use the Migration Notebook below to migrate the data from Hive to UC Resume the job Migration Notebook Migration Notebook ( HTML | DBC ) Details to Run are in the notebook </description>
    </item>
    
    <item>
      <title>Sharing Overwatch Data</title>
      <link>https://databrickslabs.github.io/overwatch/deployoverwatch/sharingoverwatch/</link>
      <pubDate>Thu, 15 Dec 2022 18:31:01 -0500</pubDate>
      
      <guid>https://databrickslabs.github.io/overwatch/deployoverwatch/sharingoverwatch/</guid>
      <description>Granting Access Via DBSQL If your storage prefix is referencing a mount point, nothing should be necessary here as the mount point is already accessible to all workspace users. If using a direct path (i.e. s3://, abfss://, or gs://) you will need to configure the appropriate access in Admin Settings &amp;ndash;&amp;gt; SQL Warehouse Settings.
If using a Hive Metastore you may still need to review your grants to ensure users have access to the Overwatch tables / views.</description>
    </item>
    
    <item>
      <title>As A Notebook (Legacy)</title>
      <link>https://databrickslabs.github.io/overwatch/deployoverwatch/runningoverwatch/notebooklegacy/</link>
      <pubDate>Mon, 12 Dec 2022 12:04:43 -0500</pubDate>
      
      <guid>https://databrickslabs.github.io/overwatch/deployoverwatch/runningoverwatch/notebooklegacy/</guid>
      <description>As of version 0.7.1.0 Overwatch will begin sunsetting this deployment method. Please reference Running A Job As A Notebook for the updated method for deploying Overwatch. This method will likely be deprecated in Overwatch version 0.8 and no longer be supported in 0.9.
The new deployment method provides support for a &amp;ldquo;single-workspace deployment&amp;rdquo; or &amp;ldquo;multi-workspace deployment&amp;rdquo; where a single Overwatch job is configured and loads data from all workspaces. More details available in Running A Job As A Notebook.</description>
    </item>
    
    <item>
      <title>Validation</title>
      <link>https://databrickslabs.github.io/overwatch/deployoverwatch/configureoverwatch/validation/</link>
      <pubDate>Mon, 12 Dec 2022 11:36:53 -0500</pubDate>
      
      <guid>https://databrickslabs.github.io/overwatch/deployoverwatch/configureoverwatch/validation/</guid>
      <description>Deployment Validations Many validations are performed to minimize mistakes. The following section offers details on all validations done. Not all validation steps are done on all runs. All validations should be performed before a first run on any given workspace. For more information on executing a full validation, see below
Validation rules Name Validation Rule Impacted columns Api Url Validation API URL should give some response with provided scope and key.</description>
    </item>
    
    <item>
      <title>Upgrades</title>
      <link>https://databrickslabs.github.io/overwatch/dataengineer/upgrade/</link>
      <pubDate>Thu, 20 May 2021 21:27:44 -0400</pubDate>
      
      <guid>https://databrickslabs.github.io/overwatch/dataengineer/upgrade/</guid>
      <description>Sometimes upgrading from one version to the next requires a schema change. In these cases, the CHANGELOG will be explicit. Upgrades MUST be executed WITH the new library (jar) and before the pipeline is executed. The general upgrade process is:
Use the compactString of parameters to instantiate the workspace The compact string can be found in your original runner notebook which you got from here Call the upgrade function for the version to which you&amp;rsquo;re upgrading and pass in the workspace object Basic pseudocode can be found below as a reference.</description>
    </item>
    
    <item>
      <title>As A JAR (Legacy)</title>
      <link>https://databrickslabs.github.io/overwatch/deployoverwatch/runningoverwatch/jarlegacy/</link>
      <pubDate>Tue, 13 Dec 2022 16:09:07 -0500</pubDate>
      
      <guid>https://databrickslabs.github.io/overwatch/deployoverwatch/runningoverwatch/jarlegacy/</guid>
      <description>As of version 0.7.1.0 Overwatch will begin sunsetting this deployment method. Please reference Running A Job As A Jar for the updated method for deploying Overwatch. This method will likely be deprecated in Overwatch version 0.8 and no longer be supported in 0.9.
The new deployment method provides support for a &amp;ldquo;single-workspace deployment&amp;rdquo; or &amp;ldquo;multi-workspace deployment&amp;rdquo; where a single Overwatch job is configured and loads data from all workspaces. More importantly, there are at most 3 parameters that can be passed in much more simplistically.</description>
    </item>
    
    <item>
      <title>Productionizing</title>
      <link>https://databrickslabs.github.io/overwatch/dataengineer/productionizing/</link>
      <pubDate>Wed, 20 Jul 2022 15:03:23 -0400</pubDate>
      
      <guid>https://databrickslabs.github.io/overwatch/dataengineer/productionizing/</guid>
      <description>Moving To Production When you&amp;rsquo;re ready to move to production, there are a few things to keep in mind and best practices to follow to get the most out of Overwatch
Cluster Logging Simplify and Unify your cluster logging directories
Many users forget to enable cluster logging and without it Overwatch cannot provide usage telemetry by notebook, job, user so it&amp;rsquo;s critical that all clusters have clusters logs enabled If users are allowed to create clusters/jobs without any governance, log files will be produced and stored all over the place.</description>
    </item>
    
    <item>
      <title>AdvancedTopics</title>
      <link>https://databrickslabs.github.io/overwatch/dataengineer/advancedtopics/</link>
      <pubDate>Mon, 12 Dec 2022 11:41:13 -0500</pubDate>
      
      <guid>https://databrickslabs.github.io/overwatch/dataengineer/advancedtopics/</guid>
      <description>Quick Reference Externalize Optimize &amp;amp; Z-Order Interacting With Overwatch and its State Optimizing Overwatch Maximizing First Run Potential Historical Loads Cluster Logs Ingest Details Joining With Slow Changing Dimensions (SCD) Optimizing Overwatch Expectation Check Note that Overwatch analyzes nearly all aspects of the Workspace and manages its own pipeline among many other tasks. This results in 1000s of spark job executions and as such, the Overwatch job will take some time to run.</description>
    </item>
    
    <item>
      <title>Configuration (Legacy)</title>
      <link>https://databrickslabs.github.io/overwatch/deployoverwatch/configureoverwatch/configurationlegacy/</link>
      <pubDate>Mon, 12 Dec 2022 11:35:49 -0500</pubDate>
      
      <guid>https://databrickslabs.github.io/overwatch/deployoverwatch/configureoverwatch/configurationlegacy/</guid>
      <description>NEW CUSTOMERS &amp;ndash; This is a legacy configuration, please use the new deployment model. EXISTING CUSTOMERS COMING FROM VERSION &amp;lt; 0.7.1.0 As of version 0.7.1.0 Overwatch will begin sunsetting this &amp;ldquo;legacy&amp;rdquo; deployment method and configuration. Please review the benefits of the new deployment method and plan to switch to this new deployment method by end of Q3 2023.
Configuration Basics The Overwatch configuration can be created as a case class of OverwatchParams or as a json string passed into the main class com.</description>
    </item>
    
    <item>
      <title>Azure</title>
      <link>https://databrickslabs.github.io/overwatch/deployoverwatch/cloudinfra/azure/</link>
      <pubDate>Mon, 12 Dec 2022 11:29:59 -0500</pubDate>
      
      <guid>https://databrickslabs.github.io/overwatch/deployoverwatch/cloudinfra/azure/</guid>
      <description>Fast Travel Configuring Overwatch on Azure Databricks Reference Architecture Reference Architecture (Legacy) Configuring Audit Log Delivery Through Event Hub Setting up Storage Accounts Mount Storage Accounts Configuring Overwatch on Azure Databricks Reach out to your Customer Success Engineer (CSE) to help you with these tasks as needed. To get started, the Basic Deployment configuration. As more modules are enabled, additional environment configuration may be required in addition to the Basic Deployment.</description>
    </item>
    
    <item>
      <title>AWS/GCP</title>
      <link>https://databrickslabs.github.io/overwatch/deployoverwatch/cloudinfra/aws/</link>
      <pubDate>Mon, 12 Dec 2022 11:29:56 -0500</pubDate>
      
      <guid>https://databrickslabs.github.io/overwatch/deployoverwatch/cloudinfra/aws/</guid>
      <description>Configuring Overwatch on AWS/GCP - Databricks Reach out to your Customer Success Engineer (CSE) to help you with these tasks as needed. To get started, the Basic Deployment configuration. As more modules are enabled, additional environment configuration may be required in addition to the Basic Deployment.
There are two primary sources of data that need to be configured:
Audit Logs-AWS/Audit Logs-GCP These will be delivered to the configured bucket. These buckets are configured on a per-workspace basis and can be delivered to the same target bucket, just ensure that the prefixes are different to avoid collisions.</description>
    </item>
    
    <item>
      <title>Data Dictionary - 0.6.1.x</title>
      <link>https://databrickslabs.github.io/overwatch/dataengineer/definitions/061x/</link>
      <pubDate>Mon, 26 Sep 2022 08:47:24 -0400</pubDate>
      
      <guid>https://databrickslabs.github.io/overwatch/dataengineer/definitions/061x/</guid>
      <description>Consumption Layer Column Descriptions ETL Tables Bronze Silver Gold Consumption Layer &amp;ldquo;Tables&amp;rdquo; (Views) All end users should be hitting consumer tables first. Digging into lower layers gets significantly more complex. Below is the data model for the consumption layer. The consumption layer is often in a stand-alone database apart from the ETL tables to minimize clutter and confusion. These entities in this layer are actually not tables at all (with a few minor exceptions such as lookup tables) but rather views.</description>
    </item>
    
    <item>
      <title>Configuration Details By Version</title>
      <link>https://databrickslabs.github.io/overwatch/deployoverwatch/configureoverwatch/configdetailsbyversion/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://databrickslabs.github.io/overwatch/deployoverwatch/configureoverwatch/configdetailsbyversion/</guid>
      <description>Overwatch Deployment Configuration By Version 0.7.2.x Configuration 0.7.1.x Configuration Column description 0.7.2.x Column Type IsRequired Description workspace_name String True Name of the workspace. workspace_id String True Id of the workspace. workspace_url String True URL of the workspace. api_url String True API URL for the Workspace (execute in scala dbutils.notebook.getContext().apiUrl.get ON THE TARGET WORKSPACE NOT DEPLOYMENT WORKSPACE to get the API URL for the workspace. NOTE: Workspace_URL and API_URL can be different for a workspace but may be the same even for multiple workspaces).</description>
    </item>
    
  </channel>
</rss>
