<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Welcome To Overwatch on Overwatch</title>
    <link>https://databrickslabs.github.io/overwatch/</link>
    <description>Recent content in Welcome To Overwatch on Overwatch</description>
    <generator>Hugo</generator>
    <language>en</language>
    <lastBuildDate>Tue, 23 Jan 2024 16:30:21 +0530</lastBuildDate>
    <atom:link href="https://databrickslabs.github.io/overwatch/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>AWS</title>
      <link>https://databrickslabs.github.io/overwatch/deployoverwatch/ucedeployment/cloudstorageaccessrequirements/aws/</link>
      <pubDate>Tue, 18 Apr 2023 11:28:39 -0500</pubDate>
      <guid>https://databrickslabs.github.io/overwatch/deployoverwatch/ucedeployment/cloudstorageaccessrequirements/aws/</guid>
      <description>&lt;p&gt;Below are the requirements needed for Storage Access setup in AWS&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;#aws-iam-role-required-for-storage-credentials&#34;&gt;AWS IAM Role/Policy required for Storage Credentials&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;#trust-relation-required-in-storage-credentials-iam-role&#34;&gt;Trust Relation required in Storage Credentials IAM Role&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;#instance-profile-required-for-overwatch-job-cluster&#34;&gt;Instance Profile required for Overwatch Job/Interactive Cluster&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;aws-iam-role-required-for-storage-credentials&#34;&gt;AWS IAM Role required for Storage Credentials&lt;/h3&gt;&#xA;&lt;p&gt;This IAM Role to authorize access to the external location. It will be configured while creating the Databricks Storage Credential.&#xA;Below policy will be used for creating the IAM Role. Please refer this &lt;a href=&#34;https://docs.databricks.com/data-governance/unity-catalog/manage-external-locations-and-credentials.html#manage-storage-credentials&#34;&gt;doc&lt;/a&gt; for a detailed description on creating IAM role for Storage Credentials&lt;/p&gt;</description>
    </item>
    <item>
      <title>Cluster Configuration</title>
      <link>https://databrickslabs.github.io/overwatch/deployoverwatch/runningoverwatch/clusterconfig/</link>
      <pubDate>Tue, 13 Dec 2022 17:01:49 -0500</pubDate>
      <guid>https://databrickslabs.github.io/overwatch/deployoverwatch/runningoverwatch/clusterconfig/</guid>
      <description>&lt;h2 id=&#34;cluster-requirements&#34;&gt;Cluster Requirements&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;DBR 11.3LTS as of 0.7.1.0&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Overwatch will likely run on different versions of DBR but is built and tested on 11.3LTS since 0.7.1&lt;/li&gt;&#xA;&lt;li&gt;Overwatch &amp;lt; 0.7.1 &amp;ndash; DBR 10.4LTS&lt;/li&gt;&#xA;&lt;li&gt;Overwatch &amp;lt; 0.6.1 &amp;ndash; DBR 9.1LTS&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Using Photon&#xA;&lt;ul&gt;&#xA;&lt;li&gt;As of 0.7.1.0 &lt;strong&gt;Photon is recommended&lt;/strong&gt; so long as the Overwatch cluster is using DBR 11.3LTS+.&#xA;Photon does increase the DBU spend but the performance boost often results&#xA;in the code running significantly more efficiently netting out a benefit. Mileage can vary between customers so&#xA;if you really want to know which is most efficient, feel free to run on both and use Overwatch to determine which is&#xA;best for you.&lt;/li&gt;&#xA;&lt;li&gt;Prior to 0.7.1.0 and DBR 11.3LTS Photon was untested&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Disable Autoscaling - See &lt;a href=&#34;#notes-on-autoscaling&#34;&gt;Notes On Autoscaling&lt;/a&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;External optimize cluster recommendations are different.&#xA;See &lt;a href=&#34;https://databrickslabs.github.io/overwatch/dataengineer/advancedtopics//#externalize-optimize--z-order-as-of-060&#34;&gt;External Optimize&lt;/a&gt; for more details&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Add the relevant &lt;a href=&#34;#cluster-dependencies&#34;&gt;dependencies&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;cluster-dependencies&#34;&gt;Cluster Dependencies&lt;/h3&gt;&#xA;&lt;p&gt;Add the following dependencies to your cluster&lt;/p&gt;</description>
    </item>
    <item>
      <title>Configuration</title>
      <link>https://databrickslabs.github.io/overwatch/deployoverwatch/configureoverwatch/configuration/</link>
      <pubDate>Mon, 12 Dec 2022 11:35:40 -0500</pubDate>
      <guid>https://databrickslabs.github.io/overwatch/deployoverwatch/configureoverwatch/configuration/</guid>
      <description>&lt;h2 id=&#34;overwatch-deployment-configuration&#34;&gt;Overwatch Deployment Configuration&lt;/h2&gt;&#xA;&lt;h3 id=&#34;how-it-works&#34;&gt;How it works&lt;/h3&gt;&#xA;&lt;p&gt;Overwatch deployment is driven by a configuration file which will ultimately be loaded into the deployment as&#xA;a csv format or delta table. This configuration will contain all the necessary details to perform the deployment. Since CSVs are a bit&#xA;cantankerous we&amp;rsquo;ve offered two different methods for building the configuration file. If you&amp;rsquo;re good at VSCode or&#xA;similar text editor and want to edit the CSV directly feel free to do so. We &lt;strong&gt;strongly&lt;/strong&gt; recommend that you create a delta&#xA;table with the csv file you just created and use that as your configuration input.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Azure</title>
      <link>https://databrickslabs.github.io/overwatch/deployoverwatch/ucedeployment/cloudstorageaccessrequirements/azure/</link>
      <pubDate>Tue, 18 Apr 2023 11:28:39 -0500</pubDate>
      <guid>https://databrickslabs.github.io/overwatch/deployoverwatch/ucedeployment/cloudstorageaccessrequirements/azure/</guid>
      <description>&lt;h2 id=&#34;creating-the-managed-identity&#34;&gt;Creating the Managed Identity&lt;/h2&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://learn.microsoft.com/en-us/azure/databricks/data-governance/unity-catalog/azure-managed-identities#--configure-a-managed-identity-for-unity-catalog&#34;&gt;Create a Managed Identity&lt;/a&gt;&#xA;to authorize access to the external location. This managed Identity will be configured using a &lt;a href=&#34;https://learn.microsoft.com/en-us/azure/databricks/data/manage-storage-credentials&#34;&gt;Databricks&#xA;Storage Credential&lt;/a&gt;. Databricks&#xA;recommends using an Access Connector for Azure Databricks.&lt;/p&gt;&#xA;&lt;p&gt;After the managed identity is created, it needs to be provisioned read/write access to the storage target for the&#xA;Overwatch Output (which will ultimately become your external location).&lt;/p&gt;&#xA;&lt;h2 id=&#34;provisioning-the-managed-identity-to-the-storage&#34;&gt;Provisioning the Managed Identity to The Storage&lt;/h2&gt;&#xA;&lt;p&gt;If you intend to provision the managed identity &lt;strong&gt;to the storage account&lt;/strong&gt; you need to grant the managed identity&lt;/p&gt;</description>
    </item>
    <item>
      <title>UC Pre-Requisites</title>
      <link>https://databrickslabs.github.io/overwatch/deployoverwatch/ucedeployment/uceprereqs/</link>
      <pubDate>Tue, 18 Apr 2023 11:28:39 -0500</pubDate>
      <guid>https://databrickslabs.github.io/overwatch/deployoverwatch/ucedeployment/uceprereqs/</guid>
      <description>&lt;h2 id=&#34;unity-catalog-prerequisites&#34;&gt;Unity Catalog Prerequisites&lt;/h2&gt;&#xA;&lt;p&gt;After all UC Pre-requisites are completed, please continue to &lt;a href=&#34;https://databrickslabs.github.io/overwatch/deployoverwatch/&#34;&gt;Deploy Overwatch&lt;/a&gt;&#xA;section.&lt;/p&gt;&#xA;&lt;p&gt;This section will walk you through the steps necessary as a prerequisite to deploy Overwatch on Unity Catalog.&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Workspace&lt;/strong&gt; should be &lt;strong&gt;UC enabled&lt;/strong&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Overwatch Pipeline &lt;strong&gt;Cluster&lt;/strong&gt; must be &lt;strong&gt;UC enabled&lt;/strong&gt; (single user and runtime version &amp;gt; 11.3+).&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://databrickslabs.github.io/overwatch/deployoverwatch/ucedeployment/cloudstorageaccessrequirements/&#34;&gt;UC &lt;strong&gt;Storage Requirements&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;Create &lt;strong&gt;Storage Credentials&lt;/strong&gt; to be used by the external locations provisioned with appropriate read/write&#xA;access to the UC External Location&#xA;(&lt;a href=&#34;https://docs.databricks.com/data-governance/unity-catalog/manage-external-locations-and-credentials.html#create-a-storage-credential&#34;&gt;AWS&lt;/a&gt; |&#xA;&lt;a href=&#34;https://docs.gcp.databricks.com/data-governance/unity-catalog/manage-external-locations-and-credentials.html#manage-storage-credentials&#34;&gt;GCP&lt;/a&gt; |&#xA;&lt;a href=&#34;https://learn.microsoft.com/en-gb/azure/databricks/data-governance/unity-catalog/manage-external-locations-and-credentials#--create-a-storage-credential&#34;&gt;AZURE&lt;/a&gt;)&#xA;with privileges:&#xA;&lt;ul&gt;&#xA;&lt;li&gt;READ FILES&lt;/li&gt;&#xA;&lt;li&gt;WRITE FILES&lt;/li&gt;&#xA;&lt;li&gt;CREATE EXTERNAL TABLE&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Create &lt;strong&gt;UC External location&lt;/strong&gt; where Overwatch data is to be stored&#xA;(&lt;a href=&#34;https://docs.databricks.com/data/manage-external-locations.html#manage-unity-catalog-external-locations-in-data-explorer&#34;&gt;AWS&lt;/a&gt; |&#xA;&lt;a href=&#34;https://docs.gcp.databricks.com/data/manage-external-locations.html#manage-unity-catalog-external-locations-in-data-explorer&#34;&gt;GCP&lt;/a&gt; |&#xA;&lt;a href=&#34;https://learn.microsoft.com/en-gb/azure/databricks/data/manage-external-locations#create-external-location&#34;&gt;AZURE&lt;/a&gt;).&#xA;Provide ALL PRIVILEGES permission to the principal (user/SP) that is going to run the Overwatch Pipeline.&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Ensure the external location path is pointing to the same storage location to which the storage credential&amp;rsquo;s&#xA;identity was authorized&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Create a &lt;strong&gt;Catalog&lt;/strong&gt; or identify an existing catalog where overwatch data will be stored.&#xA;&lt;strong&gt;Overwatch&lt;/strong&gt; code &lt;strong&gt;WILL NOT&lt;/strong&gt; create the catalog, it must be pre-existing.&lt;/li&gt;&#xA;&lt;li&gt;Principal (user/SP) executing the Overwatch Pipeline must have access to the &lt;strong&gt;catalog&lt;/strong&gt; with &lt;strong&gt;privileges&lt;/strong&gt;:&#xA;&lt;ul&gt;&#xA;&lt;li&gt;USE CATALOG&lt;/li&gt;&#xA;&lt;li&gt;USE SCHEMA&lt;/li&gt;&#xA;&lt;li&gt;SELECT&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Create ETL and Consumer Schemas&lt;/strong&gt; (i.e. databases). &lt;strong&gt;Overwatch WILL NOT&lt;/strong&gt; create the Schemas in a UC Deployment.&#xA;Principal (user/SP) executing the Overwatch Pipeline must have the following privileges on the Schema AND must be&#xA;an Owner of the Schema.&#xA;&lt;ul&gt;&#xA;&lt;li&gt;IS OWNER &amp;ndash; required since schema metadata is edited and requires schema ownership&#xA;&lt;ul&gt;&#xA;&lt;li&gt;The schema owner can be a user, service principal, or group. It&amp;rsquo;s recommended that you provision an&#xA;Overwatch_Maintainers group and place the Overwatch Admin users in this group along with any service principals&#xA;that will be writing data to the output and assign this group as owner of the Overwatch schemas.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;USE SCHEMA&lt;/li&gt;&#xA;&lt;li&gt;CREATE TABLE&lt;/li&gt;&#xA;&lt;li&gt;MODIFY&lt;/li&gt;&#xA;&lt;li&gt;SELECT&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Overwatch latest version(0.7.2.0+) should be deployed in the workspace&#xA;&lt;ul&gt;&#xA;&lt;li&gt;0.7.1.1+ is ok so long as the &lt;a href=&#34;https://databrickslabs.github.io/overwatch/deployoverwatch/ucedeployment/migratingtouc/&#34;&gt;migration process&lt;/a&gt;&#xA;is completed before executing with Unity Catalog configurations.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Other overwatch prerequisites can be found &lt;a href=&#34;https://databrickslabs.github.io/overwatch/deployoverwatch/cloudinfra/&#34;&gt;here&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;sql-command-to-grant-permissions-to-various-uc-objects&#34;&gt;SQL Command to Grant Permissions to various UC Objects&lt;/h2&gt;&#xA;&lt;p&gt;The following can be done through the UI or via commands as shown below&lt;/p&gt;</description>
    </item>
    <item>
      <title>Custom Costs</title>
      <link>https://databrickslabs.github.io/overwatch/deployoverwatch/configureoverwatch/customcosts/</link>
      <pubDate>Tue, 13 Dec 2022 14:35:00 -0500</pubDate>
      <guid>https://databrickslabs.github.io/overwatch/deployoverwatch/configureoverwatch/customcosts/</guid>
      <description>&lt;h2 id=&#34;fine-tuning-your-costs&#34;&gt;Fine-Tuning Your Costs&lt;/h2&gt;&#xA;&lt;p&gt;Every customer has their own contracts and this means that the costs associated with cloud compute and DBUs may differ&#xA;between customers. To ensure the costs in Overwatch are as accurate as possible it&amp;rsquo;s important that these costs are&#xA;configured as accurately as possible.&lt;/p&gt;&#xA;&lt;h3 id=&#34;configuring-custom-costs&#34;&gt;Configuring Custom Costs&lt;/h3&gt;&#xA;&lt;p&gt;There are three essential components to the cost function:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;The node type (instanceDetails.Api_Name) and its associated contract price (instanceDetails.Compute_Contract_Price)&#xA;by Workspace&lt;/li&gt;&#xA;&lt;li&gt;The node type (instanceDetails.Api_Name) and its associated DBUs per hour (instanceDetails.Hourly_DBUs).&#xA;These should be accurate from the default load but Databricks may adjust their DBUs/Hour by node type. This is&#xA;especially true when a node goes from beta to GA.&lt;/li&gt;&#xA;&lt;li&gt;The DBU contract prices for the SKU under which your DBUs are charged such as:&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Interactive&lt;/li&gt;&#xA;&lt;li&gt;Automated&lt;/li&gt;&#xA;&lt;li&gt;DatabricksSQL&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Databricks currently has 3 SKUs (classic/pro/serverless) but Overwatch is not able to accurately report&#xA;on DBSQL pricing at this time due to data not available in the customer-facing Databricks Product.&#xA;When this data becomes available, Overwatch will integrate it and enable DBSQL cost tracking. In the&#xA;meantime Overwatch will does it&amp;rsquo;s best to estimate DBSQL pricing so for this SKU just put your average&#xA;$DBU cost or a close estimate to your sku price here.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;JobsLight&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;The DBU contract costs are captured from the&#xA;&lt;a href=&#34;https://databrickslabs.github.io/overwatch/deployoverwatch/configureoverwatch/configuration//#databrickscontractprices&#34;&gt;Overwatch Configuration&lt;/a&gt; maintained&#xA;as a slow-changing-dimension in the &lt;a href=&#34;https://databrickslabs.github.io/overwatch/dataengineer/definitions//#dbucostdetails&#34;&gt;dbuCostDetails table&lt;/a&gt;.&#xA;The compute costs and dbu to node&#xA;associations are maintained as a slow-changing-dimension in the&#xA;&lt;a href=&#34;https://databrickslabs.github.io/overwatch/dataengineer/definitions//#instancedetails&#34;&gt;instanceDetails&lt;/a&gt; table.&lt;/p&gt;</description>
    </item>
    <item>
      <title>As A Notebook</title>
      <link>https://databrickslabs.github.io/overwatch/deployoverwatch/runningoverwatch/notebook/</link>
      <pubDate>Mon, 12 Dec 2022 12:04:26 -0500</pubDate>
      <guid>https://databrickslabs.github.io/overwatch/deployoverwatch/runningoverwatch/notebook/</guid>
      <description>&lt;h2 id=&#34;deploying-overwatch-as-a-notebook&#34;&gt;Deploying Overwatch As A Notebook&lt;/h2&gt;&#xA;&lt;p&gt;Notebooks can either be run manually or scheduled to run as a job. While the notebook can be scheduled as a job,&#xA;it&amp;rsquo;s strongly recommended that Overwatch be run as a JAR instead of a notebook. Notebook execution is great for&#xA;rapid testing and validation.&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;This deployment method requires Overwatch Version 0.7.1.0+&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;jumpstart-notebook&#34;&gt;Jumpstart Notebook&lt;/h3&gt;&#xA;&lt;p&gt;Below is an example deployment. When you&amp;rsquo;re ready to get started simply download the rapid start linked notebook below.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Modules / Scopes</title>
      <link>https://databrickslabs.github.io/overwatch/dataengineer/modules/</link>
      <pubDate>Mon, 12 Dec 2022 11:40:34 -0500</pubDate>
      <guid>https://databrickslabs.github.io/overwatch/dataengineer/modules/</guid>
      <description>&lt;h2 id=&#34;modules&#34;&gt;Modules&lt;/h2&gt;&#xA;&lt;p&gt;A module is a single workload that builds a target table. More details about all the modules are available in&#xA;&lt;a href=&#34;https://databrickslabs.github.io/overwatch/dataengineer/pipeline_management//#module-dependencies&#34;&gt;Pipeline Management&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;h2 id=&#34;scopes&#34;&gt;Scopes&lt;/h2&gt;&#xA;&lt;p&gt;Scopes are the method by which Overwatch is segmented and a scope will contain all the related modules to build&#xA;the output from Bronze through to Gold. For example there&amp;rsquo;s one scope called &amp;ldquo;jobs&amp;rdquo; but it contains all the modules&#xA;for jobs and job runs from bronze through gold as well as the jobruncostpotentialfact gold fact table.&lt;/p&gt;</description>
    </item>
    <item>
      <title>GCP</title>
      <link>https://databrickslabs.github.io/overwatch/deployoverwatch/ucedeployment/cloudstorageaccessrequirements/gcp/</link>
      <pubDate>Tue, 18 Apr 2023 11:28:39 -0500</pubDate>
      <guid>https://databrickslabs.github.io/overwatch/deployoverwatch/ucedeployment/cloudstorageaccessrequirements/gcp/</guid>
      <description>&lt;p&gt;Below are the requirement needed for Storage Access setup in GCP&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;#google-service-account-for-storage-credentials&#34;&gt;Google Service Account for Storage Credentials&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;#google-service-account-for-overwatch-job-cluster&#34;&gt;Google Service Account for Overwatch Job Cluster&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;#cluster-logging-locations-setup&#34;&gt;Cluster Logging Locations Setup&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;google-service-account-for-storage-credentials&#34;&gt;Google Service Account for Storage Credentials&lt;/h3&gt;&#xA;&lt;p&gt;Unity Catalog Storage credentials  should have the ability to read and write to a External Location(GCS bucket) by&#xA;assigning appropriate IAM roles on that bucket to a Databricks-generated Google Cloud service account. Please refer the&#xA;&lt;a href=&#34;https://docs.gcp.databricks.com/data-governance/unity-catalog/manage-external-locations-and-credentials.html#manage-storage-credentials&#34;&gt;docs&lt;/a&gt;&#xA;for detailed steps.&lt;/p&gt;</description>
    </item>
    <item>
      <title>UC Configuration Details</title>
      <link>https://databrickslabs.github.io/overwatch/deployoverwatch/ucedeployment/uceconfiguration/</link>
      <pubDate>Tue, 18 Apr 2023 11:28:39 -0500</pubDate>
      <guid>https://databrickslabs.github.io/overwatch/deployoverwatch/ucedeployment/uceconfiguration/</guid>
      <description>&lt;h2 id=&#34;configuration-changes-required-for-uc-enablement&#34;&gt;Configuration changes required for UC Enablement&lt;/h2&gt;&#xA;&lt;p&gt;There is no unique configuration process for UC; however, the format for the &lt;strong&gt;THREE CONFIGURATIONS&lt;/strong&gt; below&#xA;are specific for UC Enablement. For all other configurations, please follow the&#xA;&lt;a href=&#34;https://databrickslabs.github.io/overwatch/deployoverwatch/configureoverwatch/configuration/&#34;&gt;Configuration&lt;/a&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;etl_database_name&lt;/strong&gt; - &amp;lt;catalog_name&amp;gt;.&amp;lt;etl_database_name&amp;gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;consumer_database_name&lt;/strong&gt; - &amp;lt;catalog_name&amp;gt;.&amp;lt;consumer_database_name&amp;gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;storage_prefix&lt;/strong&gt; -  &amp;lt;UC External Location&amp;gt;/&amp;lt;storage_prefix&amp;gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;div class=&#34;notices note&#34; &gt;&lt;p&gt;With the release of Overwatch 0.8.2.0, we have added support for Multi Catalog Overwatch Deployment.&#xA;Now user can provide different catalogs for ETL Database and Consumer Database.&lt;/p&gt;</description>
    </item>
    <item>
      <title>As A JAR</title>
      <link>https://databrickslabs.github.io/overwatch/deployoverwatch/runningoverwatch/jar/</link>
      <pubDate>Tue, 13 Dec 2022 16:09:01 -0500</pubDate>
      <guid>https://databrickslabs.github.io/overwatch/deployoverwatch/runningoverwatch/jar/</guid>
      <description>&lt;h2 id=&#34;deploying-overwatch-as-a-jar-on-databricks-workflows&#34;&gt;Deploying Overwatch As A JAR On Databricks Workflows&lt;/h2&gt;&#xA;&lt;p&gt;&lt;strong&gt;This deployment method requires Overwatch Version 0.7.1.0+&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;main-class&#34;&gt;Main Class&lt;/h3&gt;&#xA;&lt;p&gt;The main class for job is &lt;code&gt;com.databricks.labs.overwatch.MultiWorkspaceRunner&lt;/code&gt;&lt;!-- raw HTML omitted --&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;dependent-library&#34;&gt;Dependent Library&lt;/h3&gt;&#xA;&lt;p&gt;&lt;code&gt;com.databricks.labs:overwatch_2.12:0.8.x.x&lt;/code&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;code&gt;com.microsoft.azure:azure-eventhubs-spark_2.12:2.3.21&lt;/code&gt; &lt;strong&gt;(Azure only - If not using system tables)&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;code&gt;com.microsoft.azure:msal4j:1.10.1&lt;/code&gt; &lt;strong&gt;(Azure Only - With AAD Auth For EH, if not using system tables)&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;parameters&#34;&gt;Parameters&lt;/h3&gt;&#xA;&lt;p&gt;As of 0.7.1.1 the config.csv referenced below can be any one of the following&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&amp;ldquo;dbfs:/path/to/config.csv&amp;rdquo; &amp;ndash; original config csv approach still works (must end with .csv)&lt;/li&gt;&#xA;&lt;li&gt;&amp;ldquo;dbfs:/path/to/deltaTable&amp;rdquo; &amp;ndash; path to a delta table containing the config&lt;/li&gt;&#xA;&lt;li&gt;&amp;ldquo;myDatabase.myConfigTable&amp;rdquo; &amp;ndash; name of delta table that contains the config&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;Note: any of the paths in examples above may be on any supported storage, dbfs:/ is not required.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Security Considerations</title>
      <link>https://databrickslabs.github.io/overwatch/deployoverwatch/configureoverwatch/securityconsiderations/</link>
      <pubDate>Tue, 13 Dec 2022 14:48:09 -0500</pubDate>
      <guid>https://databrickslabs.github.io/overwatch/deployoverwatch/configureoverwatch/securityconsiderations/</guid>
      <description>&lt;h2 id=&#34;api-access&#34;&gt;API Access&lt;/h2&gt;&#xA;&lt;p&gt;Overwatch utilizes several APIs to normalize the platform data. Overwatch leverages secret scopes and keys to acquire&#xA;a token that is authorized to access the platform. The account that owns the token (i.e. dapi token) must have&#xA;read access to the assets you wish to manage. If the token owner is a non-admin account the account must be granted&#xA;read level access to the assets to be monitored.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Pipeline_Management</title>
      <link>https://databrickslabs.github.io/overwatch/dataengineer/pipeline_management/</link>
      <pubDate>Mon, 11 Jan 2021 12:21:46 -0500</pubDate>
      <guid>https://databrickslabs.github.io/overwatch/dataengineer/pipeline_management/</guid>
      <description>&lt;h2 id=&#34;overwatch-data-promotion-process&#34;&gt;Overwatch Data Promotion Process&lt;/h2&gt;&#xA;&lt;p&gt;Overwatch data is promoted from bronze - silver - gold - presentation to ensure data consistency and quality as the&#xA;data is enriched between the stages. The presentation layer is composed of views that reference the latest schema&#xA;version of the gold layer. This disconnects the consumption layer from the underlying data structure so that developers&#xA;can transparently add and alter columns without user disruption. All tables in each layer (except consumption) are&#xA;suffixed in the ETL database with &lt;em&gt;_layer&lt;/em&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Migrating To UC</title>
      <link>https://databrickslabs.github.io/overwatch/deployoverwatch/ucedeployment/migratingtouc/</link>
      <pubDate>Tue, 18 Apr 2023 11:28:39 -0500</pubDate>
      <guid>https://databrickslabs.github.io/overwatch/deployoverwatch/ucedeployment/migratingtouc/</guid>
      <description>&lt;h2 id=&#34;migrating-existing-deployment-from-hive_metastore-to-uc&#34;&gt;Migrating Existing Deployment From Hive_Metastore To UC&lt;/h2&gt;&#xA;&lt;p&gt;Migrating a deployment from Hive Metastore has been made very simple. The steps are&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Complete the &lt;a href=&#34;https://databrickslabs.github.io/overwatch/deployoverwatch/ucedeployment/uceprereqs/&#34;&gt;UC Pre-Requisites&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;Ensure &lt;a href=&#34;https://databrickslabs.github.io/overwatch/deployoverwatch/ucedeployment/cloudstorageaccessrequirements/&#34;&gt;storage is set up correctly&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;Update the &lt;a href=&#34;https://databrickslabs.github.io/overwatch/deployoverwatch/ucedeployment/uceconfiguration/&#34;&gt;Overwatch Configuration appropriately for UC&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;Use the Migration Notebook below to migrate the data from Hive to UC&lt;/li&gt;&#xA;&lt;li&gt;Resume the job&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;migration-notebook&#34;&gt;Migration Notebook&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Migration Notebook&lt;/strong&gt; ( &lt;a href=&#34;https://databrickslabs.github.io/overwatch/assets/DeployOverwatch/UC_Migration_Utility-Hive_metastore_To_Unity_Catalog.html&#34;&gt;&lt;strong&gt;HTML&lt;/strong&gt;&lt;/a&gt; |&#xA;&lt;a href=&#34;https://databrickslabs.github.io/overwatch/assets/DeployOverwatch/UC_Migration_Utility-Hive_metastore_To_Unity_Catalog.dbc&#34;&gt;&lt;strong&gt;DBC&lt;/strong&gt;&lt;/a&gt; )&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Details to Run are in the notebook&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
    <item>
      <title>Configuration Validation</title>
      <link>https://databrickslabs.github.io/overwatch/deployoverwatch/configureoverwatch/validation/</link>
      <pubDate>Mon, 12 Dec 2022 11:36:53 -0500</pubDate>
      <guid>https://databrickslabs.github.io/overwatch/deployoverwatch/configureoverwatch/validation/</guid>
      <description>&lt;p&gt;Many validations are performed to minimize mistakes. The following section offers details on all validations&#xA;done. Not all validation steps are done on all runs. All validations should be performed before a first run on any&#xA;given workspace. For more information on executing a full validation, see &lt;a href=&#34;&#34;&gt;below&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;validation-rules&#34;&gt;Validation rules&lt;/h3&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th style=&#34;text-align: left&#34;&gt;Name&lt;/th&gt;&#xA;          &lt;th style=&#34;text-align: left&#34;&gt;Validation Rule&lt;/th&gt;&#xA;          &lt;th style=&#34;text-align: left&#34;&gt;Impacted columns&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Api Url Validation&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;API URL should give some response with provided scope and key.&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;api_url&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Primordial data validation&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Primordial Date must be in &lt;strong&gt;yyyy-MM-dd format&lt;/strong&gt; (i.e. 2022-01-30 == Jan 30, 2022) and must be less than current date.&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;primordial_date&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Excluded scope validation&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Excluded scope must be null or in colon-delimited format and include only the following audit:sparkEvents:jobs:clusters:clusterEvents:notebooks:pools:accounts:dbsql&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;excluded_scopes&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Event Hub &lt;strong&gt;Shared Access Key&lt;/strong&gt; (Azure Only)&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Audit Log data must be recognized as present and readable from the provided Event Hub Configuration fields.&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;eh_name, eh_scope_key, secret_scope&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Event Hub &lt;strong&gt;AAD Auth&lt;/strong&gt; (Azure Only)&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Audit Log data must be recognized as present and readable from the provided Event Hub Configuration fields.&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;eh_name, eh_conn_string, aad_tenant_id, aad_client_id, aad_client_secret_key, aad_authority_endpoint&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Common consumer database name&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;All workspaces must have a common consumer database name.&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;consumer_database_name&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Common ETL database name&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;All workspaces must have a common ETL database name.&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;etl_database_name&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Storage prefix validation&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;All workspaces must share a single storage prefix and the Overwatch cluster must have appropriate access to read/write.&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;storage_prefix&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Cloud provider validation&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Either Azure or AWS.&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;cloud&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Max days validation&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Must Be a Number&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;max_days&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Secrete scope validation&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Secret scope must not be empty.&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;secret_scope&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;PAT key validation&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;DBPAT must not be empty and must be able to authenticate to the workspace.&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;secret_key_dbpat&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Audit log location validation &lt;strong&gt;(AWS/GCP ONLY)&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Audit logs must present immediately within the provided path and must be read accessible&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;auditlogprefix_source_path&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Mount point validation&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Workspaces with more than 50 mount points need to provide a csv file which will contain the mount point to source mapping. &lt;strong&gt;&lt;a href=&#34;https://databrickslabs.github.io/overwatch/dataengineer/advancedtopics//#exception---remote-workspaces-with-50-mounts&#34;&gt;click here for more details&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;mount_mapping_path&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;System Table Validation&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;System table should enabled and must have data for the workspace.&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;auditlogprefix_source_path&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;h3 id=&#34;run-the-validation&#34;&gt;Run the validation&lt;/h3&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-scala&#34; data-lang=&#34;scala&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;import&lt;/span&gt; com.databricks.labs.overwatch.MultiWorkspaceDeployment&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;val&lt;/span&gt; configCsvPath &lt;span style=&#34;color:#66d9ef&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;dbfs:/FileStore/overwatch/workspaceConfig.csv&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;// Path to the config.csv&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;// temp location which will be used as a temp storage. It will be automatically cleaned after each run.&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;val&lt;/span&gt; tempLocation &lt;span style=&#34;color:#66d9ef&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/tmp/overwatch/templocation&amp;#34;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;// number of workspaces to validate in parallel. Exceeding 20 may require larger drivers or additional cluster config considerations&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;// If total workspaces &amp;lt;= 20 recommend setting parallelism == to workspace count &#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;val&lt;/span&gt; parallelism &lt;span style=&#34;color:#66d9ef&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;// Run validation&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;MultiWorkspaceDeployment&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;configCsvPath&lt;span style=&#34;color:#f92672&#34;&gt;,&lt;/span&gt; tempLocation&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;validate&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;parallelism&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;review-the-report&#34;&gt;Review The Report&lt;/h3&gt;&#xA;&lt;p&gt;The validation report will be generated in &amp;lt;etl_storage_prefix&amp;gt;/report/validationReport as delta table.&#xA;Run the below query to check the validation report. All records should say validated = true or action is necessary prior&#xA;to deployment.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Upgrades</title>
      <link>https://databrickslabs.github.io/overwatch/dataengineer/upgrade/</link>
      <pubDate>Thu, 20 May 2021 21:27:44 -0400</pubDate>
      <guid>https://databrickslabs.github.io/overwatch/dataengineer/upgrade/</guid>
      <description>&lt;p&gt;Sometimes upgrading from one version to the next requires a schema change. In these cases, the&#xA;&lt;a href=&#34;https://databrickslabs.github.io/overwatch/changelog/&#34;&gt;CHANGELOG&lt;/a&gt; will be explicit. Upgrades MUST be executed WITH the &lt;strong&gt;new library (jar)&lt;/strong&gt; and&#xA;&lt;strong&gt;before the pipeline is executed&lt;/strong&gt;.&#xA;Basic pseudocode can be found below as a reference. For actual version upgrade scripts please reference the upgrade&#xA;scripts linked to your target version in the &lt;a href=&#34;https://databrickslabs.github.io/overwatch/changelog/&#34;&gt;Changelog&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;div class=&#34;notices warning&#34; &gt;&lt;p&gt;When a schema upgrade is required between versions, &lt;strong&gt;this step cannot be skipped&lt;/strong&gt;. Overwatch will not allow you&#xA;to continue on a version that requires a newer schema.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Pipeline Validation</title>
      <link>https://databrickslabs.github.io/overwatch/deployoverwatch/pipelinevalidation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://databrickslabs.github.io/overwatch/deployoverwatch/pipelinevalidation/</guid>
      <description>&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;&#xA;&lt;p&gt;Overwatch provides a special pipeline validation module to perform a&#xA;number of data quality, data completeness, and referential integrity&#xA;checks on its output.  Pipeline validation may run as frequently as&#xA;the regularly scheduled, automated Overwatch ETL pipeline&#xA;runs/deployments or manually as a notebook command.  It is used by the&#xA;Overwatch maintainers to guard against data quality regressions when&#xA;extending and enhancing Overwatch&amp;rsquo;s internal logic.  Similarly, we are&#xA;encouraging users to apply this tool to their deployments to provide&#xA;data quality readouts and to assist with customer issue resolution.&#xA;This page describes its functionality in detail and how to add a&#xA;pipeline validation step to an Overwatch workflow.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Sharing Overwatch Data</title>
      <link>https://databrickslabs.github.io/overwatch/deployoverwatch/sharingoverwatch/</link>
      <pubDate>Thu, 15 Dec 2022 18:31:01 -0500</pubDate>
      <guid>https://databrickslabs.github.io/overwatch/deployoverwatch/sharingoverwatch/</guid>
      <description>&lt;h2 id=&#34;granting-access-via-dbsql&#34;&gt;Granting Access Via DBSQL&lt;/h2&gt;&#xA;&lt;p&gt;If your storage prefix is referencing a mount point, nothing should be necessary here as the mount point is already&#xA;accessible to all workspace users. If using a direct path (i.e. s3://, abfss://, or gs://) you will need to configure&#xA;the appropriate access in Admin Settings &amp;ndash;&amp;gt; SQL Warehouse Settings.&lt;/p&gt;&#xA;&lt;p&gt;If using a &lt;strong&gt;Hive Metastore&lt;/strong&gt; you may still need to review your grants to ensure users have access to the Overwatch&#xA;tables / views. Additionally, you may need to grant select on any file to the appropriate user groups if direct path&#xA;is used for overwatch instead of a mount point.&#xA;&lt;code&gt;GRANT SELECT ON ANY FILE TO `&amp;lt;user&amp;gt;@&amp;lt;domain-name&amp;gt;` &lt;/code&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Productionizing</title>
      <link>https://databrickslabs.github.io/overwatch/dataengineer/productionizing/</link>
      <pubDate>Wed, 20 Jul 2022 15:03:23 -0400</pubDate>
      <guid>https://databrickslabs.github.io/overwatch/dataengineer/productionizing/</guid>
      <description>&lt;h2 id=&#34;moving-to-production&#34;&gt;Moving To Production&lt;/h2&gt;&#xA;&lt;p&gt;When you&amp;rsquo;re ready to move to production, there are a few things to keep in mind and best practices to follow to&#xA;get the most out of Overwatch&lt;/p&gt;&#xA;&lt;h3 id=&#34;cluster-logging&#34;&gt;Cluster Logging&lt;/h3&gt;&#xA;&lt;p&gt;&lt;strong&gt;Simplify and Unify your cluster logging directories&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Many users forget to enable cluster logging and without it Overwatch cannot provide usage telemetry by notebook,&#xA;job, user so it&amp;rsquo;s critical that all clusters have clusters logs enabled&lt;/li&gt;&#xA;&lt;li&gt;If users are allowed to create clusters/jobs without any governance, log files will be produced and stored all&#xA;over the place. These will be very challenging to clean up and can eat up a significant amount of storage over time.&lt;/li&gt;&#xA;&lt;li&gt;Utilize cluster policies to ensure the logging location is always set and done so consistently&lt;/li&gt;&#xA;&lt;li&gt;Set up a lifecycle policy (i.e. TTL) on your cluster logging directory in your cloud storage so the logs don&amp;rsquo;t&#xA;pile up indefinitely. Suggested time to live time is 30 days.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;backups&#34;&gt;Backups&lt;/h3&gt;&#xA;&lt;h4 id=&#34;this-will-be-deprecated-from-version-721please-refer-to-the-snapshot-section-for-the-new-way-to-do-backups&#34;&gt;(This will be Deprecated from version 7.2.1.Please refer to the Snapshot section for the new way to do backups)&lt;/h4&gt;&#xA;&lt;p&gt;&lt;strong&gt;Perform Bronze Backups&lt;/strong&gt;&#xA;I know we don&amp;rsquo;t hear a lot about backups in big data world but often times the cluster logs and / or&#xA;the audit logs are transient (especially Azure deployments as Event Hub only maintains 7 days). This means that if&#xA;something happened to the bronze data your Overwatch history could be lost forever. To guard against this it&amp;rsquo;s&#xA;strongly recommended that you periodically backup the Bronze data. As of 0.6.1.1 this has been made very easy through&#xA;the &lt;code&gt;snapshot&lt;/code&gt; helper function.&lt;/p&gt;</description>
    </item>
    <item>
      <title>AdvancedTopics</title>
      <link>https://databrickslabs.github.io/overwatch/dataengineer/advancedtopics/</link>
      <pubDate>Mon, 12 Dec 2022 11:41:13 -0500</pubDate>
      <guid>https://databrickslabs.github.io/overwatch/dataengineer/advancedtopics/</guid>
      <description>&lt;h2 id=&#34;quick-reference&#34;&gt;Quick Reference&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#externalize-optimize--z-order-as-of-060&#34;&gt;Externalize Optimize &amp;amp; Z-Order&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#interacting-with-overwatch-and-its-state&#34;&gt;Interacting With Overwatch and its State&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#optimizing-overwatch&#34;&gt;Optimizing Overwatch&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#getting-a-strong-first-run&#34;&gt;Maximizing First Run Potential&lt;/a&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#historical-loading&#34;&gt;Historical Loads&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#cluster-logs-ingest-details&#34;&gt;Cluster Logs Ingest Details&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#joining-with-slow-changing-dimensions-scd&#34;&gt;Joining With Slow Changing Dimensions (SCD)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;optimizing-overwatch&#34;&gt;Optimizing Overwatch&lt;/h2&gt;&#xA;&lt;p&gt;&lt;strong&gt;Expectation Check&lt;/strong&gt; Note that Overwatch analyzes nearly all aspects of the Workspace and manages its own pipeline&#xA;among many other tasks. This results in 1000s of spark job executions and as such, the Overwatch job will take some&#xA;time to run. For small/medium workspaces, 20-40 minutes should be expected for each run. Larger workspaces can take&#xA;longer, depending on the size of the cluster, the Overwatch configuration, and the workspace.&lt;/p&gt;</description>
    </item>
    <item>
      <title>System Table Configuration</title>
      <link>https://databrickslabs.github.io/overwatch/deployoverwatch/systemtableintegration/systemtableconfiguration/</link>
      <pubDate>Tue, 23 Jan 2024 16:30:21 +0530</pubDate>
      <guid>https://databrickslabs.github.io/overwatch/deployoverwatch/systemtableintegration/systemtableconfiguration/</guid>
      <description>&lt;h2 id=&#34;configuration-changes-required-for-system-table-integration&#34;&gt;Configuration changes required for System Table Integration&lt;/h2&gt;&#xA;&lt;p&gt;There is no unique configuration process for System Table. The only change required to the configuration for integration&#xA;is below -&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;auditlogprefix_source_path&lt;/strong&gt; - Instead of adding a fully qualified path (s3 or GC) for auditlog,&#xA;add keyword &lt;strong&gt;system&lt;/strong&gt; in this column. This will enable the system table integration.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;For all other configurations, please follow the &lt;a href=&#34;https://databrickslabs.github.io/overwatch/deployoverwatch/configureoverwatch/configuration/&#34;&gt;Configuration&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;div class=&#34;notices note&#34; &gt;&lt;p&gt;Once the customer migrate to system table (version 0.8.0.0), they cannot revert back to legacy Deployment.&lt;/p&gt;</description>
    </item>
    <item>
      <title>System Table Pre-Requisites</title>
      <link>https://databrickslabs.github.io/overwatch/deployoverwatch/systemtableintegration/systemtableprereq/</link>
      <pubDate>Tue, 23 Jan 2024 16:27:22 +0530</pubDate>
      <guid>https://databrickslabs.github.io/overwatch/deployoverwatch/systemtableintegration/systemtableprereq/</guid>
      <description>&lt;p&gt;This section will walk you through the steps necessary as a prerequisite for System Table Integration with Overwatch.&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;Workspace&lt;/strong&gt; should be &lt;strong&gt;UC enabled&lt;/strong&gt; and &lt;strong&gt;System Table enabled&lt;/strong&gt;. Enabling system tables is straightforward,&#xA;you do need to have at least one unity-catalog enabled workspace. System tables must be enabled by an account admin.&#xA;You can enable system tables using the Unity Catalog REST API.  Once enabled, the tables will appear in a catalog&#xA;called system, which is included in every Unity Catalog metastore. In the system catalog youâ€™ll see schemas such as&#xA;access and billing that contain the system tables. Follow the documents to enable&#xA;&lt;a href=&#34;https://docs.databricks.com/en/administration-guide/system-tables/index.html#enable-system-tables&#34;&gt;System Tables&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>GCP</title>
      <link>https://databrickslabs.github.io/overwatch/deployoverwatch/cloudinfra/gcp/</link>
      <pubDate>Thu, 07 Sep 2023 00:00:00 +0000</pubDate>
      <guid>https://databrickslabs.github.io/overwatch/deployoverwatch/cloudinfra/gcp/</guid>
      <description>&lt;h2 id=&#34;configuring-overwatch-on-gcp---databricks&#34;&gt;Configuring Overwatch on GCP - Databricks&lt;/h2&gt;&#xA;&lt;p&gt;Reach out to your Databricks representative to help you with these tasks as needed.&lt;/p&gt;&#xA;&lt;p&gt;There are two primary sources of data that need to be configured:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://docs.gcp.databricks.com/administration-guide/account-settings/audit-logs.html&#34;&gt;Audit Logs-GCP&lt;/a&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;These will be delivered to the configured bucket. These buckets are configured on a per-workspace basis&#xA;and can be delivered to the same target bucket, just ensure that the prefixes are different to avoid collisions.&#xA;We don&amp;rsquo;t want multiple workspaces delivering into the same prefix. The audit logs contain data for every interaction&#xA;within the environment and are used to track the state of various objects through time along with which accounts&#xA;interacted with them. This data is relatively small and delivery occurs infrequently which is why it&amp;rsquo;s&#xA;rarely of any consequence to deliver audit logs to buckets even outside of the control plane region.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Cluster Logs - Crucial to get the most out of Overwatch&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Cluster logs delivery location is configured in the cluster spec &amp;ndash;&amp;gt; Advanced Options &amp;ndash;&amp;gt; Logging. These logs can&#xA;get quite large and they are stored in a very inefficient format for query and long-term storage. As such, it&amp;rsquo;s&#xA;crucial to store these logs in the same region as the worker nodes for best results. Additionally, using dedicated&#xA;buckets provides more flexibility when configuring TTL (time-to-live) to minimize long-term, unnecessary costs.&#xA;It&amp;rsquo;s not recommended to store these on DBFS directly (dbfs mount points are ok).&lt;/li&gt;&#xA;&lt;li&gt;Best Practice - Multi-Workspace &amp;ndash; When multiple workspaces are using Overwatch within a single region it&amp;rsquo;s best to&#xA;ensure that each are going to their own prefix, even if sharing a bucket. This greatly reduces Overwatch scan times&#xA;as the log files build up.&lt;/li&gt;&#xA;&lt;li&gt;To enable Cluster Logs on Multiworkspace - follow this &lt;a href=&#34;https://databrickslabs.github.io/overwatch/deployoverwatch/ucedeployment/cloudstorageaccessrequirements/gcp//#cluster-logging-locations-setup&#34;&gt;link&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;div class=&#34;notices note&#34; &gt;&lt;p&gt;&lt;strong&gt;GCP &amp;ndash; Remote Cluster Logs&lt;/strong&gt; - Databricks on GCP, does not support mounted/GCS bucket locations. Customers must&#xA;provide DBFS root path as a target for log delivery.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Azure</title>
      <link>https://databrickslabs.github.io/overwatch/deployoverwatch/cloudinfra/azure/</link>
      <pubDate>Mon, 12 Dec 2022 11:29:59 -0500</pubDate>
      <guid>https://databrickslabs.github.io/overwatch/deployoverwatch/cloudinfra/azure/</guid>
      <description>&lt;h2 id=&#34;fast-travel&#34;&gt;Fast Travel&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#configuring-overwatch-on-azure-databricks&#34;&gt;Configuring Overwatch on Azure Databricks&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#reference-architecture&#34;&gt;Reference Architecture&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#audit-log-delivery-via-event-hub&#34;&gt;Configuring Audit Log Delivery Through Event Hub&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#setting-up-storage-accounts&#34;&gt;Setting up Storage Accounts&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://docs.databricks.com/data/data-sources/azure/adls-gen2/azure-datalake-gen2-sp-access.html&#34;&gt;Mount Storage Accounts&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;configuring-overwatch-on-azure-databricks&#34;&gt;Configuring Overwatch on Azure Databricks&lt;/h2&gt;&#xA;&lt;p&gt;Reach out to your Databricks representative to help you with these tasks as needed.&#xA;&lt;!-- raw HTML omitted --&gt;&#xA;To get started, we suggest you deploy a single workspace end to end so that you can figure out the steps involved and you&#xA;can then apply these for the other workspaces to be deployed.&lt;/p&gt;</description>
    </item>
    <item>
      <title>AWS</title>
      <link>https://databrickslabs.github.io/overwatch/deployoverwatch/cloudinfra/aws/</link>
      <pubDate>Mon, 12 Dec 2022 11:29:56 -0500</pubDate>
      <guid>https://databrickslabs.github.io/overwatch/deployoverwatch/cloudinfra/aws/</guid>
      <description>&lt;h2 id=&#34;configuring-overwatch-on-aws---databricks&#34;&gt;Configuring Overwatch on AWS - Databricks&lt;/h2&gt;&#xA;&lt;p&gt;Reach out to your Customer Success Engineer (CSE) to help you with these tasks as needed.&lt;/p&gt;&#xA;&lt;p&gt;There are two primary sources of data that need to be configured:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://docs.databricks.com/administration-guide/account-settings/audit-logs.html&#34;&gt;Audit Logs-AWS&lt;/a&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;The audit logs contain data for every interaction&#xA;within the environment and are used to track the state of various objects through time along with which accounts&#xA;interacted with them. This data is relatively small and delivery occurs infrequently which is why it&amp;rsquo;s&#xA;rarely of any consequence to deliver audit logs to buckets even outside of the control plane region.&lt;/li&gt;&#xA;&lt;li&gt;For ingesting this data, you have the option of using System tables (&lt;strong&gt;RECOMMENDED&lt;/strong&gt;) or you can configure the audit&#xA;logs to be delivered to the configured bucket. These buckets are configured on a per-workspace basis and can be&#xA;delivered to the same target bucket, just ensure that the prefixes are different to avoid collisions.&#xA;We don&amp;rsquo;t want multiple workspaces delivering into the same prefix.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Cluster Logs - Crucial to get the most out of Overwatch&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Cluster logs delivery location is configured in the cluster spec &amp;ndash;&amp;gt; Advanced Options &amp;ndash;&amp;gt; Logging. These logs can&#xA;get quite large and they are stored in a very inefficient format for query and long-term storage. As such, it&amp;rsquo;s&#xA;&lt;strong&gt;crucial&lt;/strong&gt; to store these logs in the same region as the worker nodes for best results. Additionally, using dedicated&#xA;buckets provides more flexibility when configuring TTL (time-to-live) to minimize long-term, unnecessary costs.&#xA;It&amp;rsquo;s not recommended to store these on DBFS directly.&lt;/li&gt;&#xA;&lt;li&gt;Best Practice - Multi-Workspace &amp;ndash; When multiple workspaces are using Overwatch within a single region it&amp;rsquo;s best to&#xA;ensure that each are going to their own prefix, even if sharing a storage account. This greatly reduces Overwatch scan times&#xA;as the log files build up. If scan times get too long, the TTL can be reduced or additional storage accounts can&#xA;be created to increase read IOPS throughput (rarely necessary) intra-region.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;reference-architecture&#34;&gt;Reference Architecture&lt;/h2&gt;&#xA;&lt;p&gt;As of 0.7.1 Overwatch can be deployed on a single workspace and retrieve data from one or more workspaces. For more details&#xA;on requirements see &lt;a href=&#34;https://databrickslabs.github.io/overwatch/deployoverwatch//#multi-workspace-monitoring---considerations&#34;&gt;Multi-Workspace Consideration&lt;/a&gt;.&#xA;There are many cases where some workspaces should be able to monitor many workspaces and others should only monitor&#xA;themselves. Additionally, co-location of the output data and who should be able to access what data also comes into play,&#xA;this reference architecture can accommodate all of these needs. To learn more about the details walk through the&#xA;&lt;a href=&#34;https://databrickslabs.github.io/overwatch/deployoverwatch/&#34;&gt;deployment steps&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Data Dictionary - 0.6.1.x</title>
      <link>https://databrickslabs.github.io/overwatch/dataengineer/definitions/061x/</link>
      <pubDate>Mon, 26 Sep 2022 08:47:24 -0400</pubDate>
      <guid>https://databrickslabs.github.io/overwatch/dataengineer/definitions/061x/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#consumption-layer-tables-views&#34;&gt;Consumption Layer&lt;/a&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#column-descriptions&#34;&gt;Column Descriptions&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#etl-tables&#34;&gt;ETL Tables&lt;/a&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#bronze&#34;&gt;Bronze&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#silver&#34;&gt;Silver&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#gold&#34;&gt;Gold&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;consumption-layer-tables-views&#34;&gt;Consumption Layer &amp;ldquo;Tables&amp;rdquo; (Views)&lt;/h2&gt;&#xA;&lt;p&gt;All end users should be hitting consumer tables first. Digging into lower layers gets significantly more complex.&#xA;Below is the data model for the consumption layer. The consumption layer is often in a stand-alone database apart&#xA;from the ETL tables to minimize clutter and confusion. These entities in this layer are actually not tables at all&#xA;(with a few minor exceptions such as lookup tables) but rather views. This allows for the Overwatch development team&#xA;to alter the underlying columns, names, types, and structures without breaking existing transformations. Instead,&#xA;view column names will remain the same but may be repointed to a newer version of a column, etc.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Data Dictionary - 0.7.1.x</title>
      <link>https://databrickslabs.github.io/overwatch/dataengineer/definitions/071x/</link>
      <pubDate>Mon, 26 Sep 2022 08:47:24 -0400</pubDate>
      <guid>https://databrickslabs.github.io/overwatch/dataengineer/definitions/071x/</guid>
      <description>&lt;h2 id=&#34;erd&#34;&gt;ERD&lt;/h2&gt;&#xA;&lt;p&gt;The &amp;ldquo;ERD&amp;rdquo; below is a visual representation of the &lt;a href=&#34;#consumption-layer-tables-views&#34;&gt;consumer layer&lt;/a&gt; data model.&#xA;Many of the joinable lines have been omitted to reduce&#xA;chaos and complexity in the visualization. All columns with the same name are joinable (even if there&amp;rsquo;s not a line&#xA;from one table to the other). The relations depicted are to call the analyst&amp;rsquo;s attention to less obvious joins.&lt;/p&gt;&#xA;&lt;p&gt;The goal is to present a data model that unifies the different parts of the platform. The Overwatch team will continue&#xA;to work with Databricks platform teams to publish and simplify this data. The gray boxes annotated as&#xA;&amp;ldquo;Backlog/Research&amp;rdquo; are simply a known gap and a pursuit of the Overwatch dev team, it does NOT mean it&amp;rsquo;s going to be&#xA;released soon but rather that we are aware of the missing component and we hope to enable gold-level data here in&#xA;the future.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Configuration Details By Version</title>
      <link>https://databrickslabs.github.io/overwatch/deployoverwatch/configureoverwatch/configdetailsbyversion/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://databrickslabs.github.io/overwatch/deployoverwatch/configureoverwatch/configdetailsbyversion/</guid>
      <description>&lt;h2 id=&#34;overwatch-deployment-configuration-by-version&#34;&gt;Overwatch Deployment Configuration By Version&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#column-description-08xx&#34;&gt;0.8.x.x Configuration&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#column-description-072x&#34;&gt;0.7.2.x Configuration&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#column-description-071x&#34;&gt;0.7.1.x Configuration&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;column-description-08xx&#34;&gt;Column description 0.8.x.x&lt;/h3&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th style=&#34;text-align: left&#34;&gt;Column&lt;/th&gt;&#xA;          &lt;th style=&#34;text-align: left&#34;&gt;Type&lt;/th&gt;&#xA;          &lt;th style=&#34;text-align: left&#34;&gt;IsRequired&lt;/th&gt;&#xA;          &lt;th style=&#34;text-align: left&#34;&gt;Description&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;workspace_name&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;String&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;True&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Name of the workspace.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;workspace_id&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;String&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;True&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Id of the workspace. &lt;strong&gt;MUST BE VALUE AFTER THE o=&lt;/strong&gt; in the URL bar. To ensure you get the right value, run the following on the target workspace. &lt;code&gt;Initializer.getOrgId&lt;/code&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;workspace_url&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;String&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;True&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;URL of the workspace. Should be in format of &lt;em&gt;https://*.com&lt;/em&gt; or &lt;em&gt;https://*.net&lt;/em&gt;. Don&amp;rsquo;t include anything after the .com or .net suffix&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;api_url&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;String&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;True&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;API URL for the Workspace (execute in scala &lt;code&gt;dbutils.notebook.getContext().apiUrl.get&lt;/code&gt; &lt;strong&gt;ON THE TARGET WORKSPACE NOT DEPLOYMENT WORKSPACE&lt;/strong&gt; to get the API URL for the workspace. NOTE: Workspace_URL and API_URL can be different for a workspace but may be the same even for multiple workspaces). You can also use the workspace_url here.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;cloud&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;String&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;True&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Cloud provider (Azure/AWS/GCP).&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;primordial_date&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;String&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;True&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;The date from which Overwatch will capture the details. The &lt;strong&gt;format&lt;/strong&gt; should be &lt;strong&gt;yyyy-MM-dd&lt;/strong&gt; ex: 2022-05-20 == May 20 2022. **IMPORTANT NOTE: ** You should only set the primordial date in the initial run of Overwatch, and never change it again, as Overwatch will progress the dates using it&amp;rsquo;s own calculations and checkpoints.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;storage_prefix&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;String&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;True&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;CASE SENSITIVE - Lower Case&lt;/strong&gt; The location in which Overwatch will store the data. You can think of this as the Overwatch working directory. dbfs:/mnt/path/&amp;hellip; or abfss://container@myStorageAccount.dfs.core.windows.net/&amp;hellip; or s3://myBucket/&amp;hellip;   or gs://myBucket/&amp;hellip;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;etl_database_name&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;String&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;True&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;The name of the ETL data base for Overwatch (i.e. overwatch_etl or custom)&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;consumer_database_name&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;String&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;True&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;The name of the Consumer database for Overwatch. (i.e. overwatch or custom)&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;secret_scope&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;String&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;True&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Name of the secret scope. This must be created on the workspace which the Overwatch job will execute.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;secret_key_dbpat&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;String&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;True&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;This will contain the PAT token of the workspace. The key should be present in the secret_scope and should start with the letters &lt;em&gt;dapi&lt;/em&gt;.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;auditlogprefix_source_path&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;String&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;True&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;For all clouds use keyword &lt;strong&gt;system&lt;/strong&gt; to fetch data from System Tables (system.access.audit) See &lt;a href=&#34;https://databrickslabs.github.io/overwatch/deployoverwatch/systemtableintegration/systemtableconfiguration/&#34;&gt;System Table Configuration Details&lt;/a&gt; for details. If you are not using System Tables, you can enter the location of the auditlogs (&lt;strong&gt;AWS/GCP&lt;/strong&gt; Only). The contents under this directory must have the folders with the date partitions like date=2022-12-0 .&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;interactive_dbu_price&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Double&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;True&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Contract (or list) Price for interactive DBUs. The provided template has the list prices by default.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;automated_dbu_price&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Double&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;True&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Contract (or list) Price for automated DBUs. The provided template has the list prices by default.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;sql_compute_dbu_price&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Double&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;True&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Contract (or list) Price for DBSQL DBUs. This should be the closest average price across your DBSQL Skus (classic / Pro / Serverless) for now. See &lt;a href=&#34;https://databrickslabs.github.io/overwatch/deployoverwatch/configureoverwatch/customcosts/&#34;&gt;Custom Costs&lt;/a&gt; for more details. The provided template has the DBSQL Classic list prices by default.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;jobs_light_dbu_price&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Double&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;True&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Contract (or list) Price for interactive DBUs. The provided template has the list prices by default.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;max_days&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Integer&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;True&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;This is the max incrementals days that will be loaded. Usually only relevant for historical loading and rebuilds. Recommendation == 30&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;excluded_scopes&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;String&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;False&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://databrickslabs.github.io/overwatch/dataengineer/modules//#scopes&#34;&gt;Scopes&lt;/a&gt; that should not be excluded from the pipelines. Since this is a CSV, it&amp;rsquo;s critical that these are &lt;strong&gt;colon delimited&lt;/strong&gt;. Leave blank if you&amp;rsquo;d like to load all overwatch scopes.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;active&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Boolean&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;True&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Whether or not the workspace should be validated / deployed.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;proxy_host&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;String&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;False&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Proxy url for the workspace.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;proxy_port&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;String&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;False&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Proxy port for the workspace&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;proxy_user_name&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;String&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;False&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Proxy user name for the workspace.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;proxy_password_scope&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;String&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;False&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Scope which contains the proxy password key.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;proxy_password_key&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;String&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;False&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Key which contains proxy password.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;success_batch_size&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Integer&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;False&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;API Tunable - Indicates the size of the buffer on filling of which the result will be written to a temp location. This is used to tune performance in certain circumstances. Leave default except for special circumstances. Default == 200&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;error_batch_size&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Integer&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;False&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;API Tunable - Indicates the size of the error writer buffer containing API call errors. This is used to tune performance in certain circumstances. Leave default except for special circumstances. Default == 500&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;enable_unsafe_SSL&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Boolean&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;False&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;API Tunable - Enables unsafe SSL. Default == False&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;thread_pool_size&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Integer&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;False&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;API Tunable - Max number of API calls Overwatch is allowed to make in parallel. Default == 4. Increase for faster bronze but if workspace is busy, risks API endpoint saturation. Overwatch will detect saturation and back-off when detected but for safety never go over 8 without testing.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;api_waiting_time&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Long&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;False&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;API Tunable - Overwatch makes async api calls in parallel, api_waiting_time signifies the max wait time in case of no response received from the api call. Default = 300000(5 minutes)&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;mount_mapping_path&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;String&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;False&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Path to local CSV holding details of all mounts on remote workspaces (only necessary for remote workspaces with &amp;gt;50 mounts) &lt;strong&gt;&lt;a href=&#34;https://databrickslabs.github.io/overwatch/dataengineer/advancedtopics//#exception---remote-workspaces-with-50-mounts&#34;&gt;click here for more details&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;temp_dir_path&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;String&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;False&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Custom temporary working directory, directory gets cleaned up after each run.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;h4 id=&#34;azure-event-hub-specific-configurations&#34;&gt;Azure Event Hub Specific Configurations&lt;/h4&gt;&#xA;&lt;p&gt;When configuring the Azure EH configurations users can use EITHER a &lt;strong&gt;shared access key&lt;/strong&gt; OR &lt;strong&gt;AAD SP as of 072x&lt;/strong&gt;&#xA;to authenticate to the EH. Below are the required configurations for each auth method. One of the options for Azure&#xA;deployments must be used as EH is required for Azure.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
