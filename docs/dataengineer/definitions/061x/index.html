<!DOCTYPE html>
<html lang="en" class="js csstransforms3d">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="generator" content="Hugo 0.76.5" />
    <meta name="description" content="Overwatch Documentation">
<meta name="author" content="Daniel Tomes -- DatabricksLabs">

    <link rel="icon" href="https://databrickslabs.github.io/overwatch/images/favicon.png" type="image/png">

    <title>Data Dictionary - 0.6.1.x :: Overwatch</title>

    
    <link href="https://databrickslabs.github.io/overwatch/css/nucleus.css?1665782191" rel="stylesheet">
    <link href="https://databrickslabs.github.io/overwatch/css/fontawesome-all.min.css?1665782191" rel="stylesheet">
    <link href="https://databrickslabs.github.io/overwatch/css/hybrid.css?1665782191" rel="stylesheet">
    <link href="https://databrickslabs.github.io/overwatch/css/featherlight.min.css?1665782191" rel="stylesheet">
    <link href="https://databrickslabs.github.io/overwatch/css/perfect-scrollbar.min.css?1665782191" rel="stylesheet">
    <link href="https://databrickslabs.github.io/overwatch/css/auto-complete.css?1665782191" rel="stylesheet">
    <link href="https://databrickslabs.github.io/overwatch/css/atom-one-dark-reasonable.css?1665782191" rel="stylesheet">
    <link href="https://databrickslabs.github.io/overwatch/css/theme.css?1665782191" rel="stylesheet">
    <link href="https://databrickslabs.github.io/overwatch/css/hugo-theme.css?1665782191" rel="stylesheet">
    
    <link href="https://databrickslabs.github.io/overwatch/css/theme-blue.css?1665782191" rel="stylesheet">
    
    

    <script src="https://databrickslabs.github.io/overwatch/js/jquery-3.3.1.min.js?1665782191"></script>

    <style>
      :root #header + #content > #left > #rlblock_left{
          display:none !important;
      }
      
    </style>
    

<script async defer src="https://buttons.github.io/buttons.js"></script>
  </head>
  <body class="" data-url="https://databrickslabs.github.io/overwatch/dataengineer/definitions/061x/">
    <nav id="sidebar" class="">



  <div id="header-wrapper">
    <div id="header">
      <a id="logo" href="https://databrickslabs.github.io/overwatch/">
    <img src="https://databrickslabs.github.io/overwatch/images/tmp_logo4.png", width="85%">
</a>

    </div>
    
        <div class="searchbox">
    <label for="search-by"><i class="fas fa-search"></i></label>
    <input data-search-input id="search-by" type="search" placeholder="Search...">
    <span data-search-clear=""><i class="fas fa-times"></i></span>
</div>

<script type="text/javascript" src="https://databrickslabs.github.io/overwatch/js/lunr.min.js?1665782191"></script>
<script type="text/javascript" src="https://databrickslabs.github.io/overwatch/js/auto-complete.js?1665782191"></script>
<script type="text/javascript">
    
        var baseurl = "https:\/\/databrickslabs.github.io\/overwatch\/";
    
</script>
<script type="text/javascript" src="https://databrickslabs.github.io/overwatch/js/search.js?1665782191"></script>

    
  </div>
  
  <section id="homelinks">
    <ul>
      <li>
        <a class="padding" href='https://databrickslabs.github.io/overwatch/'><i class='fas fa-home'></i> Home</a>
      </li>
    </ul>
  </section>
  

    <div class="highlightable">
    <ul class="topics">

        
          
          




 
  
    
    <li data-nav-id="/gettingstarted/" title="Getting Started" class="dd-item
        
        
        
        ">
      <a href="https://databrickslabs.github.io/overwatch/gettingstarted/">
          Getting Started
          
      </a>
      
      
        <ul>
          
          
          

        
          
            
            




 
  
    
      <li data-nav-id="/gettingstarted/configuration/" title="Configuration" class="dd-item ">
        <a href="https://databrickslabs.github.io/overwatch/gettingstarted/configuration/">
        Configuration
        
        </a>
    </li>
     
  
 

            
          
            
            




 
  
    
      <li data-nav-id="/gettingstarted/modules/" title="Modules" class="dd-item ">
        <a href="https://databrickslabs.github.io/overwatch/gettingstarted/modules/">
        Modules
        
        </a>
    </li>
     
  
 

            
          
            
            




 
  
    
      <li data-nav-id="/gettingstarted/advancedtopics/" title="Advanced Topics" class="dd-item ">
        <a href="https://databrickslabs.github.io/overwatch/gettingstarted/advancedtopics/">
        Advanced Topics
        
        </a>
    </li>
     
  
 

            
          
        
        </ul>
      
    </li>
  
 

          
          




 
  
    
    <li data-nav-id="/environmentsetup/" title="Environment Setup" class="dd-item
        
        
        
        ">
      <a href="https://databrickslabs.github.io/overwatch/environmentsetup/">
          Environment Setup
          
      </a>
      
      
        <ul>
          
          
          

        
          
            
            




 
  
    
      <li data-nav-id="/environmentsetup/azure/" title="Azure" class="dd-item ">
        <a href="https://databrickslabs.github.io/overwatch/environmentsetup/azure/">
        Azure
        
        </a>
    </li>
     
  
 

            
          
            
            




 
  
    
      <li data-nav-id="/environmentsetup/aws/" title="AWS" class="dd-item ">
        <a href="https://databrickslabs.github.io/overwatch/environmentsetup/aws/">
        AWS
        
        </a>
    </li>
     
  
 

            
          
        
        </ul>
      
    </li>
  
 

          
          




 
  
    
    <li data-nav-id="/dataengineer/" title="Data Engineering" class="dd-item
        parent
        
        
        ">
      <a href="https://databrickslabs.github.io/overwatch/dataengineer/">
          Data Engineering
          
      </a>
      
      
        <ul>
          
          
            
          
          

        
          
            
            




 
  
    
    <li data-nav-id="/dataengineer/definitions/" title="Data Dictionary (Latest)" class="dd-item
        parent
        
        
        ">
      <a href="https://databrickslabs.github.io/overwatch/dataengineer/definitions/">
          Data Dictionary (Latest)
          
      </a>
      
      
        <ul>
          
          
          

        
          
            
            




 
  
    
      <li data-nav-id="/dataengineer/definitions/061x/" title="Data Dictionary - 0.6.1.x" class="dd-item active">
        <a href="https://databrickslabs.github.io/overwatch/dataengineer/definitions/061x/">
        Data Dictionary - 0.6.1.x
        
        </a>
    </li>
     
  
 

            
          
        
        </ul>
      
    </li>
  
 

            
          
            
            




 
  
    
      <li data-nav-id="/dataengineer/pipeline_management/" title="Pipeline_Management" class="dd-item ">
        <a href="https://databrickslabs.github.io/overwatch/dataengineer/pipeline_management/">
        Pipeline_Management
        
        </a>
    </li>
     
  
 

            
          
            
            




 
  
    
      <li data-nav-id="/dataengineer/upgrade/" title="Upgrade" class="dd-item ">
        <a href="https://databrickslabs.github.io/overwatch/dataengineer/upgrade/">
        Upgrade
        
        </a>
    </li>
     
  
 

            
          
            
            




 
  
    
      <li data-nav-id="/dataengineer/productionizing/" title="Productionizing" class="dd-item ">
        <a href="https://databrickslabs.github.io/overwatch/dataengineer/productionizing/">
        Productionizing
        
        </a>
    </li>
     
  
 

            
          
        
        </ul>
      
    </li>
  
 

          
          




 
  
    
    <li data-nav-id="/changelog/" title="ChangeLog" class="dd-item
        
        
        
        ">
      <a href="https://databrickslabs.github.io/overwatch/changelog/">
          ChangeLog
          
      </a>
      
      
    </li>
  
 

          
          




 
  
    
    <li data-nav-id="/faq/" title="FAQ" class="dd-item
        
        
        
        ">
      <a href="https://databrickslabs.github.io/overwatch/faq/">
          FAQ
          
      </a>
      
      
    </li>
  
 

          
          




 
  
    
    <li data-nav-id="/contributing/" title="" class="dd-item
        
        
        
        ">
      <a href="https://databrickslabs.github.io/overwatch/contributing/">
          
          
      </a>
      
      
    </li>
  
 

          
        
    </ul>

    
    

    
    <section id="footer">
      <center>

    
    <a class="github-button" href="https://github.com/databrickslabs/overwatch/subscription"
       data-icon="octicon-eye" data-show-count="true" aria-label="Watch Overwatch on GitHub">Watch</a>


    <a class="github-button" href="https://github.com/databrickslabs/overwatch" data-icon="octicon-star" data-show-count="true" aria-label="Star databrickslabs/overwatch on GitHub">Star</a>


    <a class="github-button" href="https://github.com/databrickslabs/overwatch/fork"
       data-icon="octicon-repo-forked" data-show-count="true" aria-label="Fork Overwatch on GitHub">Fork</a>

    <p>A <a href="https://github.com/databrickslabs">Databricks Labs</a> project.</p>
</center>

<script async defer src="https://buttons.github.io/buttons.js"></script>
    </section>
  </div>
</nav>




        <section id="body">
        <div id="overlay"></div>
        <div class="padding highlightable">
              
              <div>
                <div id="top-bar">
                
                
                <div id="breadcrumbs" itemscope="" itemtype="http://data-vocabulary.org/Breadcrumb">
                    <span id="sidebar-toggle-span">
                        <a href="#" id="sidebar-toggle" data-sidebar-toggle="">
                          <i class="fas fa-bars"></i>
                        </a>
                    </span>
                  
                  <span id="toc-menu"><i class="fas fa-list-alt"></i></span>
                  
                  <span class="links">
                 
                 
                    
          
          
            
            
          
          
            
            
          
          
            
            
          
          
            <a href='https://databrickslabs.github.io/overwatch/'>Welcome To OverWatch</a> > <a href='https://databrickslabs.github.io/overwatch/dataengineer/'>Data Engineering</a> > <a href='https://databrickslabs.github.io/overwatch/dataengineer/definitions/'>Data Dictionary (Latest)</a> > Data Dictionary - 0.6.1.x
          
        
          
        
          
        
          
        
                 
                  </span>
                </div>
                
                    <div class="progress">
    <div class="wrapper">
<nav id="TableOfContents">
  <ul>
    <li><a href="#consumption-layer-tables-views">Consumption Layer &ldquo;Tables&rdquo; (Views)</a>
      <ul>
        <li><a href="#data-organization">Data Organization</a></li>
        <li><a href="#column-descriptions">Column Descriptions</a></li>
      </ul>
    </li>
    <li><a href="#etl-tables">ETL Tables</a>
      <ul>
        <li><a href="#bronze">Bronze</a></li>
        <li><a href="#silver">Silver</a></li>
        <li><a href="#gold">Gold</a></li>
      </ul>
    </li>
  </ul>
</nav>
    </div>
</div>

                
              </div>
            </div>
            
        <div id="head-tags">
        
        </div>
        
        <div id="body-inner">
          
            <h1>
              
              Data Dictionary - 0.6.1.x
            </h1>
          

        


<ul>
<li><a href="#consumption-layer-tables-views">Consumption Layer</a>
<ul>
<li><a href="#column-descriptions">Column Descriptions</a></li>
</ul>
</li>
<li><a href="#etl-tables">ETL Tables</a>
<ul>
<li><a href="#bronze">Bronze</a></li>
<li><a href="#silver">Silver</a></li>
<li><a href="#gold">Gold</a></li>
</ul>
</li>
</ul>
<h2 id="consumption-layer-tables-views">Consumption Layer &ldquo;Tables&rdquo; (Views)</h2>
<p>All end users should be hitting consumer tables first. Digging into lower layers gets significantly more complex.
Below is the data model for the consumption layer. The consumption layer is often in a stand-alone database apart
from the ETL tables to minimize clutter and confusion. These entities in this layer are actually not tables at all
(with a few minor exceptions such as lookup tables) but rather views. This allows for the Overwatch development team
to alter the underlying columns, names, types, and structures without breaking existing transformations. Instead,
view column names will remain the same but may be repointed to a newer version of a column, etc.</p>

<div class="notices note" ><p>ETL should not be developed atop the consumption layer views but rather the gold layer. Before Overwatch version
upgrades, it&rsquo;s important that the engineering team review the change list and upgrade requirements before upgrading.
These upgrades may require a remap depending on the changes. As of version 1.0 release, all columns in the gold layer
will be underscored with their schema version number, column changes will reference the later release version but the
views published with Overwatch will almost always point to the latest version of each column and will not include the
schema suffix to simplify the data model for the average consumer.</p>
</div>

<h3 id="data-organization">Data Organization</h3>
<p>The large gray boxes in the simplified ERD below depict the two major, logical sections of the data model:</p>
<ul>
<li><strong>Databricks Platform</strong> - Metadata captured by the Databricks platform that can be used to assist in workspace
governance. This data can also be enriched with the Spark data enabling in-depth analyses. The breadth of metadata
is continuing to grow, stay tuned for additional capabilities.</li>
<li><strong>Spark UI</strong> The spark UI section is derived from the spark event logs and essentially contains every single piece
of data from the Spark UI. There are a few sections that are not included in the first release but the data is
present in <em>spark_events_bronze</em> albeit extremely complex to derive. The Overwatch development team is working
tirelessly to expose additional SparkUI data and will publish as soon as it&rsquo;s ready.</li>
</ul>
<p><img src="https://databrickslabs.github.io/overwatch/images/_index/Overwatch_Gold_61x.png" alt="OverwatchERD"></p>
<h3 id="column-descriptions">Column Descriptions</h3>
<p>Complete column descriptions are only provided for the consumption layer. The entity names are linked below.</p>
<ul>
<li><a href="#cluster">cluster</a></li>
<li><a href="#clusterstatefact">clusterStateFact</a></li>
<li><a href="#instancedetails">instanceDetails</a></li>
<li><a href="#job">job</a></li>
<li><a href="#jobrun">jobrun</a></li>
<li><a href="#jobruncostpotentialfact">jobRunCostPotentialFact</a></li>
<li><a href="#notebook">notebook</a></li>
<li><a href="#instancepool">instancePool</a></li>
<li><a href="#dbucostdetails">dbuCostDetail</a></li>
<li><a href="#accountlogin">accountLogin</a></li>
<li><a href="#accountmod">accountMod</a></li>
<li><a href="#sparkexecution">sparkExecution</a></li>
<li><a href="#sparkexecutor">sparkExecutor</a></li>
<li><a href="#sparkjob">sparkJob</a></li>
<li><a href="#sparkstage">sparkStage</a></li>
<li><a href="#sparktask">sparkTask</a></li>
<li><a href="#sparkstream_preview">sparkStream</a> **preview</li>
<li><a href="#common-meta-fields">Common Meta Fields</a>
<ul>
<li>There are several fields that are present in all tables. Instead of cluttering each table with them, this section
was created as a reference to each of these.</li>
</ul>
</li>
</ul>

<div class="notices info" ><p>Most tables below provide a data <strong>SAMPLE</strong> for reference. You may either click to view it or
right click the SAMPLE link and click saveTargetAs or saveLinkAs and save the file.
Note that these files are <strong>TAB</strong> delimited, so you will need to view as such if you save to local file.
The data in the files were generated from an Azure, test deployment created by Overwatch Developers.</p>
</div>

<h4 id="cluster">Cluster</h4>
<p><a href="https://databrickslabs.github.io/overwatch/assets/TableSamples/cluster.tab"><strong>SAMPLE</strong></a></p>
<p><strong>KEY</strong> &ndash; organization_id + cluster_id + unixTimeMS</p>
<p><strong>Incremental Columns</strong> &ndash; unixTimeMS</p>
<p><strong>Partition Columns</strong> &ndash; organization_id</p>
<p><strong>Write Mode</strong> &ndash; Append</p>
<table>
<thead>
<tr>
<th style="text-align:left">Column</th>
<th style="text-align:left">Type</th>
<th style="text-align:left">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">cluster_id</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Canonical Databricks cluster ID (more info in <a href="#common-meta-fields">Common Meta Fields</a>)</td>
</tr>
<tr>
<td style="text-align:left">action</td>
<td style="text-align:left">string</td>
<td style="text-align:left">create, edit, or snapImpute &ndash; depicts the type of action for the cluster &ndash; **snapImpute is used on first run to initialize the state of the cluster even if it wasn&rsquo;t created/edited since audit logs began</td>
</tr>
<tr>
<td style="text-align:left">timestamp</td>
<td style="text-align:left">timestamp</td>
<td style="text-align:left">timestamp the action took place</td>
</tr>
<tr>
<td style="text-align:left">cluster_name</td>
<td style="text-align:left">string</td>
<td style="text-align:left">user-defined name of the cluster</td>
</tr>
<tr>
<td style="text-align:left">driver_node_type</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Canonical name of the driver node type.</td>
</tr>
<tr>
<td style="text-align:left">node_type</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Canonical name of the worker node type.</td>
</tr>
<tr>
<td style="text-align:left">num_workers</td>
<td style="text-align:left">int</td>
<td style="text-align:left">The number of workers defined WHEN autoscaling is disabled</td>
</tr>
<tr>
<td style="text-align:left">autoscale</td>
<td style="text-align:left">struct</td>
<td style="text-align:left">The min/max workers defined WHEN autoscaling is enabled</td>
</tr>
<tr>
<td style="text-align:left">auto_termination_minutes</td>
<td style="text-align:left">int</td>
<td style="text-align:left">The number of minutes before the cluster auto-terminates due to inactivity</td>
</tr>
<tr>
<td style="text-align:left">enable_elastic_disk</td>
<td style="text-align:left">boolean</td>
<td style="text-align:left">Whether autoscaling disk was enabled or not</td>
</tr>
<tr>
<td style="text-align:left">is_automated</td>
<td style="text-align:left">booelan</td>
<td style="text-align:left">Whether the cluster is automated (true if automated false if interactive)</td>
</tr>
<tr>
<td style="text-align:left">cluster_type</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Type of cluster (i.e. Serverless, SQL Analytics, Single Node, Standard)</td>
</tr>
<tr>
<td style="text-align:left">security_profile</td>
<td style="text-align:left">struct</td>
<td style="text-align:left">Complex type to describe secrity features enabled on the cluster. More information <a href="">Below</a></td>
</tr>
<tr>
<td style="text-align:left">cluster_log_conf</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Logging directory if configured</td>
</tr>
<tr>
<td style="text-align:left">init_script</td>
<td style="text-align:left">array<!-- raw HTML omitted --></td>
<td style="text-align:left">Array of init scripts</td>
</tr>
<tr>
<td style="text-align:left">custom_tags</td>
<td style="text-align:left">string</td>
<td style="text-align:left">User-Defined tags AND also includes Databricks JobID and Databricks RunName when the cluster is created by a Databricks Job as an automated cluster. Other Databricks services that create clusters also store unique information here such as SqlEndpointID when a cluster is created by &ldquo;SqlAnalytics&rdquo;</td>
</tr>
<tr>
<td style="text-align:left">cluster_source</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Shows the source of the action **(TODO &ndash; checking on why null scenario with BUI)</td>
</tr>
<tr>
<td style="text-align:left">spark_env_vars</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Spark environment variables defined on the cluster</td>
</tr>
<tr>
<td style="text-align:left">spark_conf</td>
<td style="text-align:left">string</td>
<td style="text-align:left">custom spark configuration on the cluster that deviate from default</td>
</tr>
<tr>
<td style="text-align:left">acl_path_prefix</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Automated jobs pass acl to clusters via a path format, the path is defined here</td>
</tr>
<tr>
<td style="text-align:left">instance_pool_id</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Canononical pool id from which workers receive nodes</td>
</tr>
<tr>
<td style="text-align:left">driver_instance_pool_id</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Canononical pool id from which driver receives node</td>
</tr>
<tr>
<td style="text-align:left">instance_pool_name</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Name of pool from which workers receive nodes</td>
</tr>
<tr>
<td style="text-align:left">driver_instance_pool_name</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Name of pool from which driver receives node</td>
</tr>
<tr>
<td style="text-align:left">spark_version</td>
<td style="text-align:left">string</td>
<td style="text-align:left">DBR version - scala version</td>
</tr>
<tr>
<td style="text-align:left">idempotency_token</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Idempotent jobs token if used</td>
</tr>
</tbody>
</table>
<h4 id="clusterstatefact">ClusterStateFact</h4>
<p><a href="https://databrickslabs.github.io/overwatch/assets/TableSamples/clusterstatefact.tab"><strong>SAMPLE</strong></a></p>
<p><strong>KEY</strong> &ndash; organization_id + cluster_id + state + unixTimeMS_state_start</p>
<p><strong>Incremental Columns</strong> &ndash; state_start_date + unixTimeMS</p>
<p><strong>Partition Columns</strong> &ndash; organization_id + state_start_date</p>
<p><strong>Z-Order Columns</strong> &ndash; cluster_id + unixTimeMS_state_start</p>
<p><strong>Write Mode</strong> &ndash; Merge</p>
<p>Costs and state details by cluster at every state in the cluster lifecycle.</p>
<p><strong>NOTE</strong> This fact table is not normalized on time. Some states will span multiple days and must be smoothed across
days (i.e. divide by days_in_state) when trying to calculate costs by day. All states are force-terminated at the
end of the Overwatch run to the until-timestamp of the run. If the state was still active at this time, it will be
updated on the subsequent run.</p>
<table>
<thead>
<tr>
<th style="text-align:left">Column</th>
<th style="text-align:left">Type</th>
<th style="text-align:left">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">cluster_id</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Canonical Databricks cluster ID (more info in <a href="">Common Meta Fields</a>)</td>
</tr>
<tr>
<td style="text-align:left">cluster_name</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Name of cluster at beginning of state</td>
</tr>
<tr>
<td style="text-align:left">custom_tags</td>
<td style="text-align:left">string</td>
<td style="text-align:left">JSON string of key/value pairs for all cluster associated custom tags give to the cluster</td>
</tr>
<tr>
<td style="text-align:left">*_state_start</td>
<td style="text-align:left">various</td>
<td style="text-align:left">timestamp reference column at the time the state began</td>
</tr>
<tr>
<td style="text-align:left">*_state_end</td>
<td style="text-align:left">various</td>
<td style="text-align:left">timestamp reference column at the time the state ended</td>
</tr>
<tr>
<td style="text-align:left">state</td>
<td style="text-align:left">string</td>
<td style="text-align:left">state of the cluster &ndash; full list <a href="https://docs.databricks.com/dev-tools/api/latest/clusters.html#clustereventtype">HERE</a></td>
</tr>
<tr>
<td style="text-align:left">current_num_workers</td>
<td style="text-align:left">long</td>
<td style="text-align:left">number of workers in use by the cluster at the start of the state</td>
</tr>
<tr>
<td style="text-align:left">target_num_worers</td>
<td style="text-align:left">long</td>
<td style="text-align:left">number of workers targeted to be present by the completion of the state. Should be equal to <em>current_num_workers</em> except during RESIZING state</td>
</tr>
<tr>
<td style="text-align:left">uptime_since_restart_S</td>
<td style="text-align:left">double</td>
<td style="text-align:left">Seconds since the cluster was last restarted / terminated</td>
</tr>
<tr>
<td style="text-align:left">uptime_in_state_S</td>
<td style="text-align:left">double</td>
<td style="text-align:left">Seconds the cluster spent in current state</td>
</tr>
<tr>
<td style="text-align:left">uptime_in_state_H</td>
<td style="text-align:left">double</td>
<td style="text-align:left">Hours the cluster spent in current state</td>
</tr>
<tr>
<td style="text-align:left">driver_node_type_id</td>
<td style="text-align:left">string</td>
<td style="text-align:left">KEY of driver node type to enable join to <a href="#instanceDetails">instanceDetails</a></td>
</tr>
<tr>
<td style="text-align:left">node_type_id</td>
<td style="text-align:left">string</td>
<td style="text-align:left">KEY of worker node type to enable join to <a href="#instanceDetails">instanceDetails</a></td>
</tr>
<tr>
<td style="text-align:left">cloud_billable</td>
<td style="text-align:left">boolean</td>
<td style="text-align:left">All current known states are cloud billable. This means that cloud provider charges are present during this state</td>
</tr>
<tr>
<td style="text-align:left">databricks_billable</td>
<td style="text-align:left">boolean</td>
<td style="text-align:left">State incurs databricks DBU costs. All states incur DBU costs except: INIT_SCRIPTS_FINISHED, INIT_SCRIPTS_STARTED, STARTING, TERMINATING, CREATING, RESTARTING</td>
</tr>
<tr>
<td style="text-align:left">isAutomated</td>
<td style="text-align:left">boolean</td>
<td style="text-align:left">Whether the cluster was created as an &ldquo;automated&rdquo; or &ldquo;interactive&rdquo; cluster</td>
</tr>
<tr>
<td style="text-align:left">dbu_rate</td>
<td style="text-align:left">double</td>
<td style="text-align:left">Effective dbu rate used for calculations (effective at time of pipeline run)</td>
</tr>
<tr>
<td style="text-align:left">state_dates</td>
<td style="text-align:left">array<!-- raw HTML omitted --></td>
<td style="text-align:left">Array of all dates across which the state spanned</td>
</tr>
<tr>
<td style="text-align:left">days_in_state</td>
<td style="text-align:left">int</td>
<td style="text-align:left">Number of days in state</td>
</tr>
<tr>
<td style="text-align:left">worker_potential_core_H</td>
<td style="text-align:left">double</td>
<td style="text-align:left">Worker core hours available to execute spark tasks</td>
</tr>
<tr>
<td style="text-align:left">core_hours</td>
<td style="text-align:left">double</td>
<td style="text-align:left">All core hours of entire cluster (including driver). Nodes * cores * hours in state</td>
</tr>
<tr>
<td style="text-align:left">driver_compute_cost</td>
<td style="text-align:left">double</td>
<td style="text-align:left">Compute costs associated with driver runtime</td>
</tr>
<tr>
<td style="text-align:left">driver_dbu_cost</td>
<td style="text-align:left">double</td>
<td style="text-align:left">DBU costs associated with driver runtime</td>
</tr>
<tr>
<td style="text-align:left">worker_compute_cost</td>
<td style="text-align:left">double</td>
<td style="text-align:left">Compute costs associated with worker runtime</td>
</tr>
<tr>
<td style="text-align:left">worker_dbu_cost</td>
<td style="text-align:left">double</td>
<td style="text-align:left">DBU costs associated with cumulative runtime of all worker nodes</td>
</tr>
<tr>
<td style="text-align:left">total_driver_cost</td>
<td style="text-align:left">double</td>
<td style="text-align:left">Driver costs including DBUs and compute</td>
</tr>
<tr>
<td style="text-align:left">total_worker_cost</td>
<td style="text-align:left">double</td>
<td style="text-align:left">Worker costs including DBUs and compute</td>
</tr>
<tr>
<td style="text-align:left">total_compute_cost</td>
<td style="text-align:left">double</td>
<td style="text-align:left">All compute costs for Driver and Workers</td>
</tr>
<tr>
<td style="text-align:left">total_dbu_cost</td>
<td style="text-align:left">double</td>
<td style="text-align:left">All dbu costs for Driver and Workers</td>
</tr>
<tr>
<td style="text-align:left">total_cost</td>
<td style="text-align:left">double</td>
<td style="text-align:left">Total cost from Compute and DBUs for all nodes (including Driver)</td>
</tr>
<tr>
<td style="text-align:left">driverSpecs</td>
<td style="text-align:left">struct</td>
<td style="text-align:left">Driver node details</td>
</tr>
<tr>
<td style="text-align:left">workerSpecs</td>
<td style="text-align:left">struct</td>
<td style="text-align:left">Worker node details</td>
</tr>
</tbody>
</table>
<h5 id="cost-functions-explained">Cost Functions Explained</h5>
<p><strong>EXPECTATIONS</strong> &ndash; Note that Overwatch costs are derived. This is good and bad. Good as it allows for costs to be
broken down by any dimension at the millisecond level. Bad because there can be significant differences between the
derived costs and actual costs. These should generally be very close to equal but may differ within margin of error by
as much as 10%. To verify the cost functions and the elements therein feel free to review them in more detail. If
your costs are off by a large marine, please review all the components of the cost function and correct any configurations
as necessary to align your reality with the Overwatch config. The default costs are list price and often do not
accurately reflect a customer&rsquo;s costs.</p>
<ul>
<li><strong>driver_compute_cost</strong>: when cloudBillable &ndash;&gt; Driver Node Compute Contract Price Hourly (instanceDetails) * Uptime_In_State_H &ndash;&gt; otherwise 0</li>
<li><strong>worker_compute_cost</strong>: when cloudBillable &ndash;&gt; Worker Node Compute Contract Price Hourly (instanceDetails) * Uptime_In_State_H * target_num_workers &ndash;&gt; otherwise 0
<ul>
<li>target_num_workers used here is ambiguous. Assuming all targeted workers can be provisioned, the calculation is most accurate;
however, if some workers cannot be provisioned the worker_compute_cost will be slightly higher than actual while
target_num_workers &gt; current_num_workers. target_num_workers used here because the compute costs begin accumulating
as soon as the node is provisioned, not at the time it is added to the cluster.</li>
</ul>
</li>
<li><strong>driver_dbu_cost</strong>: when databricks_billable &ndash;&gt; driver_hourly_dbus (instancedetails.hourlyDBUs) * houry_dbu_rate for dbu type (dbuCostDetails.contract_price) *
uptime_in_state_H &ndash;&gt; otherwise 0</li>
<li><strong>worker_dbu_cost</strong>: when databricks_billable &ndash;&gt; driver_hourly_dbus (instancedetails.hourlyDBUs) * houry_dbu_rate for dbu type (dbuCostDetails.contract_price) *
current_num_workers * uptime_in_state_H &ndash;&gt; otherwise 0
<ul>
<li>current_num_workers used here as dbu costs do not begin until the node able to receive workloads (i.e. node is
moved from target_worker to current_worker / &ldquo;upsize_complete&rdquo; state)</li>
</ul>
</li>
<li><strong>cloudBillable</strong>: Cluster is in a running state
<ul>
<li>GAP: Note that cloud billable ends at the time the cluster is terminated even though the nodes remain provisioned
in the cloud provider for several more minutes; these additional minutes are not accounted for in this
cost function.</li>
</ul>
</li>
</ul>
<h4 id="instancedetails">InstanceDetails</h4>
<p><a href="https://databrickslabs.github.io/overwatch/assets/TableSamples/instancedetails_aws.tab"><strong>AWS Sample</strong></a> | <a href="https://databrickslabs.github.io/overwatch/assets/TableSamples/instancedetails_azure.tab"><strong>AZURE_Sample</strong></a></p>
<p><strong>KEY</strong> &ndash; Organization_ID + API_name</p>
<p><strong>Incremental Columns</strong> &ndash; Pipeline_SnapTS</p>
<p><strong>Partition Columns</strong> &ndash; organization_id</p>
<p><strong>Write Mode</strong> &ndash; Append</p>
<p>This table is unique and it&rsquo;s purpose is to enable users to identify node specific contract costs associated with
Databricks and the Cloud Provider through time.
Defaults are loaded as an example by workspace. These defaults are meant to be reasonable, not accurate by default
as there is a wide difference between cloud discount rates and prices between regions / countries.
Everytime Overwatch runs, it validates the presence of
this table and whether it has any data present for the current workspace, if it does not it creates and appends the relevant
data to it; otherwise no action is taken. This gives the user the ability to extend / customize this table to fit their
needs by workspace. Each organization_id (workspace), should provide complete cost data for each node used in that
workspace. If you decide to completely customize the table, it&rsquo;s critical to note that <strong>some columns are required</strong>
for the ETL to function; these fields are indicated below in the table with an asterisk.</p>
<p>The organization_id (i.e. workspace id) is automatically generated for each workspace if that organization_id is not
present in the table already (or the table is not present at all). Each workspace
(i.e. organization_id) often has unique costs, this table enables you to customize compute pricing.</p>
<p><strong>IMPORTANT</strong> This table must be configured such that there are no overlapping costs (by time) and no gaps (by time)
in costs for any key (organization_id + API_name) between primordial date and current date.
This means that for a record to be &ldquo;expired&rdquo; the following must be true:</p>
<ul>
<li>original key expired by setting activeUntil == expiry date</li>
<li>original key must be created with updated information and must:
<ul>
<li>have activeFrom == expiry date of previous record (no gap, no overlap)</li>
<li>have activeUntil == lit(null).cast(&ldquo;date&rdquo;)</li>
</ul>
</li>
</ul>
<p><a href="https://azure.microsoft.com/en-us/pricing/details/virtual-machines/linux/">Azure VM Pricing Page</a></p>
<p><a href="https://aws.amazon.com/ec2/pricing/on-demand/">AWS EC2 Pricing Page</a></p>
<table>
<thead>
<tr>
<th style="text-align:left">Column</th>
<th style="text-align:left">Type</th>
<th style="text-align:left">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">instance</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Common name of instance type</td>
</tr>
<tr>
<td style="text-align:left">API_name*</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Canonical KEY name of the node type &ndash; use this to join to node_ids elsewhere</td>
</tr>
<tr>
<td style="text-align:left">vCPUs*</td>
<td style="text-align:left">int</td>
<td style="text-align:left">Number of virtual cpus provisioned for the node type</td>
</tr>
<tr>
<td style="text-align:left">Memory_GB</td>
<td style="text-align:left">double</td>
<td style="text-align:left">Gigabyes of memory provisioned for the node type</td>
</tr>
<tr>
<td style="text-align:left">Compute_Contract_Price*</td>
<td style="text-align:left">double</td>
<td style="text-align:left">Contract price for the instance type as negotiated between customer and cloud vendor. This is the value used in cost functions to deliver cost estimates. It is defaulted to equal the on_demand compute price</td>
</tr>
<tr>
<td style="text-align:left">On_Demand_Cost_Hourly</td>
<td style="text-align:left">double</td>
<td style="text-align:left">On demand, list price for node type DISCLAIMER &ndash; cloud provider pricing is dynamic and this is meant as an initial reference. This value should be validated and updated to reflect actual pricing</td>
</tr>
<tr>
<td style="text-align:left">Linux_Reserved_Cost_Hourly</td>
<td style="text-align:left">double</td>
<td style="text-align:left">Reserved, list price for node type DISCLAIMER &ndash; cloud provider pricing is dynamic and this is meant as an initial reference. This value should be validated and updated to reflect actual pricing</td>
</tr>
<tr>
<td style="text-align:left">Hourly_DBUs*</td>
<td style="text-align:left">double</td>
<td style="text-align:left">Number of DBUs charged for the node type</td>
</tr>
<tr>
<td style="text-align:left">is_active</td>
<td style="text-align:left">boolean</td>
<td style="text-align:left">whether the contract price is currently active. This must be true for each key where activeUntil is null</td>
</tr>
<tr>
<td style="text-align:left">activeFrom*</td>
<td style="text-align:left">date</td>
<td style="text-align:left">The start date for the costs in this record. <strong>NOTE</strong> this MUST be equal to one other record&rsquo;s activeUntil unless this is the first record for these costs. There may be no overlap in time or gaps in time.</td>
</tr>
<tr>
<td style="text-align:left">activeUntil*</td>
<td style="text-align:left">date</td>
<td style="text-align:left">The end date for the costs in this record. Must be null to indicate the active record. Only one record can be active at all times. The key (API_name) must have zero gaps and zero overlaps from the Overwatch primordial date until now indicated by null (active)</td>
</tr>
</tbody>
</table>
<h4 id="dbucostdetails">dbuCostDetails</h4>
<p><strong>KEY</strong> &ndash; Organization_ID + sku</p>
<p><strong>Incremental Columns</strong> &ndash; activeFrom</p>
<p><strong>Partition Columns</strong> &ndash; organization_id</p>
<p><strong>Write Mode</strong> &ndash; Append</p>
<p>Slow-changing dimension to track DBU contract costs by workspace through time. This table should only need to be edited
in very rare circumstances such as historical cost correction. Note that editing these contract prices will not
retroactively modify historical pricing in the costing table such as clusterStateFact or jobRunCostPotentialFact. For
prices to be recalculated, the gold pipeline modules must be rolled back properly such that the costs can be
rebuilt with the updated values.</p>
<table>
<thead>
<tr>
<th style="text-align:left">Column</th>
<th style="text-align:left">Type</th>
<th style="text-align:left">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">sku</td>
<td style="text-align:left">string</td>
<td style="text-align:left">One of automated, interactive, jobsLight, sqlCompute</td>
</tr>
<tr>
<td style="text-align:left">contract_price</td>
<td style="text-align:left">double</td>
<td style="text-align:left">Price paid per DBU on the sku</td>
</tr>
<tr>
<td style="text-align:left">is_active</td>
<td style="text-align:left">boolean</td>
<td style="text-align:left">whether the contract price is currently active. This must be true for each key where activeUntil is null</td>
</tr>
<tr>
<td style="text-align:left">activeFrom*</td>
<td style="text-align:left">date</td>
<td style="text-align:left">The start date for the costs in this record. <strong>NOTE</strong> this MUST be equal to one other record&rsquo;s activeUntil unless this is the first record for these costs. There may be no overlap in time or gaps in time.</td>
</tr>
<tr>
<td style="text-align:left">activeUntil*</td>
<td style="text-align:left">date</td>
<td style="text-align:left">The end date for the costs in this record. Must be null to indicate the active record. Only one record can be active at all times. The key (API_name) must have zero gaps and zero overlaps from the Overwatch primordial date until now indicated by null (active)</td>
</tr>
</tbody>
</table>
<h4 id="job">Job</h4>
<p><a href="https://databrickslabs.github.io/overwatch/assets/TableSamples/job.tab"><strong>SAMPLE</strong></a></p>
<p><strong>KEY</strong> &ndash; organization_id + job_id + unixTimeMS + action + request_id</p>
<p><strong>Incremental Columns</strong> &ndash; unixTimeMS</p>
<p><strong>Partition Columns</strong> &ndash; organization_id</p>
<p><strong>Write Mode</strong> &ndash; Append</p>
<table>
<thead>
<tr>
<th style="text-align:left">Column</th>
<th style="text-align:left">Type</th>
<th style="text-align:left">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">organization_id</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Canonical workspace id</td>
</tr>
<tr>
<td style="text-align:left">job_id</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Databricks job id</td>
</tr>
<tr>
<td style="text-align:left">action</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Action type defined by the record. One of: create, reset, update, delete, resetJobAcl, changeJobAcl. More information about these actions can be found <a href="https://docs.databricks.com/dev-tools/api/latest/jobs.html">here</a></td>
</tr>
<tr>
<td style="text-align:left">timestamp</td>
<td style="text-align:left">timestamp</td>
<td style="text-align:left">timestamp the action took place</td>
</tr>
<tr>
<td style="text-align:left">job_name</td>
<td style="text-align:left">string</td>
<td style="text-align:left">User defined name of job. NOTE, all jobs created through the UI are initialized with the name, &ldquo;Untitled&rdquo; therefore UI-created-named jobs will have an edit action to set the name. The cluster is also set to automated and defaulted on UI create as well</td>
</tr>
<tr>
<td style="text-align:left">job_type</td>
<td style="text-align:left">string</td>
<td style="text-align:left">?? TBD ?? &ndash; there is a job_task_type but that&rsquo;s in Run_id</td>
</tr>
<tr>
<td style="text-align:left">timeout_seconds</td>
<td style="text-align:left">string</td>
<td style="text-align:left">null unless specified, default == null. Timeout seconds specified in UI or via api</td>
</tr>
<tr>
<td style="text-align:left">schedule</td>
<td style="text-align:left">string</td>
<td style="text-align:left">JSON - quartz cron expression of scheduled job and timezone_id</td>
</tr>
<tr>
<td style="text-align:left">notebook_path</td>
<td style="text-align:left">string</td>
<td style="text-align:left">null if job task does not point to a notebook task. If job points to notebook for execution, this is path to that notebook</td>
</tr>
<tr>
<td style="text-align:left">new_settings</td>
<td style="text-align:left">dynamic struct</td>
<td style="text-align:left">job action with &ldquo;reset&rdquo; or &ldquo;update&rdquo; where settings were changed. Includes complex type of cluster. <a href="https://docs.databricks.com/dev-tools/api/latest/jobs.html#jobsjobsettings">JobSettings Structure Found Here</a></td>
</tr>
<tr>
<td style="text-align:left">cluster</td>
<td style="text-align:left">dyanmic struct</td>
<td style="text-align:left">Where relevant, contains the &ldquo;new_cluster&rdquo; spec when cluster definition is &ldquo;new_cluster&rdquo; or automated. If job definition points to existing cluster the cluster_id can be found here</td>
</tr>
<tr>
<td style="text-align:left">aclPermissionSet</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Predefined aclPermissionsSet such as &ldquo;Admin&rdquo; or &ldquo;Owner&rdquo;. More information on these can be found <a href="https://docs.databricks.com/security/access-control/jobs-acl.html#jobs-access-control">HERE</a></td>
</tr>
<tr>
<td style="text-align:left">grants</td>
<td style="text-align:left">string JSON</td>
<td style="text-align:left">Array of explicit grants given to explicit user list</td>
</tr>
<tr>
<td style="text-align:left">target_user_id</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Databricks canonical user id to which the aclPermissionSet is to be applied</td>
</tr>
<tr>
<td style="text-align:left">session_id</td>
<td style="text-align:left">string</td>
<td style="text-align:left">session_id that requested the action</td>
</tr>
<tr>
<td style="text-align:left">request_id</td>
<td style="text-align:left">string</td>
<td style="text-align:left">request_id of the action</td>
</tr>
<tr>
<td style="text-align:left">user_agent</td>
<td style="text-align:left">string</td>
<td style="text-align:left">request origin such as browser, terraform, api, etc.</td>
</tr>
<tr>
<td style="text-align:left">response</td>
<td style="text-align:left">struct</td>
<td style="text-align:left">response of api call including errorMessage, result, and statusCode (HTTP 200,400, etc)</td>
</tr>
<tr>
<td style="text-align:left">source_ip_address</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Origin IP of action requested</td>
</tr>
</tbody>
</table>
<h4 id="jobrun">JobRun</h4>
<p><a href="https://databrickslabs.github.io/overwatch/assets/TableSamples/jobrun.tab"><strong>SAMPLE</strong></a></p>
<p><strong>KEY</strong> &ndash; organization_id + run_id + startEpochMS</p>
<p><strong>Incremental Columns</strong> &ndash; startEpochMS</p>
<p><strong>Partition Columns</strong> &ndash; organization_id</p>
<p><strong>Write Mode</strong> &ndash; Merge</p>
<p>Inventory of every canonical job run executed in a databricks workspace.</p>
<table>
<thead>
<tr>
<th style="text-align:left">Column</th>
<th style="text-align:left">Type</th>
<th style="text-align:left">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">organization_id</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Canonical workspace id</td>
</tr>
<tr>
<td style="text-align:left">run_id</td>
<td style="text-align:left">int</td>
<td style="text-align:left">Incremental Canonical run ID for the workspaced</td>
</tr>
<tr>
<td style="text-align:left">run_name</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Name of run (if named)</td>
</tr>
<tr>
<td style="text-align:left">job_runtime</td>
<td style="text-align:left">struct</td>
<td style="text-align:left">Complex type with all, standard, runtime information regarding the runtime of the job. The start begins from the moment the job run start is requested (including cluster create time if relevant) and the end time marks the moment the workspace identifies the run as terminated</td>
</tr>
<tr>
<td style="text-align:left">job_id</td>
<td style="text-align:left">int</td>
<td style="text-align:left">Canonical ID of job</td>
</tr>
<tr>
<td style="text-align:left">id_in_job</td>
<td style="text-align:left">int</td>
<td style="text-align:left">Run instance of the job_id. This can be seen in the UI in a job spec denoted as &ldquo;Run N&rdquo;. Each Job ID has first id_in_job as &ldquo;Run 1&rdquo; and is incrented and canonical ONLY for the job_id. This field omits the &ldquo;Run &quot; prefix and results in an integer value of run instance within job.</td>
</tr>
<tr>
<td style="text-align:left">job_cluster_type</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Either new OR existing. New == automated and existing == interactive cluster type</td>
</tr>
<tr>
<td style="text-align:left">job_task_type</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Job Task Type - such as Notebook, Python, etc. See <a href="https://docs.databricks.com/dev-tools/api/latest/jobs.html#jobtask">JobTask</a></td>
</tr>
<tr>
<td style="text-align:left">job_terminal_state</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Result state of run such as SUCCESS, FAILED, TIMEDOUT, CANCELLED. See <a href="https://docs.databricks.com/dev-tools/api/latest/jobs.html#runresultstate">RunResultState</a></td>
</tr>
<tr>
<td style="text-align:left">job_trigger_type</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Type of trigger: PERIODIC, ONE_TIME, RETRY. See <a href="https://docs.databricks.com/dev-tools/api/latest/jobs.html#triggertype">TriggerType</a></td>
</tr>
<tr>
<td style="text-align:left">cluster_id</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Canonical workspace cluster id</td>
</tr>
<tr>
<td style="text-align:left">notebook_params</td>
<td style="text-align:left">string JSON</td>
<td style="text-align:left">A map of (String, String) parameters sent to notebook parameter overrides. See <a href="https://docs.databricks.com/dev-tools/api/latest/jobs.html#parampair">ParamPair</a></td>
</tr>
<tr>
<td style="text-align:left">libraries</td>
<td style="text-align:left">string JSON</td>
<td style="text-align:left">Array of Libraries in the format defined <a href="https://docs.databricks.com/dev-tools/api/latest/libraries.html#library">HERE</a></td>
</tr>
<tr>
<td style="text-align:left">children</td>
<td style="text-align:left">array<!-- raw HTML omitted --></td>
<td style="text-align:left">Array of structs that show all children of this job</td>
</tr>
<tr>
<td style="text-align:left">workflow_context</td>
<td style="text-align:left">string</td>
<td style="text-align:left">?? REVIEW ??</td>
</tr>
<tr>
<td style="text-align:left">task_detail</td>
<td style="text-align:left">struct</td>
<td style="text-align:left">Unified location for JobTask contingent upon jobrun task. See <a href="https://docs.databricks.com/dev-tools/api/latest/jobs.html#jobtask">JobTask</a></td>
</tr>
<tr>
<td style="text-align:left">cancellation_detail</td>
<td style="text-align:left">struct</td>
<td style="text-align:left">All cancellation request detail and status</td>
</tr>
<tr>
<td style="text-align:left">time_detail</td>
<td style="text-align:left">struct</td>
<td style="text-align:left">All time dimensions tied to the run. runBeginTime == Time the run began to execute on the cluster, SubmissionTime == Time the run request was received by the endpoint (before cluster build/start), completionTime == time the workspace denoted the runID was terminal state</td>
</tr>
<tr>
<td style="text-align:left">started_by</td>
<td style="text-align:left">struct</td>
<td style="text-align:left">Run request received from user &ndash; email is recorded here &ndash; usage == started_by.email</td>
</tr>
<tr>
<td style="text-align:left">request_detail</td>
<td style="text-align:left">struct</td>
<td style="text-align:left">Complete request detail received by the endpoint</td>
</tr>
</tbody>
</table>
<h4 id="jobruncostpotentialfact">JobRunCostPotentialFact</h4>
<p><a href="https://databrickslabs.github.io/overwatch/assets/TableSamples/jobruncostpotentialfact.tab"><strong>SAMPLE</strong></a></p>
<p><strong>KEY</strong> &ndash; organization_id + run_id + startEpochMS</p>
<p><strong>Incremental Columns</strong> &ndash; startEpochMS</p>
<p><strong>Partition Columns</strong> &ndash; organization_id</p>
<p><strong>Write Mode</strong> &ndash; Merge</p>
<p>This fact table defines the job, the cluster, the cost, the potential, and utilization (if cluster logging is enabled)
of a cluster associated with a specific Databricks Job Run.</p>
<p><strong>Dimensionality</strong> Note that this fact table is not normalized by time but rather by job run and cluster state.
Costs are not derived from job runs but from clusters thus the state[s] of the cluster are what&rsquo;s pertinent when
tying to cost. This is <strong>extremely important</strong> in the case of long running jobs, such as streaming.</p>
<p>SCENARIO: <!-- raw HTML omitted -->
Imagine a streaming job with 12 concurrent runs on an existing cluster that run for 20 days at the end of which the
driver dies for some reason causing all runs fail and begin retrying but failing. When the 20 days end, the cost will
be captured solely on that date and even more importantly, not only will all 20 days be captured at that date but the
cost associated will be cluster runtime for 20 days * number of runs. Overwatch will automatically smooth the costs
across the concurrent runs but not the days running since this fact table is not based by on an equidistant time axis.</p>
<ul>
<li>Potential: Total core_milliseconds for which the cluster COULD execute spark tasks. This derivation only includes
the worker nodes in a state ready to receive spark tasks (i.e. Running). Nodes being added or running init scripts
are not ready for spark jobs thus those core milliseconds are omitted from the total potential.</li>
<li>Cost: Derived from the <a href="#instancedetails">instanceDetails</a> table and DBU configured contract price (see
<a href="(/gettingstarted/configuration/)"><strong>Configuration</strong></a> for more details).
The compute costs in <a href="#instancedetails">instanceDetails</a> table are taken from the &ldquo;Compute_Contract_Price&rdquo; values associated with the
instance type in instanceDetails.</li>
<li>Utilization: Utilization is a function of core milliseconds used during spark task execution divided by the total
amount of core milliseconds available given the cluster size and state. (i.e. spark_task_runtime_H / worker_potential_core_H)</li>
<li>Cluster State: The state[s] of a cluster during a run. As the cluster scales and morphs to accommodate the run&rsquo;s
needs, the state changes. The number of state changes are recorded in this table as &ldquo;run_cluster_states&rdquo;.</li>
<li>Run State: Advanced Topic for data engineers and developers. This topic is discussed in considerable detail in the
<a href="https://databrickslabs.github.io/overwatch/gettingstarted/advancedtopics/">Advanced Topics</a> section.
Given a cluster state, the run state is a state of all runs on a cluster at a given moment in time. This
is the measure used to calculate shared costs across concurrent runs. A run state cannot pass the boundaries of
a cluster state, a run that continues across cluster-state lines will result in a new run state.</li>
</ul>
<table>
<thead>
<tr>
<th style="text-align:left">Column</th>
<th style="text-align:left">Type</th>
<th style="text-align:left">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">organization_id</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Canonical workspace id</td>
</tr>
<tr>
<td style="text-align:left">job_id</td>
<td style="text-align:left">long</td>
<td style="text-align:left">Canonical ID of job</td>
</tr>
<tr>
<td style="text-align:left">id_in_job</td>
<td style="text-align:left">long</td>
<td style="text-align:left">Run instance of the job_id. This can be seen in the UI in a job spec denoted as &ldquo;Run N&rdquo;. Each Job ID has first id_in_job as &ldquo;Run 1&rdquo; and is incrented and canonical ONLY for the job_id. This field omits the &ldquo;Run &quot; prefix and results in an integer value of run instance within job.</td>
</tr>
<tr>
<td style="text-align:left">job_runtime</td>
<td style="text-align:left">struct</td>
<td style="text-align:left">Time details of job start/end in epoch millis and timestamps</td>
</tr>
<tr>
<td style="text-align:left">cluster_id</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Canonical workspace cluster id</td>
</tr>
<tr>
<td style="text-align:left">cluster_name</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Name of cluster at time of run</td>
</tr>
<tr>
<td style="text-align:left">cluster_type</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Either new OR existing. New == automated and existing == interactive cluster type</td>
</tr>
<tr>
<td style="text-align:left">custom_tags</td>
<td style="text-align:left">string</td>
<td style="text-align:left">JSON string of key/value pairs for all cluster associated custom tags give to the cluster</td>
</tr>
<tr>
<td style="text-align:left">run_terminal_state</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Final state of the job such as &ldquo;Succeeded&rdquo;, &ldquo;Failed&rdquo; or &ldquo;Cancelled&rdquo;</td>
</tr>
<tr>
<td style="text-align:left">run_trigger_type</td>
<td style="text-align:left">string</td>
<td style="text-align:left">How the run was triggered (i.e. cron / manual)</td>
</tr>
<tr>
<td style="text-align:left">run_task_type</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Type of task in the job (i.e. notebook, jar, spark-submit, etc.)</td>
</tr>
<tr>
<td style="text-align:left">driver_node_type_id</td>
<td style="text-align:left">string</td>
<td style="text-align:left">KEY of driver node type to enable join to <a href="#instanceDetails">instanceDetails</a></td>
</tr>
<tr>
<td style="text-align:left">node_type_id</td>
<td style="text-align:left">string</td>
<td style="text-align:left">KEY of worker node type to enable join to <a href="#instanceDetails">instanceDetails</a></td>
</tr>
<tr>
<td style="text-align:left">dbu_rate</td>
<td style="text-align:left">double</td>
<td style="text-align:left">Effective DBU rate at time of job run used for calculations based on configured contract price in <a href="#instanceDetails">instanceDetails</a> at the time of the Overwatch Pipeline Run</td>
</tr>
<tr>
<td style="text-align:left">running_days</td>
<td style="text-align:left">array<!-- raw HTML omitted --></td>
<td style="text-align:left">Array (or list) of dates (not strings) across which the job run executed. This simplifies day-level cost attribution, among other metrics, when trying to smooth costs for long-running / streaming jobs</td>
</tr>
<tr>
<td style="text-align:left">avg_cluster_share</td>
<td style="text-align:left">double</td>
<td style="text-align:left">Average share of the cluster the run had available assuming fair scheduling. This DOES NOT account for activity outside of jobs (i.e. interactive notebooks running alongside job runs), this measure only splits out the share among concurrent job runs. Measure is only calculated for interactive clusters, automated clusters assume 100% run allocation. For more granular utilization detail, enable cluster logging and utilize &ldquo;job_run_cluster_util&rdquo; column which derives utilization at the spark task level.</td>
</tr>
<tr>
<td style="text-align:left">avg_overlapping_runs</td>
<td style="text-align:left">double</td>
<td style="text-align:left">Number of concurrent runs shared by the cluster on average throughout the run</td>
</tr>
<tr>
<td style="text-align:left">max_overlapping_runs</td>
<td style="text-align:left">long</td>
<td style="text-align:left">Highest number of concurrent runs on the cluster during the run</td>
</tr>
<tr>
<td style="text-align:left">run_cluster_states</td>
<td style="text-align:left">long</td>
<td style="text-align:left">Count of cluster states during the job run</td>
</tr>
<tr>
<td style="text-align:left">worker_potential_core_H</td>
<td style="text-align:left">double</td>
<td style="text-align:left">cluster core hours capable of executing spark tasks, &ldquo;potential&rdquo;</td>
</tr>
<tr>
<td style="text-align:left">driver_compute_cost</td>
<td style="text-align:left">double</td>
<td style="text-align:left">Compute costs associated with driver runtime</td>
</tr>
<tr>
<td style="text-align:left">driver_dbu_cost</td>
<td style="text-align:left">double</td>
<td style="text-align:left">DBU costs associated with driver runtime</td>
</tr>
<tr>
<td style="text-align:left">worker_compute_cost</td>
<td style="text-align:left">double</td>
<td style="text-align:left">Compute costs associated with worker runtime</td>
</tr>
<tr>
<td style="text-align:left">worker_dbu_cost</td>
<td style="text-align:left">double</td>
<td style="text-align:left">DBU costs associated with cumulative runtime of all worker nodes</td>
</tr>
<tr>
<td style="text-align:left">total_driver_cost</td>
<td style="text-align:left">double</td>
<td style="text-align:left">Driver costs including DBUs and compute</td>
</tr>
<tr>
<td style="text-align:left">total_worker_cost</td>
<td style="text-align:left">double</td>
<td style="text-align:left">Worker costs including DBUs and compute</td>
</tr>
<tr>
<td style="text-align:left">total_compute_cost</td>
<td style="text-align:left">double</td>
<td style="text-align:left">All compute costs for Driver and Workers</td>
</tr>
<tr>
<td style="text-align:left">total_dbu_cost</td>
<td style="text-align:left">double</td>
<td style="text-align:left">All dbu costs for Driver and Workers</td>
</tr>
<tr>
<td style="text-align:left">total_cost</td>
<td style="text-align:left">double</td>
<td style="text-align:left">Total cost from Compute and DBUs for all nodes (including Driver)</td>
</tr>
<tr>
<td style="text-align:left">spark_task_runtimeMS</td>
<td style="text-align:left">long</td>
<td style="text-align:left">Spark core execution time in milliseconds (i.e. task was operating/locking on core)</td>
</tr>
<tr>
<td style="text-align:left">spark_task_runtime_H</td>
<td style="text-align:left">double</td>
<td style="text-align:left">Spark core execution time in Hours (i.e. task was operating/locking on core)</td>
</tr>
<tr>
<td style="text-align:left">job_run_cluster_util</td>
<td style="text-align:left">double</td>
<td style="text-align:left">Cluster utilization: spark task execution time / cluster potential. True measure by core of utilization. Only available when cluster logging is enabled.</td>
</tr>
</tbody>
</table>
<h4 id="notebook">Notebook</h4>
<p><a href="https://databrickslabs.github.io/overwatch/assets/TableSamples/notebook.tab"><strong>SAMPLE</strong></a></p>
<p><strong>KEY</strong> &ndash; organization_id + notebook_id + request_id + action + unixTimeMS</p>
<p><strong>Incremental Columns</strong> &ndash; unixTimeMS</p>
<p><strong>Partition Columns</strong> &ndash; organization_id</p>
<p><strong>Write Mode</strong> &ndash; Append</p>
<table>
<thead>
<tr>
<th style="text-align:left">Column</th>
<th style="text-align:left">Type</th>
<th style="text-align:left">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">notebook_id</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Canonical notebook id</td>
</tr>
<tr>
<td style="text-align:left">notebook_name</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Name of notebook at time of action requested</td>
</tr>
<tr>
<td style="text-align:left">notebook_path</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Path of notebook at time of action requested</td>
</tr>
<tr>
<td style="text-align:left">cluster_id</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Canonical workspace cluster id</td>
</tr>
<tr>
<td style="text-align:left">action</td>
<td style="text-align:left">string</td>
<td style="text-align:left">action recorded</td>
</tr>
<tr>
<td style="text-align:left">timestamp</td>
<td style="text-align:left">timestamp</td>
<td style="text-align:left">timestamp the action took place</td>
</tr>
<tr>
<td style="text-align:left">old_name</td>
<td style="text-align:left">string</td>
<td style="text-align:left">When action is &ldquo;renameNotebook&rdquo; this holds notebook name before rename</td>
</tr>
<tr>
<td style="text-align:left">old_path</td>
<td style="text-align:left">string</td>
<td style="text-align:left">When action is &ldquo;moveNotebook&rdquo; this holds notebook path before move</td>
</tr>
<tr>
<td style="text-align:left">new_name</td>
<td style="text-align:left">string</td>
<td style="text-align:left">When action is &ldquo;renameNotebook&rdquo; this holds notebook name after rename</td>
</tr>
<tr>
<td style="text-align:left">new_path</td>
<td style="text-align:left">string</td>
<td style="text-align:left">When action is &ldquo;moveNotebook&rdquo; this holds notebook path after move</td>
</tr>
<tr>
<td style="text-align:left">parent_path</td>
<td style="text-align:left">string</td>
<td style="text-align:left">When action is &ldquo;renameNotebook&rdquo; notebook containing, workspace path is recorded here</td>
</tr>
<tr>
<td style="text-align:left">user_email</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Email of the user requesting the action</td>
</tr>
<tr>
<td style="text-align:left">request_id</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Canonical request_id</td>
</tr>
<tr>
<td style="text-align:left">response</td>
<td style="text-align:left">struct</td>
<td style="text-align:left">HTTP response including errorMessage, result, and statusCode</td>
</tr>
</tbody>
</table>
<h4 id="instancepool">InstancePool</h4>
<p><strong>KEY</strong> &ndash; organization_id + instance_pool_id + timestamp</p>
<p><strong>Incremental Columns</strong> &ndash; timestamp</p>
<p><strong>Partition Columns</strong> &ndash; organization_id</p>
<p><strong>Write Mode</strong> &ndash; Merge</p>
<table>
<thead>
<tr>
<th style="text-align:left">Column</th>
<th style="text-align:left">Type</th>
<th style="text-align:left">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">instance_pool_id</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Canonical notebook id</td>
</tr>
<tr>
<td style="text-align:left">instance_pool_name</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Name of notebook at time of action requested</td>
</tr>
<tr>
<td style="text-align:left">actionName</td>
<td style="text-align:left">string</td>
<td style="text-align:left">action recorded</td>
</tr>
<tr>
<td style="text-align:left">timestamp</td>
<td style="text-align:left">long</td>
<td style="text-align:left">timestamp the action took place</td>
</tr>
<tr>
<td style="text-align:left">node_type_id</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Type of node in the pool</td>
</tr>
<tr>
<td style="text-align:left">idle_instance_autotermination_minutes</td>
<td style="text-align:left">long</td>
<td style="text-align:left">Minutes after which a node shall be terminated if unused</td>
</tr>
<tr>
<td style="text-align:left">min_idle_instances</td>
<td style="text-align:left">long</td>
<td style="text-align:left">Minimum number of hot instances in the pool</td>
</tr>
<tr>
<td style="text-align:left">max_capacity</td>
<td style="text-align:left">long</td>
<td style="text-align:left">Maximum number of nodes allowed in the pool</td>
</tr>
<tr>
<td style="text-align:left">preloaded_spark_versions</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Spark versions preloaded on nodes in the pool</td>
</tr>
</tbody>
</table>
<h4 id="account-tables">Account Tables</h4>

<div class="notices note" ><p><strong>Not exposed in the consumer database</strong>. These tables contain more sensitive information and by default are not
exposed in the consumer database but held back in the ETL database. This is done purposely to simplify security when/if
desired. If desired, this can be exposed in consumer database with a simple vew definition exposing the columns desired.</p>
</div>


<div class="notices note" ><p>For deeper insights regarding audit, please reference <a href="https://docs.databricks.com/administration-guide/account-settings/audit-logs.html#request-parameters">auditLogSchema</a>.
This is simplified through the use of the <em>ETL_DB.audit_log_bronze</em> and filter where serviceName == accounts for example.
Additionally, you may filter down to specific actions using &ldquo;actionName&rdquo;. An example query is provided below:</p>
</div>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-scala" data-lang="scala">spark<span style="color:#f92672">.</span>table<span style="color:#f92672">(</span><span style="color:#e6db74">&#34;overwatch.audit_log_bronze&#34;</span><span style="color:#f92672">)</span>
  <span style="color:#f92672">.</span>filter<span style="color:#f92672">(</span>&#39;serviceName <span style="color:#f92672">===</span> <span style="color:#e6db74">&#34;accounts&#34;</span> <span style="color:#f92672">&amp;&amp;</span> &#39;actionName <span style="color:#f92672">===</span> <span style="color:#e6db74">&#34;createGroup&#34;</span><span style="color:#f92672">)</span>
  <span style="color:#f92672">.</span>selectExpr<span style="color:#f92672">(</span><span style="color:#e6db74">&#34;*&#34;</span><span style="color:#f92672">,</span> <span style="color:#e6db74">&#34;requestParams.*&#34;</span><span style="color:#f92672">).</span>drop<span style="color:#f92672">(</span><span style="color:#e6db74">&#34;requestParams&#34;</span><span style="color:#f92672">)</span>      
</code></pre></div><p>Slow changing dimension of user entity through time. Also used as reference map from user_email to user_id</p>
<table>
<thead>
<tr>
<th style="text-align:left">Column</th>
<th style="text-align:left">Type</th>
<th style="text-align:left">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">organization_id</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Canonical workspace id</td>
</tr>
<tr>
<td style="text-align:left">user_id</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Canonical user id for which the action was requested (within the workspace) (target)</td>
</tr>
<tr>
<td style="text-align:left">user_email</td>
<td style="text-align:left">string</td>
<td style="text-align:left">User&rsquo;s email for which the action was requested (target)</td>
</tr>
<tr>
<td style="text-align:left">action</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Action requested to be performed</td>
</tr>
<tr>
<td style="text-align:left">added_from_ip_address</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Source IP of the request</td>
</tr>
<tr>
<td style="text-align:left">added_by</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Authenticated user that made the request</td>
</tr>
<tr>
<td style="text-align:left">user_agent</td>
<td style="text-align:left">string</td>
<td style="text-align:left">request origin such as browser, terraform, api, etc.</td>
</tr>
</tbody>
</table>
<h4 id="accountmod">AccountMod</h4>
<p><a href="https://databrickslabs.github.io/overwatch/assets/TableSamples/accountmodificationfact.tab"><strong>SAMPLE</strong></a></p>
<p><strong>KEY</strong> &ndash; organization_id + acton + mod_unixTimeMS + request_id</p>
<p><strong>Incremental Columns</strong> &ndash; mod_unixTimeMS</p>
<p><strong>Partition Columns</strong> &ndash; organization_id</p>
<p><strong>Write Mode</strong> &ndash; Append</p>
<p>TODO</p>
<h4 id="accountlogin">AccountLogin</h4>
<p><a href="https://databrickslabs.github.io/overwatch/assets/TableSamples/accountloginfact.tab"><strong>SAMPLE</strong></a></p>
<p><strong>KEY</strong> &ndash; organization_id + login_type + login_unixTimeMS + from_ip_address</p>
<p><strong>Incremental Columns</strong> &ndash; login_unixTimeMS</p>
<p><strong>Partition Columns</strong> &ndash; organization_id</p>
<p><strong>Write Mode</strong> &ndash; Append</p>

<div class="notices note" ><p><strong>Not exposed in the consumer database</strong>. This table contains more sensitive information and by default is not
exposed in the consumer database but held back in the etl datbase. This is done purposely to simplify security when/if
desired. If desired, this can be exposed in consumer database with a simple vew definition exposing the columns desired.</p>
</div>

<table>
<thead>
<tr>
<th style="text-align:left">Column</th>
<th style="text-align:left">Type</th>
<th style="text-align:left">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">user_id</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Canonical user id (within the workspace)</td>
</tr>
<tr>
<td style="text-align:left">user_email</td>
<td style="text-align:left">string</td>
<td style="text-align:left">User&rsquo;s email</td>
</tr>
<tr>
<td style="text-align:left">login_type</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Type of login such as web, ssh, token</td>
</tr>
<tr>
<td style="text-align:left">ssh_username</td>
<td style="text-align:left">string</td>
<td style="text-align:left">username used to login via SSH</td>
</tr>
<tr>
<td style="text-align:left">groups_user_name</td>
<td style="text-align:left">string</td>
<td style="text-align:left">?? To research ??</td>
</tr>
<tr>
<td style="text-align:left">account_admin_userID</td>
<td style="text-align:left">string</td>
<td style="text-align:left">?? To research ??</td>
</tr>
<tr>
<td style="text-align:left">login_from_ip_address</td>
<td style="text-align:left">struct</td>
<td style="text-align:left">Details about the source login and target logged into</td>
</tr>
<tr>
<td style="text-align:left">user_agent</td>
<td style="text-align:left">string</td>
<td style="text-align:left">request origin such as browser, terraform, api, etc.</td>
</tr>
</tbody>
</table>

<div class="notices note" ><p>The following sections are related to Spark. Everything that can be seend/found in the SparkUI is visibel in the
spark tables below. A reasonable understanding of the Spark hierarchy is necessary to
make this section simpler. Please <a href="(/gettingstarted/modules//#sparkevents)"><strong>reference Spark Hierarchy For More Details</strong></a> for more details.</p>
</div>

<h4 id="sparkexecution">SparkExecution</h4>
<p><a href="https://databrickslabs.github.io/overwatch/assets/TableSamples/sparkexecution.tab"><strong>SAMPLE</strong></a></p>
<p><strong>KEY</strong> &ndash; organization_id + spark_context_id + execution_id + date + unixTimeMS</p>
<p><strong>Incremental Columns</strong> &ndash; date + unixTimeMS</p>
<p><strong>Partition Columns</strong> &ndash; organization_id</p>
<p><strong>Write Mode</strong> &ndash; Merge</p>
<table>
<thead>
<tr>
<th style="text-align:left">Column</th>
<th style="text-align:left">Type</th>
<th style="text-align:left">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">organization_id</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Canonical workspace id</td>
</tr>
<tr>
<td style="text-align:left">spark_context_id</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Canonical context ID &ndash; One Spark Context per Cluster</td>
</tr>
<tr>
<td style="text-align:left">cluster_id</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Canonical workspace cluster id</td>
</tr>
<tr>
<td style="text-align:left">execution_id</td>
<td style="text-align:left">long</td>
<td style="text-align:left">Spark Execution ID</td>
</tr>
<tr>
<td style="text-align:left">description</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Description provided by spark</td>
</tr>
<tr>
<td style="text-align:left">details</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Execution StackTrace</td>
</tr>
<tr>
<td style="text-align:left">sql_execution_runtime</td>
<td style="text-align:left">struct</td>
<td style="text-align:left">Complete runtime detail breakdown</td>
</tr>
</tbody>
</table>
<h4 id="sparkexecutor">SparkExecutor</h4>
<p><a href="https://databrickslabs.github.io/overwatch/assets/TableSamples/sparkexecutor.tab"><strong>SAMPLE</strong></a></p>
<p><strong>KEY</strong> &ndash; organization_id + spark_context_id + executor_id + date + unixTimeMS</p>
<p><strong>Incremental Columns</strong> &ndash; date + unixTimeMS</p>
<p><strong>Partition Columns</strong> &ndash; organization_id</p>
<p><strong>Write Mode</strong> &ndash; Merge</p>
<table>
<thead>
<tr>
<th style="text-align:left">Column</th>
<th style="text-align:left">Type</th>
<th style="text-align:left">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">organization_id</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Canonical workspace id</td>
</tr>
<tr>
<td style="text-align:left">spark_context_id</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Canonical context ID &ndash; One Spark Context per Cluster</td>
</tr>
<tr>
<td style="text-align:left">cluster_id</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Canonical workspace cluster id</td>
</tr>
<tr>
<td style="text-align:left">executor_id</td>
<td style="text-align:left">int</td>
<td style="text-align:left">Executor ID</td>
</tr>
<tr>
<td style="text-align:left">executor_info</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Executor Detail</td>
</tr>
<tr>
<td style="text-align:left">removed_reason</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Reason executor was removed</td>
</tr>
<tr>
<td style="text-align:left">executor_alivetime</td>
<td style="text-align:left">struct</td>
<td style="text-align:left">Complete lifetime detail breakdown</td>
</tr>
</tbody>
</table>
<h4 id="sparkjob">SparkJob</h4>
<p><a href="https://databrickslabs.github.io/overwatch/assets/TableSamples/sparkjob.tab"><strong>SAMPLE</strong></a></p>
<p><strong>KEY</strong> &ndash; organization_id + spark_context_id + job_id + unixTimeMS</p>
<p><strong>Incremental Columns</strong> &ndash; date + unixTimeMS</p>
<p><strong>Partition Columns</strong> &ndash; organization_id + date</p>
<p><strong>Z-Order Columns</strong> &ndash; cluster_id</p>
<p><strong>Write Mode</strong> &ndash; Merge</p>
<table>
<thead>
<tr>
<th style="text-align:left">Column</th>
<th style="text-align:left">Type</th>
<th style="text-align:left">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">organization_id</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Canonical workspace id</td>
</tr>
<tr>
<td style="text-align:left">spark_context_id</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Canonical context ID &ndash; One Spark Context per Cluster</td>
</tr>
<tr>
<td style="text-align:left">cluster_id</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Canonical workspace cluster id</td>
</tr>
<tr>
<td style="text-align:left">job_id</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Spark Job ID</td>
</tr>
<tr>
<td style="text-align:left">job_group_id</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Spark Job Group ID &ndash; NOTE very powerful for many reasons. See <a href="https://databrickslabs.github.io/overwatch/gettingstarted/modules//#sparkevents">SparkEvents</a></td>
</tr>
<tr>
<td style="text-align:left">execution_id</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Spark Execution ID</td>
</tr>
<tr>
<td style="text-align:left">stage_ids</td>
<td style="text-align:left">array[long]</td>
<td style="text-align:left">Array of all Spark Stage IDs nested within this Spark Job</td>
</tr>
<tr>
<td style="text-align:left">notebook_id</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Canonical Databricks Workspace Notebook ID</td>
</tr>
<tr>
<td style="text-align:left">notebook_path</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Databricks Notebook Path</td>
</tr>
<tr>
<td style="text-align:left">user_email</td>
<td style="text-align:left">string</td>
<td style="text-align:left">email of user that owned the request, for Databricks jobs this will be the job owner</td>
</tr>
<tr>
<td style="text-align:left">db_job_id</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Databricks Job Id executing the Spark Job</td>
</tr>
<tr>
<td style="text-align:left">db_id_in_job</td>
<td style="text-align:left">string</td>
<td style="text-align:left">&ldquo;id_in_job&rdquo; such as &ldquo;Run 10&rdquo; without &ldquo;Run &quot; prefix. This is a critical join column when working looking up Databricks Jobs metadata</td>
</tr>
<tr>
<td style="text-align:left">job_runtime</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Complete job runtime detail breakdown</td>
</tr>
<tr>
<td style="text-align:left">job_result</td>
<td style="text-align:left">struct</td>
<td style="text-align:left">Job Result and Exception if present</td>
</tr>
</tbody>
</table>
<h4 id="sparkstage">SparkStage</h4>
<p><a href="https://databrickslabs.github.io/overwatch/assets/TableSamples/sparkstage.tab"><strong>SAMPLE</strong></a></p>
<p><strong>KEY</strong> &ndash; organization_id + spark_context_id + stage_id + stage_attempt_id + unixTimeMS</p>
<p><strong>Incremental Columns</strong> &ndash; date + unixTimeMS</p>
<p><strong>Partition Columns</strong> &ndash; organization_id + date</p>
<p><strong>Z-Order Columns</strong> &ndash; cluster_id</p>
<p><strong>Write Mode</strong> &ndash; Merge</p>
<table>
<thead>
<tr>
<th style="text-align:left">Column</th>
<th style="text-align:left">Type</th>
<th style="text-align:left">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">organization_id</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Canonical workspace id</td>
</tr>
<tr>
<td style="text-align:left">spark_context_id</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Canonical context ID &ndash; One Spark Context per Cluster</td>
</tr>
<tr>
<td style="text-align:left">cluster_id</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Canonical workspace cluster id</td>
</tr>
<tr>
<td style="text-align:left">stage_id</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Spark Stage ID</td>
</tr>
<tr>
<td style="text-align:left">stage_attempt_id</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Spark Stage Attempt ID</td>
</tr>
<tr>
<td style="text-align:left">stage_runtime</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Complete stage runtime detail</td>
</tr>
<tr>
<td style="text-align:left">stage_info</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Lineage of all accumulables for the Spark Stage</td>
</tr>
</tbody>
</table>
<h4 id="sparktask">SparkTask</h4>
<p><a href="https://databrickslabs.github.io/overwatch/assets/TableSamples/sparktask.tab"><strong>SAMPLE</strong></a></p>
<p><strong>KEY</strong> &ndash; organization_id + spark_context_id + task_id + task_attempt_id + stage_id + stage_attempt_id + host + unixTimeMS</p>
<p><strong>Incremental Columns</strong> &ndash; date + unixTimeMS</p>
<p><strong>Partition Columns</strong> &ndash; organization_id + date</p>
<p><strong>Z-Order Columns</strong> &ndash; cluster_id</p>
<p><strong>Write Mode</strong> &ndash; Merge</p>

<div class="notices warning" ><p><strong>USE THE PARTITION COLUMN</strong> (date) and Indexed Column (cluster_id) in all joins and filters where possible.
This table can get extremely large, select samples or smaller date ranges and reduce joins and columns selected
to improve performance.</p>
</div>

<table>
<thead>
<tr>
<th style="text-align:left">Column</th>
<th style="text-align:left">Type</th>
<th style="text-align:left">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">organization_id</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Canonical workspace id</td>
</tr>
<tr>
<td style="text-align:left">workspace_name</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Customizable human-legible name of the workspace, should be globally unique within the organization</td>
</tr>
<tr>
<td style="text-align:left">spark_context_id</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Canonical context ID &ndash; One Spark Context per Cluster</td>
</tr>
<tr>
<td style="text-align:left">cluster_id</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Canonical workspace cluster id</td>
</tr>
<tr>
<td style="text-align:left">task_id</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Spark Task ID</td>
</tr>
<tr>
<td style="text-align:left">task_attempt_id</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Spark Task Attempt ID</td>
</tr>
<tr>
<td style="text-align:left">stage_id</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Spark Stage ID</td>
</tr>
<tr>
<td style="text-align:left">stage_attempt_id</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Spark Stage Attempt ID</td>
</tr>
<tr>
<td style="text-align:left">executor_id</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Spark Executor ID</td>
</tr>
<tr>
<td style="text-align:left">host</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Internal IP address of node</td>
</tr>
<tr>
<td style="text-align:left">task_runtime</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Complete task runtime detail</td>
</tr>
<tr>
<td style="text-align:left">task_metrics</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Lowest level compute metrics provided by spark such as spill bytes, read/write bytes, shuffle info, GC time, Serialization, etc.</td>
</tr>
<tr>
<td style="text-align:left">task_info</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Lineage of all accumulables for the Spark Task</td>
</tr>
<tr>
<td style="text-align:left">task_type</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Spark task Type (i.e. ResultTask, ShuffleMapTask, etc)</td>
</tr>
<tr>
<td style="text-align:left">task_end_reason</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Task end status, state, and details plus stake trace when error</td>
</tr>
</tbody>
</table>
<h4 id="sparkstream_preview">SparkStream_preview</h4>
<p><strong>KEY</strong> &ndash; organization_id + spark_context_id + cluster_id + stream_id + stream_run_id + stream_batch_id + stream_timestamp</p>
<p><strong>Incremental Columns</strong> &ndash; date + stream_timestamp</p>
<p><strong>Partition Columns</strong> &ndash; organization_id + date</p>
<p><strong>Z-Order Columns</strong> &ndash; cluster_id</p>
<p><strong>Write Mode</strong> &ndash; Merge</p>
<p>Remains in preview through version 0.6.0 as more feedback is requested from users and use-cases before this table
structure solidifes.</p>
<table>
<thead>
<tr>
<th style="text-align:left">Column</th>
<th style="text-align:left">Type</th>
<th style="text-align:left">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">spark_context_id</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Canonical context ID &ndash; One Spark Context per Cluster</td>
</tr>
<tr>
<td style="text-align:left">cluster_id</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Canonical workspace cluster id</td>
</tr>
<tr>
<td style="text-align:left">stream_id</td>
<td style="text-align:left">string</td>
<td style="text-align:left">GUID ID of the spark stream</td>
</tr>
<tr>
<td style="text-align:left">stream_name</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Name of stream if named</td>
</tr>
<tr>
<td style="text-align:left">stream_run_id</td>
<td style="text-align:left">string</td>
<td style="text-align:left">GUID ID of the spark stream run</td>
</tr>
<tr>
<td style="text-align:left">stream_batch_id</td>
<td style="text-align:left">long</td>
<td style="text-align:left">GUID ID of the spark stream run batch</td>
</tr>
<tr>
<td style="text-align:left">stream_timestamp</td>
<td style="text-align:left">long</td>
<td style="text-align:left">Unix time (millis) the stream reported its batch complete metrics</td>
</tr>
<tr>
<td style="text-align:left">streamSegment</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Type of event from the event listener such as &lsquo;Progressed&rsquo;</td>
</tr>
<tr>
<td style="text-align:left">streaming_metrics</td>
<td style="text-align:left">dynamic struct</td>
<td style="text-align:left">All metrics available for the stream batch run</td>
</tr>
<tr>
<td style="text-align:left">execution_ids</td>
<td style="text-align:left">array<!-- raw HTML omitted --></td>
<td style="text-align:left">Array of execution_ids in the spark_context. Can explode and tie back to sparkExecution and other spark tables</td>
</tr>
</tbody>
</table>
<h4 id="common-meta-fields">Common Meta Fields</h4>
<table>
<thead>
<tr>
<th style="text-align:left">Column</th>
<th style="text-align:left">Type</th>
<th style="text-align:left">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">organization_id</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Workspace / Organization ID on which the cluster was instantiated</td>
</tr>
<tr>
<td style="text-align:left">cluster_id</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Canonical workspace cluster id</td>
</tr>
<tr>
<td style="text-align:left">unixTimeMS</td>
<td style="text-align:left">long</td>
<td style="text-align:left">unix time epoch as a long in milliseconds</td>
</tr>
<tr>
<td style="text-align:left">timestamp</td>
<td style="text-align:left">string</td>
<td style="text-align:left">unixTimeMS as a timestamp type in milliseconds</td>
</tr>
<tr>
<td style="text-align:left">date</td>
<td style="text-align:left">string</td>
<td style="text-align:left">unixTimeMS as a date type</td>
</tr>
<tr>
<td style="text-align:left">created_by</td>
<td style="text-align:left">string</td>
<td></td>
</tr>
<tr>
<td style="text-align:left">last_edited_by</td>
<td style="text-align:left">string</td>
<td style="text-align:left">last user to edit the state of the entity</td>
</tr>
<tr>
<td style="text-align:left">last_edited_ts</td>
<td style="text-align:left">string</td>
<td style="text-align:left">timestamp at which the entitiy&rsquo;s sated was last edited</td>
</tr>
<tr>
<td style="text-align:left">deleted_by</td>
<td style="text-align:left">string</td>
<td style="text-align:left">user that deleted the entity</td>
</tr>
<tr>
<td style="text-align:left">deleted_ts</td>
<td style="text-align:left">string</td>
<td style="text-align:left">timestamp at which the entity was deleted</td>
</tr>
<tr>
<td style="text-align:left">event_log_start</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Spark Event Log BEGIN file name / path</td>
</tr>
<tr>
<td style="text-align:left">event_log_end</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Spark Event Log END file name / path</td>
</tr>
<tr>
<td style="text-align:left">Pipeline_SnapTS</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Snapshot timestmap of Overwatch run that added the record</td>
</tr>
<tr>
<td style="text-align:left">Overwatch_RunID</td>
<td style="text-align:left">string</td>
<td style="text-align:left">Overwatch canonical ID that resulted in the record load</td>
</tr>
</tbody>
</table>
<h2 id="etl-tables">ETL Tables</h2>
<p>The following are the list of potential tables, the module with which it&rsquo;s created and the layer in which it lives.
This list consists of only the ETL tables created to facilitate and deliver the <a href="#consumption-layer-tables">consumption layer</a>
<!-- raw HTML omitted --><!-- raw HTML omitted -->
The gold and consumption layers are the only layers that maintain column name uniformity and naming convention across
all tables. Users should always reference Consumption and Gold layers unless the data necessary has not been curated.</p>
<h3 id="bronze">Bronze</h3>
<table>
<thead>
<tr>
<th style="text-align:left">Table</th>
<th style="text-align:left">Scope</th>
<th style="text-align:left">Layer</th>
<th style="text-align:left">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">audit_log_bronze</td>
<td style="text-align:left">audit</td>
<td style="text-align:left">bronze</td>
<td style="text-align:left">Raw audit log data full schema</td>
</tr>
<tr>
<td style="text-align:left">audit_log_raw_events</td>
<td style="text-align:left">audit</td>
<td style="text-align:left">bronze (azure)</td>
<td style="text-align:left">Intermediate staging table responsible for coordinating intermediate events from azure Event Hub</td>
</tr>
<tr>
<td style="text-align:left">cluster_events_bronze</td>
<td style="text-align:left">clusterEvents</td>
<td style="text-align:left">bronze</td>
<td style="text-align:left">Raw landing of dataframe derived from JSON response from cluster events api call. Note: cluster events expire after 30 days of last termination. (<a href="https://docs.databricks.com/dev-tools/api/latest/clusters.html#events">reference</a>)</td>
</tr>
<tr>
<td style="text-align:left">clusters_snapshot_bronze</td>
<td style="text-align:left">clusters</td>
<td style="text-align:left">bronze</td>
<td style="text-align:left">API snapshot of existing clusters defined in Databricks workspace at the time of the Overwatch run. Snapshot is taken on each run</td>
</tr>
<tr>
<td style="text-align:left">jobs_snapshot_bronze</td>
<td style="text-align:left">jobs</td>
<td style="text-align:left">bronze</td>
<td style="text-align:left">API snapshot of existing jobs defined in Databricks workspace at the time of the Overwatch run. Snapshot is taken on each run</td>
</tr>
<tr>
<td style="text-align:left">pools_snapshot_bronze</td>
<td style="text-align:left">pools</td>
<td style="text-align:left">bronze</td>
<td style="text-align:left">API snapshot of existing pools defined in Databricks workspace at the time of the Overwatch run. Snapshot is taken on each run</td>
</tr>
<tr>
<td style="text-align:left">spark_events_bronze</td>
<td style="text-align:left">sparkEvents</td>
<td style="text-align:left">bronze</td>
<td style="text-align:left">Raw landing of the master sparkEvents schema and data for all cluster logs. Cluster log locations are defined by cluster specs and all locations will be scanned for new files not yet captured by Overwatch. Overwatch uses an implicit schema generation here, as such, <strong>a lack of real-world can cause unforeseen issues</strong>.</td>
</tr>
<tr>
<td style="text-align:left">spark_events_processedfiles</td>
<td style="text-align:left">sparkEvents</td>
<td style="text-align:left">bronze</td>
<td style="text-align:left">Table that keeps track of all previously processed cluster log files (spark event logs) to minimize future file scanning and improve performance. This table can be used to reprocess and/or find specific eventLog files.</td>
</tr>
<tr>
<td style="text-align:left">pipeline_report</td>
<td style="text-align:left">NA</td>
<td style="text-align:left">tracking</td>
<td style="text-align:left">Tracking table used to identify state and status of each Overwatch Pipeline run. This table is also used to control the start and end points of each run. Altering the timestamps and status of this table will change the ETL start/end points.</td>
</tr>
</tbody>
</table>
<h3 id="silver">Silver</h3>
<table>
<thead>
<tr>
<th style="text-align:left">Table</th>
<th style="text-align:left">Scope</th>
<th style="text-align:left">Layer</th>
<th style="text-align:left">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">account_login_silver</td>
<td style="text-align:left">accounts</td>
<td style="text-align:left">silver</td>
<td style="text-align:left">Login events</td>
</tr>
<tr>
<td style="text-align:left">account_mods_silver</td>
<td style="text-align:left">accounts</td>
<td style="text-align:left">silver</td>
<td style="text-align:left">Account modification events</td>
</tr>
<tr>
<td style="text-align:left">cluster_spec_silver</td>
<td style="text-align:left">clusters</td>
<td style="text-align:left">silver</td>
<td style="text-align:left">Slow changing dimension used to track all clusters through time including edits but <strong>excluding state change</strong>.</td>
</tr>
<tr>
<td style="text-align:left">cluster_state_detail_silver</td>
<td style="text-align:left">clusterEvents</td>
<td style="text-align:left">silver</td>
<td style="text-align:left">State detail for each cluster event enriched with cost information</td>
</tr>
<tr>
<td style="text-align:left">job_status_silver</td>
<td style="text-align:left">jobs</td>
<td style="text-align:left">silver</td>
<td style="text-align:left">Slow changing dimension used to track all jobs specifications through time</td>
</tr>
<tr>
<td style="text-align:left">jobrun_silver</td>
<td style="text-align:left">jobs</td>
<td style="text-align:left">silver</td>
<td style="text-align:left">Historical run of every job since Overwatch began capturing the audit_log_data</td>
</tr>
<tr>
<td style="text-align:left">notebook_silver</td>
<td style="text-align:left">notebooks</td>
<td style="text-align:left">silver</td>
<td style="text-align:left">Slow changing dimension used to track all notebook changes as it morphs through time along with which user instigated the change. This does not include specific change details of the commands within a notebook just metadata changes regarding the notebook.</td>
</tr>
<tr>
<td style="text-align:left">pools_silver</td>
<td style="text-align:left">pools</td>
<td style="text-align:left">silver</td>
<td style="text-align:left">Slow changing dimension used to track all changes to instance pools</td>
</tr>
<tr>
<td style="text-align:left">spark_executions_silver</td>
<td style="text-align:left">sparkEvents</td>
<td style="text-align:left">silver</td>
<td style="text-align:left">All spark event data relevant to spark executions</td>
</tr>
<tr>
<td style="text-align:left">spark_executors_silver</td>
<td style="text-align:left">sparkEvents</td>
<td style="text-align:left">silver</td>
<td style="text-align:left">All spark event data relevant to spark executors</td>
</tr>
<tr>
<td style="text-align:left">spark_jobs_silver</td>
<td style="text-align:left">sparkEvents</td>
<td style="text-align:left">silver</td>
<td style="text-align:left">All spark event data relevant to spark jobs</td>
</tr>
<tr>
<td style="text-align:left">spark_stages_silver</td>
<td style="text-align:left">sparkEvents</td>
<td style="text-align:left">silver</td>
<td style="text-align:left">All spark event data relevant to spark stages</td>
</tr>
<tr>
<td style="text-align:left">spark_tasks_silver</td>
<td style="text-align:left">sparkEvents</td>
<td style="text-align:left">silver</td>
<td style="text-align:left">All spark event data relevant to spark tasks</td>
</tr>
</tbody>
</table>
<h3 id="gold">Gold</h3>
<table>
<thead>
<tr>
<th style="text-align:left">Table</th>
<th style="text-align:left">Scope</th>
<th style="text-align:left">Layer</th>
<th style="text-align:left">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">account_login_gold</td>
<td style="text-align:left">accounts</td>
<td style="text-align:left">gold</td>
<td style="text-align:left">Login events</td>
</tr>
<tr>
<td style="text-align:left">account_mods_gold</td>
<td style="text-align:left">accounts</td>
<td style="text-align:left">gold</td>
<td style="text-align:left">Account modification events</td>
</tr>
<tr>
<td style="text-align:left">cluster_gold</td>
<td style="text-align:left">clusters</td>
<td style="text-align:left">gold</td>
<td style="text-align:left">Slow-changing dimension with all cluster creates and edits through time. These events <strong>DO NOT INCLUDE automated cluster resize events or cluster state changes</strong>. Automated cluster resize and cluster state changes will be in clusterstatefact_gold. If user changes min/max nodes or node count (non-autoscaling) the event will be registered here AND clusterstatefact_gold.</td>
</tr>
<tr>
<td style="text-align:left">clusterStateFact_gold</td>
<td style="text-align:left">clusterEvents</td>
<td style="text-align:left">gold</td>
<td style="text-align:left">All cluster event changes along with the time spent in each state and the core hours in each state. This table should be used to find cluster anomalies and/or calculate compute/DBU costs of some given scope.</td>
</tr>
<tr>
<td style="text-align:left">job_gold</td>
<td style="text-align:left">jobs</td>
<td style="text-align:left">gold</td>
<td style="text-align:left">Slow-changing dimension of all changes to a job definition through time</td>
</tr>
<tr>
<td style="text-align:left">jobrun_gold</td>
<td style="text-align:left">jobs</td>
<td style="text-align:left">gold</td>
<td style="text-align:left">Dimensional data for each job run in the databricks workspace</td>
</tr>
<tr>
<td style="text-align:left">notebook_gold</td>
<td style="text-align:left">notebooks</td>
<td style="text-align:left">gold</td>
<td style="text-align:left">Slow changing dimension used to track all notebook changes as it morphs through time along with which user instigated the change. This does not include specific change details of the commands within a notebook just metadata changes regarding the notebook.</td>
</tr>
<tr>
<td style="text-align:left">instancepool_gold</td>
<td style="text-align:left">pools</td>
<td style="text-align:left">gold</td>
<td style="text-align:left">Slow changing dimension used to track all changes to instance pools</td>
</tr>
<tr>
<td style="text-align:left">sparkexecution_gold</td>
<td style="text-align:left">sparkEvents</td>
<td style="text-align:left">gold</td>
<td style="text-align:left">All spark event data relevant to spark executions</td>
</tr>
<tr>
<td style="text-align:left">sparkexecutor_gold</td>
<td style="text-align:left">sparkEvents</td>
<td style="text-align:left">gold</td>
<td style="text-align:left">All spark event data relevant to spark executors</td>
</tr>
<tr>
<td style="text-align:left">sparkjob_gold</td>
<td style="text-align:left">sparkEvents</td>
<td style="text-align:left">gold</td>
<td style="text-align:left">All spark event data relevant to spark jobs</td>
</tr>
<tr>
<td style="text-align:left">sparkstage_gold</td>
<td style="text-align:left">sparkEvents</td>
<td style="text-align:left">gold</td>
<td style="text-align:left">All spark event data relevant to spark stages</td>
</tr>
<tr>
<td style="text-align:left">sparktask_gold</td>
<td style="text-align:left">sparkEvents</td>
<td style="text-align:left">gold</td>
<td style="text-align:left">All spark event data relevant to spark tasks</td>
</tr>
<tr>
<td style="text-align:left">sparkstream_gold</td>
<td style="text-align:left">sparkEvents</td>
<td style="text-align:left">gold</td>
<td style="text-align:left">All spark event data relevant to spark streams</td>
</tr>
</tbody>
</table>


<footer class="footline">
	
</footer>

        
        </div> 
        

      </div>

    <div id="navigation">
        
        
        
        
            
            
                
                    
                    
                
                

                    
                    
                        
                    
                    

                    
                        
            
            
                
                    
                        
                        
                    
                
                

                    
                    
                    

                    
                        
            
            
                
                    
                
                

                    
                    
                    

                    
            
        
                    
                        
            
            
                
                    
                
                

                    
                    
                    

                    
            
        
                    
                        
            
            
                
                    
                
                

                    
                    
                    

                    
            
        
                    
            
        
                    
                        
            
            
                
                    
                
                

                    
                    
                    

                    
                        
            
            
                
                    
                
                

                    
                    
                    

                    
            
        
                    
                        
            
            
                
                    
                
                

                    
                    
                    

                    
            
        
                    
            
        
                    
                        
            
            
                
                    
                    
                
                

                    
                    
                        
                    
                    

                    
                        
            
            
                
                    
                    
                
                

                    
                    
                    

                    
                        
            
            
                
                    
                    
                
                

                    
                    
                    

                    
            
        
                    
            
        
                    
                        
            
            
                
                    
                        
                        
                    
                
                

                    
                    
                    

                    
            
        
                    
                        
            
            
                
                    
                
                

                    
                    
                    

                    
            
        
                    
                        
            
            
                
                    
                
                

                    
                    
                    

                    
            
        
                    
            
        
                    
                        
            
            
                
                    
                
                

                    
                    
                    

                    
            
        
                    
                        
            
            
                
                    
                
                

                    
                    
                    

                    
            
        
                    
                        
            
            
                
                    
                
                

                    
                    
                    

                    
            
        
                    
            
        
        
        


	 
	 
		
			<a class="nav nav-prev" href="https://databrickslabs.github.io/overwatch/dataengineer/definitions/" title="Data Dictionary (Latest)"> <i class="fa fa-chevron-left"></i></a>
		
		
			<a class="nav nav-next" href="https://databrickslabs.github.io/overwatch/dataengineer/pipeline_management/" title="Pipeline_Management" style="margin-right: 0px;"><i class="fa fa-chevron-right"></i></a>
		
	
    </div>

    </section>
    
    <div style="left: -1000px; overflow: scroll; position: absolute; top: -1000px; border: none; box-sizing: content-box; height: 200px; margin: 0px; padding: 0px; width: 200px;">
      <div style="border: none; box-sizing: content-box; height: 200px; margin: 0px; padding: 0px; width: 200px;"></div>
    </div>
    <script src="https://databrickslabs.github.io/overwatch/js/clipboard.min.js?1665782191"></script>
    <script src="https://databrickslabs.github.io/overwatch/js/perfect-scrollbar.min.js?1665782191"></script>
    <script src="https://databrickslabs.github.io/overwatch/js/perfect-scrollbar.jquery.min.js?1665782191"></script>
    <script src="https://databrickslabs.github.io/overwatch/js/jquery.sticky.js?1665782191"></script>
    <script src="https://databrickslabs.github.io/overwatch/js/featherlight.min.js?1665782191"></script>
    <script src="https://databrickslabs.github.io/overwatch/js/highlight.pack.js?1665782191"></script>
    <script>hljs.initHighlightingOnLoad();</script>
    <script src="https://databrickslabs.github.io/overwatch/js/modernizr.custom-3.6.0.js?1665782191"></script>
    <script src="https://databrickslabs.github.io/overwatch/js/learn.js?1665782191"></script>
    <script src="https://databrickslabs.github.io/overwatch/js/hugo-learn.js?1665782191"></script>

    <link href="https://databrickslabs.github.io/overwatch/mermaid/mermaid.css?1665782191" rel="stylesheet" />
    <script src="https://databrickslabs.github.io/overwatch/mermaid/mermaid.js?1665782191"></script>
    <script>
        mermaid.initialize({ startOnLoad: true });
    </script>
    

  </body>
</html>

