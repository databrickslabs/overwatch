[
{
	"uri": "http://localhost:1313/overwatch/deployoverwatch/ucedeployment/cloudstorageaccessrequirements/aws/",
	"title": "AWS",
	"tags": [],
	"description": "",
	"content": "Below are the requirements needed for Storage Access setup in AWS\nAWS IAM Role/Policy required for Storage Credentials\nTrust Relation required in Storage Credentials IAM Role\nInstance Profile required for Overwatch Job/Interactive Cluster\nAWS IAM Role required for Storage Credentials This IAM Role to authorize access to the external location. It will be configured while creating the Databricks Storage Credential. Below policy will be used for creating the IAM Role. Please refer this doc for a detailed description on creating IAM role for Storage Credentials\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Action\u0026#34;: [ \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:GetObjectVersion\u0026#34;, \u0026#34;s3:PutObject\u0026#34;, \u0026#34;s3:PutObjectAcl\u0026#34;, \u0026#34;s3:DeleteObject\u0026#34;, \u0026#34;s3:ListBucket\u0026#34;, \u0026#34;s3:GetBucketLocation\u0026#34;, \u0026#34;s3:GetLifecycleConfiguration\u0026#34;, \u0026#34;s3:PutLifecycleConfiguration\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::\u0026lt;EXTERNAL-LOCATION-BUCKET-NAME\u0026gt;/*\u0026#34;, \u0026#34;arn:aws:s3:::\u0026lt;EXTERNAL-LOCATION-BUCKET-NAME\u0026gt; \u0026#34; ], \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34; }, { \u0026#34;Action\u0026#34;: [ \u0026#34;kms:Decrypt\u0026#34;, \u0026#34;kms:Encrypt\u0026#34;, \u0026#34;kms:GenerateDataKey*\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:kms:\u0026lt;KMS_KEY\u0026gt;\u0026#34; ], \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34; }, { \u0026#34;Action\u0026#34;: [ \u0026#34;sts:AssumeRole\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:iam::\u0026lt;AWS-ACCOUNT-ID\u0026gt;:role/\u0026lt;THIS-IAM-ROLE\u0026gt;\u0026#34; ], \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34; } ] } Trust Relation required in Storage Credentials IAM Role Add the below policy in Trust Relation of Storage Credentials IAM, to make this role self-assuming. Please refer this doc for a detailed description on setting up Trust relation while creating IAM role for Storage Credentials\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;AWS\u0026#34;: \u0026#34;arn:aws:iam::414351767826:role/unity-catalog-prod-UCMasterRole-14S5ZJVKOTYTL\u0026#34; //DO NOT CHANGE }, \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRole\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: { \u0026#34;sts:ExternalId\u0026#34;: \u0026#34;e6e8162c-a42f-43a0-af86-312058795a14\u0026#34; } } } ] } Instance Profile required for Overwatch Job Cluster After the Storage Credentials is created, the existing instance profile attached to the Overwatch Job cluster needs to be provisioned read/write access to the storage target for the Overwatch Output (which will ultimately become your external location). This Instance profile can be used in the Job CLuster/Interactive Cluster running Overwatch.\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;PermitSelectedBucketsList\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:ListBucket\u0026#34;, \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:PutObject\u0026#34;, \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:DeleteObject\u0026#34;, \u0026#34;s3:PutObjectAcl\u0026#34;, \u0026#34;s3:GetBucketNotification\u0026#34;, \u0026#34;s3:PutBucketNotification\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::\u0026lt;EXTERNAL-LOCATION-BUCKET-NAME\u0026gt;/*\u0026#34;, \u0026#34;arn:aws:s3:::\u0026lt;EXTERNAL-LOCATION-BUCKET-NAME\u0026gt; \u0026#34; ] }, { \u0026#34;Action\u0026#34;: [ \u0026#34;kms:Decrypt\u0026#34;, \u0026#34;kms:Encrypt\u0026#34;, \u0026#34;kms:GenerateDataKey*\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:kms:\u0026lt;KMS_KEY\u0026gt;\u0026#34; ], \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;DenyAuditLogsBucketCRUD\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Deny\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:PutObject\u0026#34;, \u0026#34;s3:PutObjectAcl\u0026#34;, \u0026#34;s3:DeleteObject\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::\u0026lt;AUDIT-LOG-BUCKET-NAME\u0026gt;/*\u0026#34; ] } ] } Why Is Delete Required In The Policy\nTEMP DIRECTORY Overwatch scrapes the data from the logs and from the api calls, as mentioned in its documentation. It requires a temporary location to write intermediate datasets. DELTA VACUUM The delta tables need to be vacuumed and optimized to maintain efficiency and optimize the tables for downstream use Please visit the Databricks docs for more details on Delta Optimize \u0026amp; Delta Vacuum. "
},
{
	"uri": "http://localhost:1313/overwatch/deployoverwatch/ucedeployment/cloudstorageaccessrequirements/",
	"title": "UC Storage Requirements",
	"tags": [],
	"description": "",
	"content": "As a prerequisite for Overwatch UC Enablement it is required that Overwatch Job Cluster have Read/Write access to UC external location(s3 bucket, Azure ADLS container, GC Bucket ) and Read (at least) to the cluster log storage locations. Please find the specific configuration based on the cloud provider in the below section.\nThis access is required because the Overwatch pipelines utilize low-level file access from executors to optimize interaction with log files produced from the clusters. Going directly to the files to scan modification dates and validate sizes / etc are much more efficient (12-20x) then doing this sequentially through the external location APIS. Databricks is in the process of making \u0026ldquo;Volumes\u0026rdquo; GA and Overwatch will continue to monitor capabilities and remove these requirements as soon as its feasible.\nAzure AWS GCP "
},
{
	"uri": "http://localhost:1313/overwatch/deployoverwatch/runningoverwatch/clusterconfig/",
	"title": "Cluster Configuration",
	"tags": [],
	"description": "",
	"content": "Cluster Requirements DBR 11.3LTS as of 0.7.1.0 Overwatch will likely run on different versions of DBR but is built and tested on 11.3LTS since 0.7.1 Overwatch \u0026lt; 0.7.1 \u0026ndash; DBR 10.4LTS Overwatch \u0026lt; 0.6.1 \u0026ndash; DBR 9.1LTS Using Photon As of 0.7.1.0 Photon is recommended so long as the Overwatch cluster is using DBR 11.3LTS+. Photon does increase the DBU spend but the performance boost often results in the code running significantly more efficiently netting out a benefit. Mileage can vary between customers so if you really want to know which is most efficient, feel free to run on both and use Overwatch to determine which is best for you. Prior to 0.7.1.0 and DBR 11.3LTS Photon was untested Disable Autoscaling - See Notes On Autoscaling External optimize cluster recommendations are different. See External Optimize for more details Add the relevant dependencies Cluster Dependencies Add the following dependencies to your cluster\nOverwatch Assembly (fat jar): com.databricks.labs:overwatch_2.12:\u0026lt;latest\u0026gt; (Azure Only, if not using System Tables) azure-eventhubs-spark - integration with Azure EventHubs Maven Coordinate: com.microsoft.azure:azure-eventhubs-spark_2.12:2.3.21 (Azure Only - With AAD Auth For EH, if not using system tables) msal4j - library to support AAD Authorization Maven Coordinate: com.microsoft.azure:msal4j:1.10.1 If maven isn\u0026rsquo;t accessible in your environment you have two options: On Github, go to the Releases page, find the version you\u0026rsquo;re interested in, and scroll down to the Assets section, there you can download the jar or uber jar, as needed. Compile an uber jar with all dependencies. To do this, download the uber_pom.xml (Ensure the Overwatch version specified in the pom file is the version you want, there are 2 places to check) and run mvn clean package from within the same directory as the pom.xml This method does require you have maven installed and configured correctly Cluster Config Recommendations Azure Node Type (Driver \u0026amp; Worker) - Standard_D16s_v3 Use n Standard_E*d[s]_v4 for workers for historical loads and Photon Large / Very Large workspaces may see a significant improvement using Standard_E16d[s]_v4 workers but mileage varies, cost/benefit analysis required Node Count - 2 This may be increased if necessary but note that bronze is not linearly scalable; thus, increasing core count may not improve runtimes. Please see Optimizing Overwatch for more information. AWS Node Type (Driver \u0026amp; Worker) - R5d.4xlarge Use n i3.*xlarge for workers for historical loads Large / Very Large workspaces may see a significant improvement using i3.4xlarge workers but mileage varies, cost/benefit analysis required Node Count - 2 This may be increased if necessary but note that bronze is not linearly scalable; thus, increasing core count may not improve runtimes. Please see Optimizing Overwatch for more information. Notes On Autoscaling Auto-scaling compute \u0026ndash; Not Recommended Note that autoscaling compute will not be extremely efficient due to some of the compute tails as a result of log file size skew and storage mediums. Additionally, some modules require thousands of API calls (for historical loads) and have to be throttled to protect the workspace. Auto-scaling Local Storage \u0026ndash; Strongly Recommended for historical loads Some of the data sources can grow to be quite large and require very large shuffle stages which requires sufficient local disk. If you choose not to use auto-scaling storage be sure you provision sufficient local disk space. SSD or NVME (preferred) \u0026ndash; It\u0026rsquo;s strongly recommended to use fast local disks as there can be very large shuffles "
},
{
	"uri": "http://localhost:1313/overwatch/deployoverwatch/configureoverwatch/configuration/",
	"title": "Configuration",
	"tags": [],
	"description": "",
	"content": "Overwatch Deployment Configuration How it works Overwatch deployment is driven by a configuration file which will ultimately be loaded into the deployment as a csv format or delta table. This configuration will contain all the necessary details to perform the deployment. Since CSVs are a bit cantankerous we\u0026rsquo;ve offered two different methods for building the configuration file. If you\u0026rsquo;re good at VSCode or similar text editor and want to edit the CSV directly feel free to do so. We strongly recommend that you create a delta table with the csv file you just created and use that as your configuration input.\nIF YOU INTEND TO USE EXCEL (RECOMMENDED) note that Excel may alter several pieces of the CSV; thus we recommend you complete all your edits in the .xlsx file and then Save As a .csv extension. Before you upload the file spot-check the CSV to ensure everything looks correct. Rest assured though, there are many validations that will alert you if there are any issues.\nIF YOU INTEND TO USE CSV do not open or edit the CSV in Excel unless you\u0026rsquo;re skilled with editing CSVs in Excel. Excel may attempt to auto-format certain fields and can break the required format, especially workspace_id and date fields.\nRegardless of the method you choose, DO NOT DELETE FIELDS you don\u0026rsquo;t think you need. If the field doesn\u0026rsquo;t pertain to you just leave it blank to avoid any strange issues when parsing the CSV.\nBuilding Your Config Please use the template for your version. If you are upgrading from 071x to 072x the upgrade script in the change log will upgrade the script for you.\n0.8.0.x+ (.XLSX | .CSV) Added sql_endpoint column for multi account deployment 0.7.2.x+ (.XLSX | .CSV) Two column names were changed in 072x to make the scope a little broader \u0026ldquo;etl_storage_prefix\u0026rdquo; \u0026ndash;\u0026gt; \u0026ldquo;storage_prefix\u0026rdquo; \u0026ldquo;auditlogprefix_source_aws\u0026rdquo; \u0026ndash;\u0026gt; \u0026ldquo;auditlogprefix_source_path\u0026rdquo; 0.7.1.x (.XLSX | .CSV) To Download Right-click the file type you want and click \u0026ldquo;Save link as\u0026hellip;\u0026rdquo; or \u0026ldquo;Save target as\u0026hellip;\u0026rdquo;\nColumn description Below are the full details of each column in the latest config. For exact/complete configuration options by version please see Configuration Details By Version\nColumn Type IsRequired Description workspace_name String True Name of the workspace. workspace_id String True Id of the workspace. MUST BE VALUE AFTER THE o= in the URL bar. To ensure you get the right value, run the following on the target workspace. Initializer.getOrgId workspace_url String True URL of the workspace. Should be in format of https://*.com or https://*.net. Don\u0026rsquo;t include anything after the .com or .net suffix api_url String True API URL for the Workspace (execute in scala dbutils.notebook.getContext().apiUrl.get ON THE TARGET WORKSPACE NOT DEPLOYMENT WORKSPACE to get the API URL for the workspace. NOTE: Workspace_URL and API_URL can be different for a workspace but may be the same even for multiple workspaces). You can also use the workspace_url here. cloud String True Cloud provider (Azure/AWS/GCP). primordial_date String True The date from which Overwatch will capture the details. The format should be yyyy-MM-dd ex: 2022-05-20 == May 20 2022. **IMPORTANT NOTE: ** You should only set the primordial date in the initial run of Overwatch, and never change it again, as Overwatch will progress the dates using it\u0026rsquo;s own calculations and checkpoints. storage_prefix String True CASE SENSITIVE - Lower Case The location in which Overwatch will store the data. You can think of this as the Overwatch working directory. dbfs:/mnt/path/\u0026hellip; or abfss://container@myStorageAccount.dfs.core.windows.net/\u0026hellip; or s3://myBucket/\u0026hellip; or gs://myBucket/\u0026hellip; etl_database_name String True The name of the ETL data base for Overwatch (i.e. overwatch_etl or custom) consumer_database_name String True The name of the Consumer database for Overwatch. (i.e. overwatch or custom) secret_scope String True Name of the secret scope. This must be created on the workspace which the Overwatch job will execute. secret_key_dbpat String True This will contain the PAT token of the workspace. The key should be present in the secret_scope and should start with the letters dapi. auditlogprefix_source_path String True For all clouds use keyword system to fetch data from System Tables (system.access.audit) See System Table Configuration Details for details. If you are not using System Tables, you can enter the location of the auditlogs (AWS/GCP Only). The contents under this directory must have the folders with the date partitions like date=2022-12-0 . interactive_dbu_price Double True Contract (or list) Price for interactive DBUs. The provided template has the list prices by default. automated_dbu_price Double True Contract (or list) Price for automated DBUs. The provided template has the list prices by default. sql_compute_dbu_price Double True Contract (or list) Price for DBSQL DBUs. This should be the closest average price across your DBSQL Skus (classic / Pro / Serverless) for now. See Custom Costs for more details. The provided template has the DBSQL Classic list prices by default. jobs_light_dbu_price Double True Contract (or list) Price for interactive DBUs. The provided template has the list prices by default. max_days Integer True This is the max incrementals days that will be loaded. Usually only relevant for historical loading and rebuilds. Recommendation == 30 excluded_scopes String False Scopes that should not be excluded from the pipelines. Since this is a CSV, it\u0026rsquo;s critical that these are colon delimited. Leave blank if you\u0026rsquo;d like to load all overwatch scopes. active Boolean True Whether or not the workspace should be validated / deployed. proxy_host String False Proxy url for the workspace. proxy_port String False Proxy port for the workspace proxy_user_name String False Proxy user name for the workspace. proxy_password_scope String False Scope which contains the proxy password key. proxy_password_key String False Key which contains proxy password. success_batch_size Integer False API Tunable - Indicates the size of the buffer on filling of which the result will be written to a temp location. This is used to tune performance in certain circumstances. Leave default except for special circumstances. Default == 200 error_batch_size Integer False API Tunable - Indicates the size of the error writer buffer containing API call errors. This is used to tune performance in certain circumstances. Leave default except for special circumstances. Default == 500 enable_unsafe_SSL Boolean False API Tunable - Enables unsafe SSL. Default == False thread_pool_size Integer False API Tunable - Max number of API calls Overwatch is allowed to make in parallel. Default == 4. Increase for faster bronze but if workspace is busy, risks API endpoint saturation. Overwatch will detect saturation and back-off when detected but for safety never go over 8 without testing. api_waiting_time Long False API Tunable - Overwatch makes async api calls in parallel, api_waiting_time signifies the max wait time in case of no response received from the api call. Default = 300000(5 minutes) mount_mapping_path String False Path to local CSV holding details of all mounts on remote workspaces (only necessary for remote workspaces with \u0026gt;50 mounts) click here for more details temp_dir_path String False Custom temporary working directory, directory gets cleaned up after each run. sql_endpoint String False Use http path from warehouse connection details. **IMPORTANT NOTE: ** This column only needs to be added for multi account deployment, see Multi Account System Table Integration for more details Azure Event Hub Specific Configurations When configuring the Azure EH configurations users can use EITHER a shared access key OR AAD SP as of 072x to authenticate to the EH. Below are the required configurations for each auth method. One of the options for Azure deployments must be used as EH is required for Azure.\nShared Access Key Requirements Review Authorizing Access Via SAS Policy for more details.\nColumn Type IsRequired Description eh_name String True (AZURE) Event hub name (Azure Only) The event hub will contain the audit logs of the workspace eh_scope_key String True (AZURE) Name of the key in the \u0026lt;secret_scope\u0026gt; that holds the connection string to the Event Hub WITH THE SHARED ACCESS KEY IN IT \u0026ndash; See EH Configuration for details AAD Requirements\nReview Authorizing Access Via AAD SPN for more details.\nEnsure the dependent library for AAD Auth is attached com.microsoft.azure:msal4j:1.10.1\nColumn Type IsRequired Description eh_name String True (AZURE) Event hub name The event hub will contain the audit logs of the workspace eh_conn_string String True (AZURE) Event hub connection string without shared access key. ex: \u0026ldquo;Endpoint=sb://evhub-ns.servicebus.windows.net\u0026rdquo; aad_tenant_id String True (AZURE) Tenant ID for Service principle. aad_client_id String True (AZURE) Client ID for Service principle. aad_client_secret_key String True (AZURE) Name of the key in the \u0026lt;secret_scope\u0026gt; that holds the SPN secret for the Service principle. aad_authority_endpoint String True (AZURE) Endpoint of the authority. Default value is \u0026ldquo;https://login.microsoftonline.com/\u0026quot; Converting Your Config From CSV To Delta (STRONGLY RECOMMENDED) AS OF version 0.7.1.1 you may now use a CSV OR a Delta Table OR Delta Path for your config\nWe heard you! Customers want to set up their initial config as a CSV to get all their workspaces configured but once they\u0026rsquo;re configured it\u0026rsquo;s challenging to make small edits. Now you can convert your initial CSV to delta and instead of referencing the path to a config.csv file instead reference a delta table or delta path to the location of your config. This allows for simple update statements to switch records from active == true to active == false or quickly disable scopes, etc. Below are the details of how to reference the relevant path and some code to quickly and safely convert your CSV to a delta source. To do so just upgrade to 0.7.1.1+ and everywhere the docs reference the config.csv switch it to reference the appropriate path.\nSource Config Format Path Reference CSV dbfs:/myPath/overwatch/configs/prod_config.csv Delta Table database.overwatch_prod_config Delta Path dbfs:/myPath/overwatch/configs/prod_config Convert To Delta Table (Example)\nspark.read .option(\u0026#34;header\u0026#34;, \u0026#34;true\u0026#34;) .option(\u0026#34;ignoreLeadingWhiteSpace\u0026#34;, true) .option(\u0026#34;ignoreTrailingWhiteSpace\u0026#34;, true) .csv(\u0026#34;/path/to/config.csv\u0026#34;) .coalesce(1) .write.format(\u0026#34;delta\u0026#34;) .saveAsTable(\u0026#34;database.overwatch_prod_config\u0026#34;) Convert To Delta Path (Example)\nspark.read .option(\u0026#34;header\u0026#34;, \u0026#34;true\u0026#34;) .option(\u0026#34;ignoreLeadingWhiteSpace\u0026#34;, true) .option(\u0026#34;ignoreTrailingWhiteSpace\u0026#34;, true) .csv(\u0026#34;/path/to/config.csv\u0026#34;) .coalesce(1) .write.format(\u0026#34;delta\u0026#34;) .save(\u0026#34;/myPath/overwatch/configs/prod_config\u0026#34;) "
},
{
	"uri": "http://localhost:1313/overwatch/deployoverwatch/cloudinfra/",
	"title": "Cloud Infrastructure",
	"tags": [],
	"description": "",
	"content": "This section will walk you through the steps necessary to deploy Overwatch on a specific cloud. The references are left fairly generic as Overwatch is built to be extremely flexible. Overwatch currently supports hundreds of enterprise customers and they all come with their own variation of cloud controls and requirements. The references are a simple place to start from and if you need assistance finding the optimal solution for your deployment, please reach out to your account team and one of our specialists will help you find the right architecture / configuration to meet your needs.\nCloud Infrastructure Setup Azure AWS GCP "
},
{
	"uri": "http://localhost:1313/overwatch/dataengineer/definitions/",
	"title": "Data Dictionary (Latest)",
	"tags": [],
	"description": "",
	"content": "ERD The \u0026ldquo;ERD\u0026rdquo; below is a visual representation of the consumer layer data model. Many of the joinable lines have been omitted to reduce chaos and complexity in the visualization. All columns with the same name are joinable (even if there\u0026rsquo;s not a line from one table to the other). The relations depicted are to call the analyst\u0026rsquo;s attention to less obvious joins.\nThe goal is to present a data model that unifies the different parts of the platform. The Overwatch team will continue to work with Databricks platform teams to publish and simplify this data. The gray boxes annotated as \u0026ldquo;Backlog/Research\u0026rdquo; are simply a known gap, and a pursuit of the Overwatch dev team, it does NOT mean it\u0026rsquo;s going to be released soon but rather that we are aware of the missing component, and we hope to enable gold-level data here in the future.\nConsumption Layer \u0026ldquo;Tables\u0026rdquo; (Views) All end users should be hitting consumer tables first. Digging into lower layers gets significantly more complex. Below is the data model for the consumption layer. The consumption layer is often in a stand-alone database apart from the ETL tables to minimize clutter and confusion. These entities in this layer are actually not tables at all (with a few minor exceptions such as lookup tables) but rather views. This allows for the Overwatch development team to alter the underlying columns, names, types, and structures without breaking existing transformations. Instead, view column names will remain the same but may be repointed to a newer version of a column, etc.\nETL should not be developed atop the consumption layer views but rather the gold layer. Before Overwatch version upgrades, it\u0026rsquo;s important that the engineering team review the change list and upgrade requirements before upgrading. These upgrades may require a remap depending on the changes. As of version 1.0 release, all columns in the gold layer will be underscored with their schema version number, column changes will reference the later release version but the views published with Overwatch will almost always point to the latest version of each column and will not include the schema suffix to simplify the data model for the average consumer.\nData Organization The large gray boxes in the simplified ERD below depict the two major, logical sections of the data model:\nDatabricks Platform - Metadata captured by the Databricks platform that can be used to assist in workspace governance. This data can also be enriched with the Spark data enabling in-depth analyses. The breadth of metadata is continuing to grow, stay tuned for additional capabilities. Spark UI The spark UI section is derived from the spark event logs and essentially contains every single piece of data from the Spark UI. There are a few sections that are not included in the first release but the data is present in spark_events_bronze albeit extremely complex to derive. The Overwatch development team is working tirelessly to expose additional SparkUI data and will publish as soon as it\u0026rsquo;s ready. Column Descriptions Complete column descriptions are only provided for the consumption layer. The entity names are linked below.\ncluster\nclusterStateFact\ninstanceDetails\njob\njobrun\njobRunCostPotentialFact\nsqlQueryHistory\nnotebook\ninstancePool\ndbuCostDetail\naccountLogin\naccountMod\nsparkExecution\nsparkExecutor\nsparkJob\nsparkStage\nsparkTask\nsparkStream\nwarehouse\nnotebookCommands\nCommon Meta Fields\nThere are several fields that are present in all tables. Instead of cluttering each table with them, this section was created as a reference to each of these. Most tables below provide a data SAMPLE for reference. You may either click to view it or right click the SAMPLE link and click saveTargetAs or saveLinkAs and save the file. Note that these files are TAB delimited, so you will need to view as such if you save to local file. The data in the files were generated from an Azure, test deployment created by Overwatch Developers.\nCluster SAMPLE\nKEY \u0026ndash; organization_id + cluster_id + unixTimeMS\nIncremental Columns \u0026ndash; unixTimeMS\nPartition Columns \u0026ndash; organization_id\nWrite Mode \u0026ndash; Append\nColumn Type Description cluster_id string Canonical Databricks cluster ID (more info in Common Meta Fields) action string create, edit, or snapImpute \u0026ndash; depicts the type of action for the cluster \u0026ndash; **snapImpute is used on first run to initialize the state of the cluster even if it wasn\u0026rsquo;t created/edited since audit logs began timestamp timestamp timestamp the action took place cluster_name string user-defined name of the cluster driver_node_type string Canonical name of the driver node type. node_type string Canonical name of the worker node type. num_workers int The number of workers defined WHEN autoscaling is disabled autoscale struct The min/max workers defined WHEN autoscaling is enabled auto_termination_minutes int The number of minutes before the cluster auto-terminates due to inactivity enable_elastic_disk boolean Whether autoscaling disk was enabled or not is_automated booelan Whether the cluster is automated (true if automated false if interactive) cluster_type string Type of cluster (i.e. Serverless, SQL Analytics, Single Node, Standard) security_profile struct Complex type to describe security features enabled on the cluster. More information Below cluster_log_conf string Logging directory if configured init_script array Array of init scripts custom_tags string User-Defined tags AND also includes Databricks JobID and Databricks RunName when the cluster is created by a Databricks Job as an automated cluster. Other Databricks services that create clusters also store unique information here such as SqlEndpointID when a cluster is created by \u0026ldquo;SqlAnalytics\u0026rdquo; cluster_source string Shows the source of the action spark_env_vars string Spark environment variables defined on the cluster spark_conf string custom spark configuration on the cluster that deviate from default acl_path_prefix string Automated jobs pass acl to clusters via a path format, the path is defined here instance_pool_id string Canonical pool id from which workers receive nodes driver_instance_pool_id string Canonical pool id from which driver receives node instance_pool_name string Name of pool from which workers receive nodes driver_instance_pool_name string Name of pool from which driver receives node spark_version string DBR version - scala version idempotency_token string Idempotent jobs token if used ClusterStateFact SAMPLE\nKEY \u0026ndash; organization_id + cluster_id + state + unixTimeMS_state_start\nIncremental Columns \u0026ndash; state_start_date + unixTimeMS\nPartition Columns \u0026ndash; organization_id + state_start_date\nZ-Order Columns \u0026ndash; cluster_id + unixTimeMS_state_start\nWrite Mode \u0026ndash; Merge\nUnsupported Scenarios A few scenarios are not yet supported by Overwatch; they are called out here. Please stay tuned for updates as it\u0026rsquo;s our intention to include everything we can as soon as possible after Databricks product GAs new features but there will be a delay.\nCosts for DBSQL clusters Even though DBSQL warehouses may show up here (they normally will not), the costs for these cannot be calculated at this time. Databricks doesn\u0026rsquo;t yet publish the warehouse event logs (i.e. start/stop/scale) and until that is available, we cannot estimate costs for warehouses like we do for traditional clusters. Costs for Fleet Instance Type Right now for AWS Fleet Clusters are not supported. For Fleet Instance type specific instances are determined by the price and capacity conditions at the time of cluster creation, and right now Databricks doesn\u0026rsquo;t publish information when each of these specific instances are allocated for fleet instance type, so we cannot estimate costs for warehouses like we do for traditional clusters. Costs and state details by cluster at every state in the cluster lifecycle. The Cost Functions are detailed below the definitions of this table.\nAny static clusters spanning 90 days without any state changes will never get a state closure and result in costs increasing forever. This should be a VERY rare circumstance and usually only happens in extremely stable, small streams. This max days for clsf will be externalized as an override config in the future but for now it\u0026rsquo;s static.\nThis fact table is not normalized on time. Some states will span multiple days and must be smoothed across days (i.e. divide by days_in_state) when trying to calculate costs by day. All states are force-terminated at the end of the Overwatch run to the until-timestamp of the run. If the state was still active at this time, it will be updated on the subsequent run.\nColumn Type Description cluster_id string Canonical Databricks cluster ID (more info in Common Meta Fields) cluster_name string Name of cluster at beginning of state custom_tags string JSON string of key/value pairs for all cluster associated custom tags give to the cluster *_state_start various timestamp reference column at the time the state began *_state_end various timestamp reference column at the time the state ended state string state of the cluster \u0026ndash; full list HERE current_num_workers long number of workers in use by the cluster at the start of the state target_num_workers long number of workers targeted to be present by the completion of the state. Should be equal to current_num_workers except during RESIZING state uptime_since_restart_S double Seconds since the cluster was last restarted / terminated uptime_in_state_S double Seconds the cluster spent in current state uptime_in_state_H double Hours the cluster spent in current state driver_node_type_id string KEY of driver node type to enable join to instanceDetails node_type_id string KEY of worker node type to enable join to instanceDetails cloud_billable boolean All current known states are cloud billable. This means that cloud provider charges are present during this state databricks_billable boolean State incurs databricks DBU costs. All states incur DBU costs except: INIT_SCRIPTS_FINISHED, INIT_SCRIPTS_STARTED, STARTING, TERMINATING, CREATING, RESTARTING isAutomated boolean Whether the cluster was created as an \u0026ldquo;automated\u0026rdquo; or \u0026ldquo;interactive\u0026rdquo; cluster dbu_rate double Effective dbu rate used for calculations (effective at time of pipeline run) excluding dbu increases due to photon \u0026ndash; photon uplifts included in dbu_totals runtime_engine string One of STANDARD or PHOTON. When PHOTON, pricing is adjusted when deriving the dbu_costs state_dates array Array of all dates across which the state spanned days_in_state int Number of days in state worker_potential_core_H double Worker core hours available to execute spark tasks core_hours double All core hours of entire cluster (including driver). Nodes * cores * hours in state driver_compute_cost double Compute costs associated with driver runtime driver_dbu_cost double DBU costs associated with driver runtime worker_compute_cost double Compute costs associated with worker runtime worker_dbu_cost double DBU costs associated with cumulative runtime of all worker nodes total_driver_cost double Driver costs including DBUs and compute total_worker_cost double Worker costs including DBUs and compute total_compute_cost double All compute costs for Driver and Workers total_dbu_cost double All dbu costs for Driver and Workers total_cost double Total cost from Compute and DBUs for all nodes (including Driver) driverSpecs struct Driver node details workerSpecs struct Worker node details Cost Functions Explained EXPECTATIONS \u0026ndash; Note that Overwatch costs are derived. This is good and bad. Good as it allows for costs to be broken down by any dimension at the millisecond level. Bad because there can be significant differences between the derived costs and actual costs. These should generally be very close to equal but may differ within margin of error by as much as 10%. To verify the cost functions and the elements therein feel free to review them in more detail. If your costs are off by a large margin, please review all the components of the cost function and correct any configurations as necessary to align your reality with the Overwatch config. The default costs are list price and often do not accurately reflect a customer\u0026rsquo;s costs.\ncloudBillable: Cluster is in a running state GAP: Note that cloud billable ends at the time the cluster is terminated even though the nodes remain provisioned in the cloud provider for several more minutes; these additional minutes are not accounted for in this cost function. driver_compute_cost: when cloudBillable \u0026ndash;\u0026gt; Driver Node Compute Contract Price Hourly (instanceDetails) * Uptime_In_State_H \u0026ndash;\u0026gt; otherwise 0 worker_compute_cost: when cloudBillable \u0026ndash;\u0026gt; Worker Node Compute Contract Price Hourly (instanceDetails) * Uptime_In_State_H * target_num_workers \u0026ndash;\u0026gt; otherwise 0 target_num_workers used here is ambiguous. Assuming all targeted workers can be provisioned, the calculation is most accurate; however, if some workers cannot be provisioned the worker_compute_cost will be slightly higher than actual while target_num_workers \u0026gt; current_num_workers. target_num_workers used here because the compute costs begin accumulating as soon as the node is provisioned, not at the time it is added to the cluster. photon_kicker: WHEN runtime_engine == \u0026ldquo;Photon\u0026rdquo; and sku != \u0026ldquo;SqlCompute\u0026rdquo; and isAutomated THEN 2.9 WHEN runtime_engine == \u0026ldquo;Photon\u0026rdquo; and sku != \u0026ldquo;SqlCompute\u0026rdquo; and !isAutomated THEN 2 otherwise 1 worker_dbus: WHEN databricks_billable and !SingleNode THEN current_num_workers * driver_hourly_dbus (instancedetails.hourlyDBUs) * uptime_in_state_H * photon_kicker NOTE: current_num_workers == current_worker, not target. current_num_workers only includes worker nodes after they have become ready and able to receive workloads. otherwise 0 driver_dbus: when databricks_billable \u0026ndash;\u0026gt; driver_hourly_dbus (instancedetails.hourlyDBUs) * uptime_in_state_H * photon_kicker \u0026ndash;\u0026gt; otherwise 0 NOTE: In Single Node Clusters \u0026ndash; only the driver will have dbus worker_dbu_cost: houry_dbu_rate for sku (dbuCostDetails.contract_price) * worker_dbus driver_dbu_cost: houry_dbu_rate for sku (dbuCostDetails.contract_price) * driver_dbus Cost may not appear for a cluster until a state change is observed (i.e. starting/terminating/expanded_disk/resizing/etc). This means that Overwatch may not recognize costs for a cluster until at least one state change has been observed by Overwatch since the primordial date (or first run date - 30d whichever is greater).\nInstanceDetails AWS Sample | AZURE_Sample\nKEY \u0026ndash; Organization_ID + API_name\nIncremental Columns \u0026ndash; Pipeline_SnapTS\nPartition Columns \u0026ndash; organization_id\nWrite Mode \u0026ndash; Append\nThis table is unique and it\u0026rsquo;s purpose is to enable users to identify node specific contract costs associated with Databricks and the Cloud Provider through time. Defaults are loaded as an example by workspace. These defaults are meant to be reasonable, not accurate by default as there is a wide difference between cloud discount rates and prices between regions / countries. Everytime Overwatch runs, it validates the presence of this table and whether it has any data present for the current workspace, if it does not it creates and appends the relevant data to it; otherwise no action is taken. This gives the user the ability to extend / customize this table to fit their needs by workspace. Each organization_id (workspace), should provide complete cost data for each node used in that workspace. If you decide to completely customize the table, it\u0026rsquo;s critical to note that some columns are required for the ETL to function; these fields are indicated below in the table with an asterisk.\nThe organization_id (i.e. workspace id) is automatically generated for each workspace if that organization_id is not present in the table already (or the table is not present at all). Each workspace (i.e. organization_id) often has unique costs, this table enables you to customize compute pricing.\nIMPORTANT This table must be configured such that there are no overlapping costs (by time) and no gaps (by time) in costs for any key (organization_id + API_name) between primordial date and current date. This means that for a record to be \u0026ldquo;expired\u0026rdquo; the following must be true:\noriginal key expired by setting activeUntil == expiry date original key must be created with updated information and must: have activeFrom == expiry date of previous record (no gap, no overlap) have activeUntil == lit(null).cast(\u0026ldquo;date\u0026rdquo;) Azure VM Pricing Page\nAWS EC2 Pricing Page\nColumn Type Description instance string Common name of instance type API_name* string Canonical KEY name of the node type \u0026ndash; use this to join to node_ids elsewhere vCPUs* int Number of virtual cpus provisioned for the node type Memory_GB double Gigabyes of memory provisioned for the node type Compute_Contract_Price* double Contract price for the instance type as negotiated between customer and cloud vendor. This is the value used in cost functions to deliver cost estimates. It is defaulted to equal the on_demand compute price On_Demand_Cost_Hourly double On demand, list price for node type DISCLAIMER \u0026ndash; cloud provider pricing is dynamic and this is meant as an initial reference. This value should be validated and updated to reflect actual pricing Linux_Reserved_Cost_Hourly double Reserved, list price for node type DISCLAIMER \u0026ndash; cloud provider pricing is dynamic and this is meant as an initial reference. This value should be validated and updated to reflect actual pricing Hourly_DBUs* double Number of DBUs charged for the node type is_active boolean whether the contract price is currently active. This must be true for each key where activeUntil is null activeFrom* date The start date for the costs in this record. NOTE this MUST be equal to one other record\u0026rsquo;s activeUntil unless this is the first record for these costs. There may be no overlap in time or gaps in time. activeUntil* date The end date for the costs in this record. Must be null to indicate the active record. Only one record can be active at all times. The key (API_name) must have zero gaps and zero overlaps from the Overwatch primordial date until now indicated by null (active) dbuCostDetails KEY \u0026ndash; Organization_ID + sku\nIncremental Columns \u0026ndash; activeFrom\nPartition Columns \u0026ndash; organization_id\nWrite Mode \u0026ndash; Append\nSlow-changing dimension to track DBU contract costs by workspace through time. This table should only need to be edited in very rare circumstances such as historical cost correction. Note that editing these contract prices will not retroactively modify historical pricing in the costing table such as clusterStateFact or jobRunCostPotentialFact. For prices to be recalculated, the gold pipeline modules must be rolled back properly such that the costs can be rebuilt with the updated values.\nColumn Type Description sku string One of automated, interactive, jobsLight, sqlCompute contract_price double Price paid per DBU on the sku is_active boolean whether the contract price is currently active. This must be true for each key where activeUntil is null activeFrom* date The start date for the costs in this record. NOTE this MUST be equal to one other record\u0026rsquo;s activeUntil unless this is the first record for these costs. There may be no overlap in time or gaps in time. activeUntil* date The end date for the costs in this record. Must be null to indicate the active record. Only one record can be active at all times. The key (API_name) must have zero gaps and zero overlaps from the Overwatch primordial date until now indicated by null (active) Job SAMPLE\nThe below columns closely mirror the APIs listed below by action. For more details about these fields and their structures please reference the relevant Databricks Documentation for the action.\nNote \u0026ndash; Databricks has moved to API2.1 for all jobs-related functions and in-turn, Databricks has moved several fields from the root level to a nested level to support multi-task jobs. These root level fields are still visible in Overwatch as some customers are still using legacy APIs and many customers have historical data by which this data was generated using the legacy 2.0 APIs. These fields can be identified by the prefix, \u0026ldquo;Legacy\u0026rdquo; in the Description and have been colored red on the ERD.\nAction API SnapImpute Only created during the first Overwatch Run to initialize records of existing jobs not present in the audit logs. These jobs are still available in the UI but have not been modified since the collection of audit logs begun thus no events have been identified and therefore must be imputed to maximize coverage Create \u0026ldquo;Create New Job API\u0026rdquo; Update \u0026ldquo;Partially Update a Job\u0026rdquo; Reset \u0026ldquo;Overwrite All Settings for a Job\u0026rdquo; Delete \u0026ldquo;Delete A Job\u0026rdquo; ChangeJobAcl \u0026ldquo;Update Job Permissions\u0026rdquo; ResetJobAcls \u0026ldquo;Replace Job Permissions\u0026rdquo; \u0026ndash; Not yet supported KEY \u0026ndash; organization_id + job_id + unixTimeMS + action + request_id\nIncremental Columns \u0026ndash; unixTimeMS\nPartition Columns \u0026ndash; organization_id\nWrite Mode \u0026ndash; Append\nColumn Type Description organization_id string Canonical workspace id workspace_name string Customer defined name of the workspace or workspace_id (default) job_id long Databricks job id action string Action type defined by the record. One of: create, reset, update, delete, resetJobAcl, changeJobAcl. More information about these actions can be found here date date Date of the action for the key timestamp timestamp Timestamp the action took place job_name string User defined name of job. tags map The tags applied to the job if they exist tasks array The tasks defined for the job job_clusters array The job clusters defined for the job libraries array LEGACY \u0026ndash; Libraries defined in the job \u0026ndash; Nested within tasks as of API 2.1 timeout_seconds string Job-level timeout seconds. Databricks supports timeout seconds at both the job level and the task level. Task level timeout_seconds can be found nested within tasks max_concurrent_runs long Job-level \u0026ndash; maximum concurrent executions of the job max_retries long LEGACY \u0026ndash; Max retries for legacy jobs \u0026ndash; Nested within tasks as of API 2.1 retry_on_timeout boolean LEGACY \u0026ndash; whether or not to retry if a job run times out \u0026ndash; Nested within tasks as of API 2.1 min_retry_interval_millis long LEGACY \u0026ndash; Minimal interval in milliseconds between the start of the failed run and the subsequent retry run. The default behavior is that unsuccessful runs are immediately retried. \u0026ndash; Nested within tasks as of API 2.1 schedule struct Schedule by which the job should execute and whether or not it is paused existing_cluster_id string LEGACY \u0026ndash; If compute is existing interactive cluster the cluster_id will be here \u0026ndash; Nested within tasks as of API 2.1 new_cluster struct LEGACY \u0026ndash; The cluster_spec identified as an automated cluster for legacy jobs \u0026ndash; Can be found nested within tasks now but ONLY for direct API Calls, editing legacy jobs, AND sparkSumbit tasks (as they cannot use job_clusters), otherwise, new_clusters defined through the UI will be defined as \u0026ldquo;job_clusters\u0026rdquo; and referenced by a \u0026ldquo;job_cluster_key\u0026rdquo; in the tasks field. git_source struct Specification for a remote repository containing the notebooks used by this job\u0026rsquo;s notebook tasks. task_detail_legacy struct LEGACY \u0026ndash; The job execution details used to be defined at the root level for API 2.0 as of API 2.1 they have been nested within tasks. The logic definition will be defined here for legacy jobs only (or new jobs created using the 2.0 jobs API) is_from_dlt boolean Whether or not the job was created from DLT \u0026ndash; Unsupported as OW doesn\u0026rsquo;t yet support DLT but left here as a reference in case it can be helpful aclPermissionSet struct Only populated for \u0026ldquo;ChangeJobAcl\u0026rdquo; actions. Defines the new ACLs for a job target_user_id string Databricks canonical user id to which the aclPermissionSet is to be applied session_id string session_id that requested the action request_id string request_id of the action user_agent string request origin such as browser, terraform, api, etc. response struct response of api call including errorMessage, result, and statusCode (HTTP 200,400, etc) source_ip_address string Origin IP of action requested created_by string Email account that created the job created_ts long Timestamp the job was created deleted_by string Email account that deleted the job \u0026ndash; will be null if job has not been deleted deleted_ts long Timestamp the job was deleted \u0026ndash; will be null if job has not been deleted last_edited_by string Email account that made the previous edit \u0026ndash; defaults to created by if no edits made last_edited_ts long Timestamp the job was last edited JobRun SAMPLE\nIn Databricks, jobs (also called Workflows) can have 1 or more tasks associated with it. Whenever the job is triggered, a job run occurs, which will have the execution of the tasks. In this table, each row represents the metrics of each task executed during the job run. Each record references the full lifecycle of a single task run with some legacy fields to accommodate historical job-level runs (and jobs/runs still being created/launched from the deprecated Jobs 2.0 API). Since the inception of multi-task jobs and Databricks jobs API 2.1, all run logic has been migrated from the job-level to the task-level. Overwatch must accommodate both as many customers have historical data that is still important. As such, some of the fields seem redundant and the analyst must apply the correct logic based on the circumstances. Please carefully review the field descriptions to understand the rules.\nUnsupported Scenarios A few scenarios are not yet supported by Overwatch; they are called out here. Please stay tuned for updates as it\u0026rsquo;s our intention to include everything we can as soon as possible after Databricks product GAs new features but there will be a delay.\nDLT details \u0026ndash; Delta Live tables aren\u0026rsquo;t yet supported even though the run_ids may show up here Runs that failed to launch due to error in the launch request \u0026ndash; these never actually create a run and never receive a run_id therefore they will not be present in this table at this time. Runs executing for more than 30 Days \u0026ndash; This is a limitation for performance. This will be an externalized config at a later time but for now the hard limit is 30 days. The progress of this feature can be tracked in Issue 528 KEY \u0026ndash; organization_id + run_id + startEpochMS\nIncremental Columns \u0026ndash; startEpochMS\nPartition Columns \u0026ndash; organization_id\nWrite Mode \u0026ndash; Merge\nInventory of every canonical task run executed by databricks workspace.\nColumn Type Description organization_id string Canonical workspace id workspace_name string Customer defined name of the workspace or workspace_id (default) job_id long ID of the job (workflow) job_name string Name of the job job_trigger_type string One of \u0026ldquo;cron\u0026rdquo; (automated scheduled), \u0026ldquo;manual\u0026rdquo;, \u0026ldquo;repair\u0026rdquo; terminal_state string State of the task run at the time of Overwatch pipeline execution run_id long The lowest level of the run_id (i.e. legacy jobs may not have a task_run_id, in this case, it will be the same as job_run_id). run_name string The name of the run if the run is named (i.e. in submitRun) otherwise this is set == taskKey multitask_parent_run_id long If the task belongs to a multi-task job the job_run_id will be populated here, otherwise it will be null job_run_id long he run id of the job, not the task task_run_id long The run id of the task repair_id long If the task or job was repaired, the repair id will be present here and the details of the repair will be in repair_details task_key string The name of the task is actually a key and must be unique within a job, this field specifies the task that was executed in this task_run_id cluster_type string Type of cluster used in the execution, one of \u0026ldquo;new\u0026rdquo;, \u0026ldquo;job_cluster\u0026rdquo;, \u0026ldquo;existing\u0026rdquo;, \u0026ldquo;SQL Warehouse\u0026rdquo;, null \u0026ndash; will be null for DLT pipelines and/or in situations where the type is not provided from Databricks cluster_id string The cluster ID of the compute used to execute the task run. If task executed on a SQL Warehouse, the warehouse_id will be populated here. cluster_name string The name of the compute asset used to execute the task run job_cluster_key string When the task compute is a job_cluster the name of the job_cluster will be provided here job_cluster struct When the task compute is a job_cluster, the cluster_definition of the job_cluster used to execute the task new_cluster struct LEGACY + SparkSubmit jobs \u0026ndash; new clusters are no longer used for tasks except for sparkSubmit jobs as they cannot use job_clusters. Job_clusters are used everywhere else tags map Job tags at the time of the run task_detail struct The details of the task logic such as notebook_task, sql_task, spark_python_task, etc. task_dependencies array The list of tasks the task depends on to be successful in order to run task_runtime struct The runtime of the task from launch to termination (including compute spin-up time) task_execution_runtime struct The execution time of the task (excluding compute spin-up time) task_type string Type of task to be executed \u0026ndash; this should mirror the \u0026ldquo;type\u0026rdquo; selected in the \u0026ldquo;type\u0026rdquo; drop down in the job definition. May be null for submitRun as this jobType schedule struct Schedule by which the job should execute and whether or not it is paused libraries array LEGACY \u0026ndash; Libraries defined in the job \u0026ndash; Nested within tasks as of API 2.1 manual_override_params struct When task is executed manually and the default parameters were manually overridden the overridden parameters will be captured here repair_details array Details of the repair run including any references to previous repairs timeout_seconds string Job-level timeout seconds. Databricks supports timeout seconds at both the job level and the task level. Task level timeout_seconds can be found nested within tasks retry_on_timeout boolean LEGACY \u0026ndash; whether or not to retry if a job run times out \u0026ndash; Nested within tasks as of API 2.1 max_retries long LEGACY \u0026ndash; Max retries for legacy jobs \u0026ndash; Nested within tasks as of API 2.1 min_retry_interval_millis long LEGACY \u0026ndash; Minimal interval in milliseconds between the start of the failed run and the subsequent retry run. The default behavior is that unsuccessful runs are immediately retried. \u0026ndash; Nested within tasks as of API 2.1 max_concurrent_runs long Job-level \u0026ndash; maximum concurrent executions of the job run_as_user_name string The user email of the principal configured to execute the job parent_run_id long The upstream run_id of the run that called current run using dbutils.notebook.run \u0026ndash; DO NOT confuse this with multitask_parent_run_id, these are different workflow_context string The workflow context (as a json string) provided when using Notebook Workflows (i.e. dbutils.notebook.run) task_detail_legacy struct LEGACY \u0026ndash; The details of the task logic for legacy jobs such as notebook_task, spark_python_task, etc. These must be separated from the task level details as the structures have been altered in many cases submitRun_details struct When task_type == submitRun, full job and run definition provided in the submitRun API Call. Since no existing job definition is present for a submitRun \u0026ndash; all the details of the run submission are captured here created_by string Email account that created the job last_edited_by string Email account that made the previous edit \u0026ndash; defaults to created by if no edits made request_detail struct All request details of the lifecycle and their results are captured here including submission, cancellation, completions, and execution start time_detail struct All events in the run lifecycle timestamps are captured here in the event deeper timestamp analysis is required JobRunCostPotentialFact SAMPLE\nDatabricks has moved to \u0026ldquo;multi-task jobs\u0026rdquo; and each run now refers to the run of a task not a job. Please reference jobRuns table for more detail\nUnsupported Scenarios Costs for runs of that execute SQL/DBT/DLT tasks KEY \u0026ndash; organization_id + run_id + startEpochMS\nIncremental Columns \u0026ndash; startEpochMS\nPartition Columns \u0026ndash; organization_id\nWrite Mode \u0026ndash; Merge\nThis fact table defines the job, the cluster, the cost, the potential, and utilization (if cluster logging is enabled) of a cluster associated with a specific Databricks Job Run.\nDimensionality Note that this fact table is not normalized by time but rather by job run and cluster state. Costs are not derived from job runs but from clusters thus the state[s] of the cluster are what\u0026rsquo;s pertinent when tying to cost. This is extremely important in the case of long running jobs, such as streaming.\nSCENARIO: Imagine a streaming job with 12 concurrent runs on an existing cluster that run for 20 days at the end of which the driver dies for some reason causing all runs fail and begin retrying but failing. When the 20 days end, the cost will be captured solely on that date and even more importantly, not only will all 20 days be captured at that date but the cost associated will be cluster runtime for 20 days * number of runs. Overwatch will automatically smooth the costs across the concurrent runs but not the days running since this fact table is not based by on an equidistant time axis.\nPotential: Total core_milliseconds for which the cluster COULD execute spark tasks. This derivation only includes the worker nodes in a state ready to receive spark tasks (i.e. Running). Nodes being added or running init scripts are not ready for spark jobs thus those core milliseconds are omitted from the total potential. Cost: Derived from the instanceDetails table and DBU configured contract price (see Configuration for more details). The compute costs in instanceDetails table are taken from the \u0026ldquo;Compute_Contract_Price\u0026rdquo; values associated with the instance type in instanceDetails. Utilization: Utilization is a function of core milliseconds used during spark task execution divided by the total amount of core milliseconds available given the cluster size and state. (i.e. spark_task_runtime_H / worker_potential_core_H) Cluster State: The state[s] of a cluster during a run. As the cluster scales and morphs to accommodate the run\u0026rsquo;s needs, the state changes. The number of state changes are recorded in this table as \u0026ldquo;run_cluster_states\u0026rdquo;. Run State: Advanced Topic for data engineers and developers. This topic is discussed in considerable detail in the Advanced Topics section. Given a cluster state, the run state is a state of all runs on a cluster at a given moment in time. This is the measure used to calculate shared costs across concurrent runs. A run state cannot pass the boundaries of a cluster state, a run that continues across cluster-state lines will result in a new run state. Column Type Description organization_id string Canonical workspace id workspace_name string Customer defined name of the workspace or workspace_id (default) job_id long Canonical ID of job job_name string Name of the runName if run is named, otherwise it will be job name job_trigger_type string One of \u0026ldquo;cron\u0026rdquo; (automated scheduled), \u0026ldquo;manual\u0026rdquo;, \u0026ldquo;repair\u0026rdquo; terminal_state string State of the task run at the time of Overwatch pipeline execution run_id long The lowest level of the run_id (i.e. legacy jobs may not have a task_run_id, in this case, it will be the job_run_id). run_name string The name of the run if the run is named (i.e. in submitRun) otherwise this is set == taskKey multitask_parent_run_id long If the task belongs to a multi-task job the job_run_id will be populated here, otherwise it will be null job_run_id long The run id of the job, not the task task_run_id long The run id of the task except for legacy and repair_id long If the task or job was repaired, the repair id will be present here and the details of the repair will be in repair_details task_key string The name of the task is actually a key and must be unique within a job, this field specifies the task that was executed in this task_run_id task_type string Type of task to be executed \u0026ndash; this should mirror the \u0026ldquo;type\u0026rdquo; selected in the \u0026ldquo;type\u0026rdquo; drop down in the job definition. May be null for submitRun as this jobType task_runtime struct The runtime of the task from start to termination. Databricks does not publish task_launch_time task_execution_runtime struct Until Databricks publishes task_launch_time this will equal task_runtime cluster_type string Type of type cluster used in the execution, one of \u0026ldquo;automated\u0026rdquo;, \u0026ldquo;interactive, null \u0026ndash; will be null for DLT pipelines and/or in situations where the type is not provided from Databricks. In the future you can expect \u0026ldquo;SQL Warehouse\u0026rdquo; and other types of compute to show up here. cluster_id string The cluster ID of the compute used to execute the task run. If task executed on a SQL Warehouse, the warehouse_id will be populated here. cluster_name string The name of the compute asset used to execute the task run cluster_tags map Tags present on the compute that executed the run parent_run_id long The upstream run_id of the run that called current run using dbutils.notebook.run \u0026ndash; DO NOT confuse this with multitask_parent_run_id, these are different running_days array Array (or list) of dates (not strings) across which the job run executed. This simplifies day-level cost attribution, among other metrics, when trying to smooth costs for long-running / streaming jobs avg_cluster_share double Average share of the cluster the run had available assuming fair scheduling. This DOES NOT account for activity outside of jobs (i.e. interactive notebooks running alongside job runs), this measure only splits out the share among concurrent job runs. Measure is only calculated for interactive clusters, automated clusters assume 100% run allocation. For more granular utilization detail, enable cluster logging and utilize \u0026ldquo;job_run_cluster_util\u0026rdquo; column which derives utilization at the spark task level. avg_overlapping_runs double Number of concurrent runs shared by the cluster on average throughout the run max_overlapping_runs long Highest number of concurrent runs on the cluster during the run run_cluster_states long Count of cluster state transitions during the job run driver_node_type_id string Driver Node type for the compute asset (not supported for Warehouses yet) node_type_id string Worker Node type for the compute asset (not supported for Warehouses yet) worker_potential_core_H double cluster core hours capable of executing spark tasks, \u0026ldquo;potential\u0026rdquo; dbu_rate double Effective DBU rate at time of job run used for calculations based on configured contract price in instanceDetails at the time of the Overwatch Pipeline Run driver_compute_cost double Compute costs associated with driver runtime driver_dbu_cost double DBU costs associated with driver runtime worker_compute_cost double Compute costs associated with worker runtime worker_dbu_cost double DBU costs associated with cumulative runtime of all worker nodes total_driver_cost double Driver costs including DBUs and compute total_worker_cost double Worker costs including DBUs and compute total_compute_cost double All compute costs for Driver and Workers total_dbu_cost double All dbu costs for Driver and Workers total_cost double Total cost from Compute and DBUs for all nodes (including Driver) spark_task_runtimeMS long Spark core execution time in milliseconds (i.e. task was operating/locking on core). Cluster logging must be enabled spark_task_runtime_H double Spark core execution time in Hours (i.e. task was operating/locking on core). Cluster logging must be enabled job_run_cluster_util double Cluster utilization: spark task execution time / cluster potential. True measure by core of utilization. Cluster logging must be enabled. created_by string Email account that created the job last_edited_by string Email account that made the previous edit \u0026ndash; defaults to created by if no edits made sqlQueryHistory SAMPLE\nKEY \u0026ndash; organization_id + warehouse_id + query_id + query_start_time_ms\nIncremental Columns \u0026ndash; query_start_time_ms\nPartition Columns \u0026ndash; organization_id\nWrite Mode \u0026ndash; Merge\nColumn Type Description organization_id string Canonical workspace id workspace_name string Customizable human-legible name of the workspace, should be globally unique within the organization warehouse_id string ID of the SQL warehouse. query_id string ID of the query executed in the warehouse query_end_time_ms long Query execution end time user_name string User name who created the query user_id long Id of the user who created the query executed_as_user_id long Id of the user who executed the query executed_as_user_name string User name who executed the query duration long Duration of the query execution error_message string Error message for failed queries execution_end_time_ms long Query execution end time in ms query_start_time_ms long Query start time in ms query_text text Query text which is executed in the warehouse rows_produced long Number of rows returned as query output spark_ui_url string URL of the Spark UI statement_type string Statement type of the query being executed, e.g - Select, Update etc status string Current status of the query being executed, e.g - FINISHED, RUNNING etc compilation_time_ms long Time spent loading metadata and optimizing the query, in milliseconds. execution_time_ms long ime spent executing the query, in milliseconds. network_sent_bytes long Size of data transferred over network in bytes photon_total_time_ms long Total execution time for all individual Photon query engine tasks in the query, in milliseconds. pruned_bytes long Size of data pruned in bytes pruned_files_count long Total number of files pruned read_bytes long Size of data red in bytes read_cache_bytes long Size of data cached during reading in bytes read_files_count long Total number of files in read read_partitions_count long Total number of partitions used while reading read_remote_bytes long Shuffle fetches from remote executor result_fetch_time_ms long Time spent fetching the query results after the execution finished, in milliseconds. result_from_cache long Flag to check whether result is fetched from cache rows_produced_count long Total number of rows produced after fetching the data rows_read_count string Total number of rows in the output after fetcing the data spill_to_disk_bytes long Data spilled to disk in bytes task_total_time_ms long Sum of execution time for all of the querys tasks, in milliseconds. total_time_ms long Total execution time of the query from the clients point of view, in milliseconds. This is equivalent to duration write_remote_bytes long Shuffle writes to the remote executor Notebook SAMPLE\nKEY \u0026ndash; organization_id + notebook_id + request_id + action + unixTimeMS\nIncremental Columns \u0026ndash; unixTimeMS\nPartition Columns \u0026ndash; organization_id\nWrite Mode \u0026ndash; Append\nColumn Type Description notebook_id string Canonical notebook id notebook_name string Name of notebook at time of action requested notebook_path string Path of notebook at time of action requested cluster_id string Canonical workspace cluster id action string action recorded timestamp timestamp timestamp the action took place old_name string When action is \u0026ldquo;renameNotebook\u0026rdquo; this holds notebook name before rename old_path string When action is \u0026ldquo;moveNotebook\u0026rdquo; this holds notebook path before move new_name string When action is \u0026ldquo;renameNotebook\u0026rdquo; this holds notebook name after rename new_path string When action is \u0026ldquo;moveNotebook\u0026rdquo; this holds notebook path after move parent_path string When action is \u0026ldquo;renameNotebook\u0026rdquo; notebook containing, workspace path is recorded here user_email string Email of the user requesting the action request_id string Canonical request_id response struct HTTP response including errorMessage, result, and statusCode InstancePool KEY \u0026ndash; organization_id + instance_pool_id + timestamp\nIncremental Columns \u0026ndash; timestamp\nPartition Columns \u0026ndash; organization_id\nWrite Mode \u0026ndash; Merge\nColumn Type Description instance_pool_id string Canonical notebook id instance_pool_name string Name of notebook at time of action requested actionName string action recorded timestamp long timestamp the action took place node_type_id string Type of node in the pool idle_instance_autotermination_minutes long Minutes after which a node shall be terminated if unused min_idle_instances long Minimum number of hot instances in the pool max_capacity long Maximum number of nodes allowed in the pool preloaded_spark_versions string Spark versions preloaded on nodes in the pool Account Tables Not exposed in the consumer database. These tables contain more sensitive information and by default are not exposed in the consumer database but held back in the ETL database. This is done purposely to simplify security when/if desired. If desired, this can be exposed in consumer database with a simple vew definition exposing the columns desired.\nFor deeper insights regarding audit, please reference auditLogSchema. This is simplified through the use of the ETL_DB.audit_log_bronze and filter where serviceName == accounts for example. Additionally, you may filter down to specific actions using \u0026ldquo;actionName\u0026rdquo;. An example query is provided below:\nspark.table(\u0026#34;overwatch.audit_log_bronze\u0026#34;) .filter(\u0026#39;serviceName === \u0026#34;accounts\u0026#34; \u0026amp;\u0026amp; \u0026#39;actionName === \u0026#34;createGroup\u0026#34;) .selectExpr(\u0026#34;*\u0026#34;, \u0026#34;requestParams.*\u0026#34;).drop(\u0026#34;requestParams\u0026#34;) AccountLogin SAMPLE\nKEY \u0026ndash; organization_id + login_type + login_unixTimeMS + from_ip_address\nIncremental Columns \u0026ndash; login_unixTimeMS\nPartition Columns \u0026ndash; organization_id\nWrite Mode \u0026ndash; Append\nColumn Type Description login_unixTimeMS string Unix Timestamp when the user logged in login_date string Date when user logged in login_type string How did the user log in. One of aadTokenLogin, login, aadBrowserLogin, tokenLogin, samlLogin, jwtLogin, ssh login_user string Canonical user id (within the workspace) user_email string User\u0026rsquo;s email login_type string Type of login such as web, ssh, token from_ip_address struct Details about the source login and target logged into user_agent string request origin such as browser, terraform, api, etc. request_id string GUID of the login request response struct HTTP Response to login attempt, including statusCode, error message, and result (if any) AccountMod SAMPLE\nKEY \u0026ndash; organization_id + acton + mod_unixTimeMS + request_id\nIncremental Columns \u0026ndash; mod_unixTimeMS\nPartition Columns \u0026ndash; organization_id\nWrite Mode \u0026ndash; Append\nThis table tracks any admin changes made to user entities, such as group membership, user updates, etc.\nColumn Type Description mod_unixTimeMS bigint Unix timestamp when the modification happened mod_date date Date when the modification happened action string Action performed, one of: add, addPrincipalToGroup, removePrincipalFromGroup, setAdmin, updateUser, delete endpoint string Mechanism for making the change, one of: scim, adminConsole, autoUserCreation, roleAssignment modified_by string Email of user making the change user_name string Email or username of user profile being changed user_id string Canonical user id (within the workspace) of user profile group_name string In case the modification is to a group, the group name, otherwise this will ne NULL group_id string In case the modification is to a group, the group ID, otherwise this will ne NULL from_ip_address string IP Address where the change originated from user_agent string request origin such as browser, terraform, api, etc. request_id string GUID of the login request response struct HTTP Response to login attempt, including statusCode, error message, and result (if any) The following sections are related to Spark. Everything that can be seend/found in the SparkUI is visibel in the spark tables below. A reasonable understanding of the Spark hierarchy is necessary to make this section simpler. Please reference Spark Hierarchy For More Details for more details.\nSparkExecution SAMPLE\nKEY \u0026ndash; organization_id + spark_context_id + execution_id + date + unixTimeMS\nIncremental Columns \u0026ndash; date + unixTimeMS\nPartition Columns \u0026ndash; organization_id\nWrite Mode \u0026ndash; Merge\nColumn Type Description organization_id string Canonical workspace id spark_context_id string Canonical context ID \u0026ndash; One Spark Context per Cluster cluster_id string Canonical workspace cluster id execution_id long Spark Execution ID description string Description provided by spark details string Execution StackTrace sql_execution_runtime struct Complete runtime detail breakdown SparkExecutor SAMPLE\nKEY \u0026ndash; organization_id + spark_context_id + executor_id + date + unixTimeMS\nIncremental Columns \u0026ndash; date + unixTimeMS\nPartition Columns \u0026ndash; organization_id\nWrite Mode \u0026ndash; Merge\nColumn Type Description organization_id string Canonical workspace id spark_context_id string Canonical context ID \u0026ndash; One Spark Context per Cluster cluster_id string Canonical workspace cluster id executor_id int Executor ID executor_info string Executor Detail removed_reason string Reason executor was removed executor_alivetime struct Complete lifetime detail breakdown SparkJob SAMPLE\nKEY \u0026ndash; organization_id + spark_context_id + job_id + unixTimeMS\nIncremental Columns \u0026ndash; date + unixTimeMS\nPartition Columns \u0026ndash; organization_id + date\nZ-Order Columns \u0026ndash; cluster_id\nWrite Mode \u0026ndash; Merge\nColumn Type Description organization_id string Canonical workspace id spark_context_id string Canonical context ID \u0026ndash; One Spark Context per Cluster cluster_id string Canonical workspace cluster id job_id string Spark Job ID job_group_id string Spark Job Group ID \u0026ndash; NOTE very powerful for many reasons. See SparkEvents execution_id string Spark Execution ID stage_ids array[long] Array of all Spark Stage IDs nested within this Spark Job notebook_id string Canonical Databricks Workspace Notebook ID notebook_path string Databricks Notebook Path user_email string email of user that owned the request, for Databricks jobs this will be the job owner db_job_id string Databricks Job Id executing the Spark Job db_id_in_job string \u0026ldquo;id_in_job\u0026rdquo; such as \u0026ldquo;Run 10\u0026rdquo; without \u0026ldquo;Run \u0026quot; prefix. This is a critical join column when working looking up Databricks Jobs metadata job_runtime string Complete job runtime detail breakdown job_result struct Job Result and Exception if present SparkStage SAMPLE\nKEY \u0026ndash; organization_id + spark_context_id + stage_id + stage_attempt_id + unixTimeMS\nIncremental Columns \u0026ndash; date + unixTimeMS\nPartition Columns \u0026ndash; organization_id + date\nZ-Order Columns \u0026ndash; cluster_id\nWrite Mode \u0026ndash; Merge\nColumn Type Description organization_id string Canonical workspace id spark_context_id string Canonical context ID \u0026ndash; One Spark Context per Cluster cluster_id string Canonical workspace cluster id stage_id string Spark Stage ID stage_attempt_id string Spark Stage Attempt ID stage_runtime string Complete stage runtime detail stage_info string Lineage of all accumulables for the Spark Stage SparkTask SAMPLE\nKEY \u0026ndash; organization_id + spark_context_id + task_id + task_attempt_id + stage_id + stage_attempt_id + host + unixTimeMS\nIncremental Columns \u0026ndash; date + unixTimeMS\nPartition Columns \u0026ndash; organization_id + date\nZ-Order Columns \u0026ndash; cluster_id\nWrite Mode \u0026ndash; Merge\nUSE THE PARTITION COLUMN (date) and Indexed Column (cluster_id) in all joins and filters where possible. This table can get extremely large, select samples or smaller date ranges and reduce joins and columns selected to improve performance.\nColumn Type Description organization_id string Canonical workspace id workspace_name string Customizable human-legible name of the workspace, should be globally unique within the organization spark_context_id string Canonical context ID \u0026ndash; One Spark Context per Cluster cluster_id string Canonical workspace cluster id task_id string Spark Task ID task_attempt_id string Spark Task Attempt ID stage_id string Spark Stage ID stage_attempt_id string Spark Stage Attempt ID executor_id string Spark Executor ID host string Internal IP address of node task_runtime string Complete task runtime detail task_metrics string Lowest level compute metrics provided by spark such as spill bytes, read/write bytes, shuffle info, GC time, Serialization, etc. task_info string Lineage of all accumulables for the Spark Task task_type string Spark task Type (i.e. ResultTask, ShuffleMapTask, etc) task_end_reason string Task end status, state, and details plus stake trace when error SparkStream KEY \u0026ndash; organization_id + spark_context_id + cluster_id + stream_id + stream_run_id + stream_batch_id + stream_timestamp\nIncremental Columns \u0026ndash; date + stream_timestamp\nPartition Columns \u0026ndash; organization_id + date\nZ-Order Columns \u0026ndash; cluster_id\nWrite Mode \u0026ndash; Merge\nColumn Type Description spark_context_id string Canonical context ID \u0026ndash; One Spark Context per Cluster cluster_id string Canonical workspace cluster id stream_id string GUID ID of the spark stream stream_name string Name of stream if named stream_run_id string GUID ID of the spark stream run stream_batch_id long GUID ID of the spark stream run batch stream_timestamp long Unix time (millis) the stream reported its batch complete metrics streamSegment string Type of event from the event listener such as \u0026lsquo;Progressed\u0026rsquo; streaming_metrics dynamic struct All metrics available for the stream batch run execution_ids array Array of execution_ids in the spark_context. Can explode and tie back to sparkExecution and other spark tables Warehouse SAMPLE\nKEY \u0026ndash; organization_id + warehouse_id + unixTimeMS\nIncremental Columns \u0026ndash; unixTimeMS\nPartition Columns \u0026ndash; organization_id\nWrite Mode \u0026ndash; Append\nColumn Type Description organization_id string Canonical workspace id workspace_name string Customizable human-legible name of the workspace, should be globally unique within the organization warehouse_id string Canonical workspace warehouse id warehouse_name string User-defined name of the warehouse service_name string Name of the service corresponding to DBSQL warehouse action_name string create, edit, or snapImpute  depicts the type of action for the warehouse  **snapImpute is used on first run to initialize the state of the cluster even if it wasnt created/edited since audit logs began user_email string Email of the user requesting the action cluster_size string Size of the clusters allocated for this warehouse min_num_clusters long Minimum number of available clusters that will be maintained for this SQL warehouse max_num_clusters long Maximum number of clusters that the autoscaler will create to handle concurrent queries auto_stop_mins long The amount of time in minutes that a SQL warehouse must be idle (i.e., no RUNNING queries) before it is automatically stopped spot_instance_policy string Configurations whether the warehouse should use spot instances enable_photon boolean Configures whether the warehouse should use Photon optimized clusters channel struct This column contains channel details. Some examples - CHANNEL_NAME_UNSPECIFIED, CHANNEL_NAME_PREVIEW, CHANNEL_NAME_CURRENT, CHANNEL_NAME_PREVIOUS, CHANNEL_NAME_CUSTOM enable_serverless_compute boolean Flag indicating whether the warehouse should use serverless compute warehouse_type string Warehouse type: PRO or CLASSIC warehouse_state string State of the warehouse size string Size of the clusters allocated for this warehouse creator_id long warehouse creator id tags map A set of key-value pairs that will be tagged on all resources (e.g., AWS instances and EBS volumes) associated with this SQL warehouse num_clusters long current number of clusters running for the service num_active_sessions long current number of active sessions for the warehouse jdbc_url string the jdbc connection string for this warehouse created_by string warehouse creator name NotebookCommands KEY \u0026ndash; notebook_id + unixTimeMS\nIncremental Columns \u0026ndash; unixTimeMS\nPartition Columns \u0026ndash; organization_id\nWrite Mode \u0026ndash; Merge NotebookCommands are not available for notebooks run on a SQL Warehouse yet. This feature will be added in a future release\nColumn Type Description organization_id string Canonical workspace id workspace_name string Customizable human-legible name of the workspace, should be globally unique within the organization date date unixTimeMS as a date type timestamp long unixTimeMS as a timestamp type in milliseconds notebook_id string id for the notebook in the workspace notebook_path string Notebook path in the workspace notebook_name string Canonical notebook name for the workspace command_id string id of the notebook command command_text string The actual text of the command execution_time_s double Notebook command execution time in second source_ip_address string Origin IP of action requested user_identity struct User information as available. Will include userid and email address estimated_dbu_cost double dbu cost per second for the command runtime status string Status of the notebook command run cluster_id string Canonical workspace cluster id cluster_name string The name of the compute asset used to execute the task run custom_tags string JSON string of key/value pairs for all cluster associated custom tags give to the cluster node_type_id string Worker Node type for the compute asset (not supported for Warehouses yet) node_count long Cluster worker node count response struct HTTP response including errorMessage, result, and statusCode user_agent string Request origin such as browser, terraform, api, etc unixTimeMS long Unix time epoch as a long in milliseconds Pipeline_SnapTS string Snapshot timestamp of Overwatch run that added the record Overwatch_RunID string Overwatch canonical ID that resulted in the record load Common Meta Fields Column Type Description organization_id string Workspace / Organization ID on which the cluster was instantiated cluster_id string Canonical workspace cluster id unixTimeMS long unix time epoch as a long in milliseconds timestamp string unixTimeMS as a timestamp type in milliseconds date string unixTimeMS as a date type created_by string last_edited_by string last user to edit the state of the entity last_edited_ts string timestamp at which the entitiy\u0026rsquo;s sated was last edited deleted_by string user that deleted the entity deleted_ts string timestamp at which the entity was deleted event_log_start string Spark Event Log BEGIN file name / path event_log_end string Spark Event Log END file name / path Pipeline_SnapTS string Snapshot timestmap of Overwatch run that added the record Overwatch_RunID string Overwatch canonical ID that resulted in the record load ETL Tables The following are the list of potential tables, the module with which it\u0026rsquo;s created and the layer in which it lives. This list consists of only the ETL tables created to facilitate and deliver the consumption layer The gold and consumption layers are the only layers that maintain column name uniformity and naming convention across all tables. Users should always reference Consumption and Gold layers unless the data necessary has not been curated.\nBronze Table Scope Layer Description audit_log_bronze audit bronze Raw audit log data full schema audit_log_raw_events audit bronze (azure) Intermediate staging table responsible for coordinating intermediate events from azure Event Hub cluster_events_bronze clusterEvents bronze Raw landing of dataframe derived from JSON response from cluster events api call. Note: cluster events expire after 30 days of last termination. (reference) clusters_snapshot_bronze clusters bronze API snapshot of existing clusters defined in Databricks workspace at the time of the Overwatch run. Snapshot is taken on each run jobs_snapshot_bronze jobs bronze API snapshot of existing jobs defined in Databricks workspace at the time of the Overwatch run. Snapshot is taken on each run pools_snapshot_bronze pools bronze API snapshot of existing pools defined in Databricks workspace at the time of the Overwatch run. Snapshot is taken on each run spark_events_bronze sparkEvents bronze Raw landing of the master sparkEvents schema and data for all cluster logs. Cluster log locations are defined by cluster specs and all locations will be scanned for new files not yet captured by Overwatch. Overwatch uses an implicit schema generation here, as such, a lack of real-world can cause unforeseen issues. spark_events_processedfiles sparkEvents bronze Table that keeps track of all previously processed cluster log files (spark event logs) to minimize future file scanning and improve performance. This table can be used to reprocess and/or find specific eventLog files. warehouses_snapshot_bronze DBSQL bronze API snapshot of existing warehouse defined in Databricks workspace at the time of the Overwatch run. Snapshot is taken on each run pipeline_report NA tracking Tracking table used to identify state and status of each Overwatch Pipeline run. This table is also used to control the start and end points of each run. Altering the timestamps and status of this table will change the ETL start/end points. Silver Table Scope Layer Description account_login_silver accounts silver Login events account_mods_silver accounts silver Account modification events cluster_spec_silver clusters silver Slow changing dimension used to track all clusters through time including edits but excluding state change. cluster_state_detail_silver clusterEvents silver State detail for each cluster event enriched with cost information job_status_silver jobs silver Slow changing dimension used to track all jobs specifications through time jobrun_silver jobs silver Historical run of every job since Overwatch began capturing the audit_log_data notebook_silver notebooks silver Slow changing dimension used to track all notebook changes as it morphs through time along with which user instigated the change. This does not include specific change details of the commands within a notebook just metadata changes regarding the notebook. pools_silver pools silver Slow changing dimension used to track all changes to instance pools spark_executions_silver sparkEvents silver All spark event data relevant to spark executions spark_executors_silver sparkEvents silver All spark event data relevant to spark executors spark_jobs_silver sparkEvents silver All spark event data relevant to spark jobs spark_stages_silver sparkEvents silver All spark event data relevant to spark stages spark_tasks_silver sparkEvents silver All spark event data relevant to spark tasks sql_query_history_silver DBSQL silver History of all the sql queries executed through SQL warehouses warehouse_spec_silver DBSQL silver State detail for each warehouse event Gold Table Scope Layer Description account_login_gold accounts gold Login events account_mods_gold accounts gold Account modification events cluster_gold clusters gold Slow-changing dimension with all cluster creates and edits through time. These events DO NOT INCLUDE automated cluster resize events or cluster state changes. Automated cluster resize and cluster state changes will be in clusterstatefact_gold. If user changes min/max nodes or node count (non-autoscaling) the event will be registered here AND clusterstatefact_gold. clusterStateFact_gold clusterEvents gold All cluster event changes along with the time spent in each state and the core hours in each state. This table should be used to find cluster anomalies and/or calculate compute/DBU costs of some given scope. job_gold jobs gold Slow-changing dimension of all changes to a job definition through time jobrun_gold jobs gold Dimensional data for each job run in the databricks workspace notebook_gold notebooks gold Slow changing dimension used to track all notebook changes as it morphs through time along with which user instigated the change. This does not include specific change details of the commands within a notebook just metadata changes regarding the notebook. instancepool_gold pools gold Slow changing dimension used to track all changes to instance pools sparkexecution_gold sparkEvents gold All spark event data relevant to spark executions sparkexecutor_gold sparkEvents gold All spark event data relevant to spark executors sparkjob_gold sparkEvents gold All spark event data relevant to spark jobs sparkstage_gold sparkEvents gold All spark event data relevant to spark stages sparktask_gold sparkEvents gold All spark event data relevant to spark tasks sparkstream_gold sparkEvents gold All spark event data relevant to spark streams sql_query_history_gold DBSQL gold History of all the sql queries executed through SQL warehouses warehouse_gold DBSQL gold Slow-changing dimension with all warehouse creates and edits through time. notebookCommands_gold audit,notebooks,clusterEvents gold Information related to Notebook Commands "
},
{
	"uri": "http://localhost:1313/overwatch/deployoverwatch/ucedeployment/cloudstorageaccessrequirements/azure/",
	"title": "Azure",
	"tags": [],
	"description": "",
	"content": "Creating the Managed Identity Create a Managed Identity to authorize access to the external location. This managed Identity will be configured using a Databricks Storage Credential. Databricks recommends using an Access Connector for Azure Databricks.\nAfter the managed identity is created, it needs to be provisioned read/write access to the storage target for the Overwatch Output (which will ultimately become your external location).\nProvisioning the Managed Identity to The Storage If you intend to provision the managed identity to the storage account you need to grant the managed identity\nStorage Blob Data Contributor If you intend to provision the managed identity to a specific container you need to grant the managed identity\nStorage Blob Data Contributor Storage Blob Delegator "
},
{
	"uri": "http://localhost:1313/overwatch/deployoverwatch/ucedeployment/uceprereqs/",
	"title": "UC Pre-Requisites",
	"tags": [],
	"description": "",
	"content": "Unity Catalog Prerequisites After all UC Pre-requisites are completed, please continue to Deploy Overwatch section.\nThis section will walk you through the steps necessary as a prerequisite to deploy Overwatch on Unity Catalog.\nWorkspace should be UC enabled. Overwatch Pipeline Cluster must be UC enabled (single user and runtime version \u0026gt; 11.3+). UC Storage Requirements Create Storage Credentials to be used by the external locations provisioned with appropriate read/write access to the UC External Location (AWS | GCP | AZURE) with privileges: READ FILES WRITE FILES CREATE EXTERNAL TABLE Create UC External location where Overwatch data is to be stored (AWS | GCP | AZURE). Provide ALL PRIVILEGES permission to the principal (user/SP) that is going to run the Overwatch Pipeline. Ensure the external location path is pointing to the same storage location to which the storage credential\u0026rsquo;s identity was authorized Create a Catalog or identify an existing catalog where overwatch data will be stored. Overwatch code WILL NOT create the catalog, it must be pre-existing. Principal (user/SP) executing the Overwatch Pipeline must have access to the catalog with privileges: USE CATALOG USE SCHEMA SELECT Create ETL and Consumer Schemas (i.e. databases). Overwatch WILL NOT create the Schemas in a UC Deployment. Principal (user/SP) executing the Overwatch Pipeline must have the following privileges on the Schema AND must be an Owner of the Schema. IS OWNER \u0026ndash; required since schema metadata is edited and requires schema ownership The schema owner can be a user, service principal, or group. It\u0026rsquo;s recommended that you provision an Overwatch_Maintainers group and place the Overwatch Admin users in this group along with any service principals that will be writing data to the output and assign this group as owner of the Overwatch schemas. USE SCHEMA CREATE TABLE MODIFY SELECT Overwatch latest version(0.7.2.0+) should be deployed in the workspace 0.7.1.1+ is ok so long as the migration process is completed before executing with Unity Catalog configurations. Other overwatch prerequisites can be found here SQL Command to Grant Permissions to various UC Objects The following can be done through the UI or via commands as shown below\nSQL Command to grant permissions on Storage Credentials GRANT READ FILES, WRITE FILES, CREATE EXTERNAL TABLE ON STORAGE CREDENTIAL `\u0026lt;storage-credential-name\u0026gt;` TO `\u0026lt;principal\u0026gt;`; SQL Command to grant permissions on External Locations GRANT ALL PRIVILEGES ON EXTERNAL LOCATION `\u0026lt;external-location-name\u0026gt;` TO `\u0026lt;principal\u0026gt;`; SQL Command to grant permissions on Catalog GRANT USE CATALOG, USE SCHEMA, SELECT ON CATALOG \u0026lt;catalog-name\u0026gt; TO `\u0026lt;principal\u0026gt;` SQL Command to grant permissions on ETL Database GRANT USE SCHEMA, CREATE TABLE, MODIFY ON SCHEMA \u0026lt;catalog-name\u0026gt;.\u0026lt;etl-database\u0026gt; TO `\u0026lt;principal\u0026gt;`; SQL Command to grant permissions on Consumer Database GRANT USE SCHEMA, CREATE TABLE, MODIFY ON SCHEMA \u0026lt;catalog-name\u0026gt;.\u0026lt;consumer-database\u0026gt; TO `\u0026lt;principal\u0026gt;`; "
},
{
	"uri": "http://localhost:1313/overwatch/deployoverwatch/configureoverwatch/customcosts/",
	"title": "Custom Costs",
	"tags": [],
	"description": "",
	"content": "Fine-Tuning Your Costs Every customer has their own contracts and this means that the costs associated with cloud compute and DBUs may differ between customers. To ensure the costs in Overwatch are as accurate as possible it\u0026rsquo;s important that these costs are configured as accurately as possible.\nConfiguring Custom Costs There are three essential components to the cost function:\nThe node type (instanceDetails.Api_Name) and its associated contract price (instanceDetails.Compute_Contract_Price) by Workspace The node type (instanceDetails.Api_Name) and its associated DBUs per hour (instanceDetails.Hourly_DBUs). These should be accurate from the default load but Databricks may adjust their DBUs/Hour by node type. This is especially true when a node goes from beta to GA. The DBU contract prices for the SKU under which your DBUs are charged such as: Interactive Automated DatabricksSQL Databricks currently has 3 SKUs (classic/pro/serverless) but Overwatch is not able to accurately report on DBSQL pricing at this time due to data not available in the customer-facing Databricks Product. When this data becomes available, Overwatch will integrate it and enable DBSQL cost tracking. In the meantime Overwatch will does it\u0026rsquo;s best to estimate DBSQL pricing so for this SKU just put your average $DBU cost or a close estimate to your sku price here. JobsLight The DBU contract costs are captured from the Overwatch Configuration maintained as a slow-changing-dimension in the dbuCostDetails table. The compute costs and dbu to node associations are maintained as a slow-changing-dimension in the instanceDetails table.\nIMPORTANT These tables are automatically created in the dataTarget upon first initialization of the pipeline. DO NOT try to manually create the target database outside of Overwatch as that will lead to database validation errors. INSTANCE DETAILS IS NOT MAINTAINED BY THE OVERWATCH PIPELINE After the first run, this table will not be edited by the Overwatch Pipeline again. Customers often update the compute_contract_price to reflect their contract prices with the cloud vendor. Updating this table would override these customizations. If you are using instance types that are not present in instanceDetails table you will see $0 costs. Be sure to periodically add new node types. To customize costs configure and complete your first Bronze run. Before running Silver/Gold, alter the instanceDetails table to your satisfaction. Note that this is only true for instanceDetails not dbuCostDetails as Overwatch will maintain this table for you.\nNote that each subsequent workspace referencing the same dataTarget will append default compute prices to instanceDetails table if no data is present for that organization_id (i.e. workspace). If you would like to customize compute costs for all workspaces,\nexport the instanceDetails dataset to external editor (after first init), add the required metrics referenced above for each workspace, and update the target table with the customized cost information. Note that the instanceDetails object in the consumer database is just a view so you must edit/overwrite the underlying table in the ETL database. The view will automatically be recreated upon first pipeline run.\nIMPORTANT These cost lookup tables are slow-changing-dimensions and thus they have specific rule requirements; familiarize yourself with the details at the links below. If the rules fail, the Gold Pipeline will fail with specific costing errors to help you resolve it.\nInstanceDetails Table Details dbuCostDetails Table Details Helpful Tool (AZURE_Only) to get pricing by region by node.\nSample compute details available below. These are only meant for reference, they do not have all required fields. Follow the instruction above for how to implement custom costs. AWS Example | Azure Example Compute Costs Are Estimated Overwatch is not able to determine the VM SKU provided by your cloud provider (i.e. OnDemand/Spot/Reserved); thus the exact price of compute asset at the time of provisioning isn\u0026rsquo;t available. The compute_contract_price configured in instanceDetails is best configured as the average price you received over some previous period to allow Overwatch to best estimate the compute costs; nonetheless, the compute costs are just that, estimates.\n"
},
{
	"uri": "http://localhost:1313/overwatch/deployoverwatch/runningoverwatch/notebook/",
	"title": "As A Notebook",
	"tags": [],
	"description": "",
	"content": "Deploying Overwatch As A Notebook Notebooks can either be run manually or scheduled to run as a job. While the notebook can be scheduled as a job, it\u0026rsquo;s strongly recommended that Overwatch be run as a JAR instead of a notebook. Notebook execution is great for rapid testing and validation.\nThis deployment method requires Overwatch Version 0.7.1.0+\nJumpstart Notebook Below is an example deployment. When you\u0026rsquo;re ready to get started simply download the rapid start linked notebook below.\nOverwatch Runner ( HTML | DBC ) Deployment Example import com.databricks.labs.overwatch.MultiWorkspaceDeployment val configTable = \u0026#34;overwatch.overwatch_config\u0026#34; // Path to the config table // temp location which will be used as a temp storage. It will be automatically cleaned after each run. val tempLocation = \u0026#34;/tmp/overwatch/templocation\u0026#34; // number of workspaces to process in parallel. Exceeding 20 may require larger drivers or additional cluster config considerations // If total workspaces \u0026lt;= 20 recommend setting parallelism == to workspace count val parallelism = 4 // Run validation MultiWorkspaceDeployment(configTable, tempLocation) .deploy(parallelism) // To run only specific pipelines (i.e. Bronze / Silver Gold) as second argument can be passed to deploy as a // comma-delimited list of pipelines to run // ex: MultiWorkspaceDeployment(configCsvPath, tempLocation).deploy(parallelism,\u0026#34;Bronze\u0026#34;) // MultiWorkspaceDeployment(configCsvPath, tempLocation).deploy(parallelism,\u0026#34;Silver\u0026#34;) // MultiWorkspaceDeployment(configCsvPath, tempLocation).deploy(parallelism,\u0026#34;Gold\u0026#34;) // MultiWorkspaceDeployment(configCsvPath, tempLocation).deploy(parallelism,\u0026#34;Bronze,Silver\u0026#34;) // MultiWorkspaceDeployment(configCsvPath, tempLocation).deploy(parallelism,\u0026#34;Silver,Gold\u0026#34;) Review The Deployment Status Once the deployment is completed, all the deployment update details will be stored in a deployment report. It will be saved to \u0026lt;storage_prefix\u0026gt;/report/deploymentReport as a delta table. Run the below query to check the deployment report.\nNOTE The deployment report maintains a full history of deployments. If you\u0026rsquo;ve deployed multiple times be sure to look at the snapTS and only review the validations relevant for the latest deployment.\nselect * from delta.`\u0026lt;storage_prefix\u0026gt;/report/deploymentReport` order by snapTS desc display( spark.read.format(\u0026#34;delta\u0026#34;).load(\u0026#34;\u0026lt;storage_prefix\u0026gt;/report/deploymentReport\u0026#34;) .orderBy(\u0026#39;snapTS.desc) ) "
},
{
	"uri": "http://localhost:1313/overwatch/dataengineer/modules/",
	"title": "Modules / Scopes",
	"tags": [],
	"description": "",
	"content": "Modules A module is a single workload that builds a target table. More details about all the modules are available in Pipeline Management.\nScopes Scopes are the method by which Overwatch is segmented and a scope will contain all the related modules to build the output from Bronze through to Gold. For example there\u0026rsquo;s one scope called \u0026ldquo;jobs\u0026rdquo; but it contains all the modules for jobs and job runs from bronze through gold as well as the jobruncostpotentialfact gold fact table.\nCertain scopes depend on others; those dependencies are noted below and in Module Dependencies section of Pipeline Management.\nScopes are:\naudit clusters clusterEvents pools jobs accounts notebooks DBSQL sparkEvents notebookCommands The default is to use all scopes so if none are specified in the configuration, all scopes will be enabled. Currently, under normal, daily operations, there no significant cost to any of these modules. It\u0026rsquo;s likely best to leave them all turned on unless there\u0026rsquo;s a specific reason not to. The very first run can be an exception to \u0026ldquo;no significant cost\u0026rdquo;. The historical load of 60+ days can take some time to load depending on the size of the workspace and quantity of historical data to be loaded. See Advanced Topics for more details on optimizing the first run.\nThese modules control which parts of Overwatch are run when the Overwatch job executes. Many of the modules are dependent on other modules (details below). At present, the audit module is always required as it contains most of the metadata to enable the other overwatch modules.\nA full list of which tables are ultimately made available by which module can be found in the Data Definitions section.\nUpcoming modules include (in no particular order):\nsecurity costs data-profiler recommendations real-time Hard dates have not been set for these but will be delivered as soon as available. Please star the GitHub repo for release updates.\nAudit Requires: None\nEnables: All\nAudit is the base, fundamental module from which the other modules build upon. This module is required.\nFor details on how to configure audit logging please refer to the Databricks docs online and/or the environment setup details for your cloud provider.\nThe source data is landed in the bronze layer table audit_log_bronze. The schema is ultimately inferred but a minimum base, required schema is defined to ensure requisite data fields are present. Additional data will land in the audit table as events occur in the workspace and the schema will evolve as necessary to accommodate the new events.\nEvents for entities such as clusters and jobs are only recorded when a user creates, edits, deletes the entity. As such, if the audit logging begins on Day 10, but the cluster and/or job were created and last edited before Day 10 it\u0026rsquo;s likely that much or all of the metadata will be missing for the particular job. This is the reason the entity snapshots are so important. The entity snapshots significantly close this gap for existing workspace entities but there may still be missing data in the early days of Overwatch runs. It\u0026rsquo;s important to ensure Overwatch runs periodically to reduce/eliminate data gaps.\nClusters Requires: Audit\nEnables: All\nGold Entities: Cluster\nThe clusters module is pretty low-level. The cluster_id field throughout the layers is the primary method through which users can tie spark-side data to databricks-side data. Additionally, cluster_ids are required to calculated costs for clusters and for jobs since they are the basis for the compute. Without this metadata, the value of Overwatch is signifcantly stifled.\nIn addition to cluster_id, the clusters module also provides the cluster logging path of all clusters identified by Overwatch. From the logging path the SparkEvents (event log) locations can be extrapolated and consumed; without the clusters module, the sparkEvents cannot function.\nClusterEvents Requires: Clusters\nEnables: ClusterStateFact|JobRunCostPotentialFact\nGold Entities: ClusterStateFact\nCluster Events are just what they sound like, events that occur on a cluster which usually result in a cluster state change. All of these events can be seen in the UI if you navigate to a spcific cluster and click on eEvent Log. This data acquisition is driven from a paginated API calls and can only go back 30 days. Note that a large, autoscaling cluster with many users is likely to have MANY EVENTS. The api can return 500 results (max) per call and the process for getting all the results are via pagination. This is particularly relevant on the first run. The API can only go so fast so the first run will take quite a while for larger workspaces with a lot of cluster events, plan your initial load cluster size appropriately.\nA lot of work has been done to optimize this speed and across nodes in your cluster but remember that there workspace API limits and Overwatch will not exceed them but can put significant pressure on them so it\u0026rsquo;s best to not do a full scale first-run of clusterEvents during times when other resources are heavily utilizing api calls.\nPools Requires: Audit\nGold Entities: InstancePool\nSimple module that offers observability to configuration changes (not state) of an instance pool. Databricks does not yet publish the state change data for instance pools; thus Overwatch cannot deliver metrics for how long a node was used, how long it was idle, when it became idle, when it was terminated, etc. When Databricks makes this data available, Overwatch will add this to the data model in a release shortly thereafter.\nJobs Requires: Audit|Clusters|ClusterEvents Enables: JobRunCostPotentialFact\nGold Entities: Job|JobRun|JobRunCostPotentialFact\nOverwatch curates all the job definitions through time, capturing as much metadata about these job definitions as possible. In addition to the job definition metadata, Overwatch also captures each run and ties the run to a cluster_id and a run_id. There are two types of *run_id\u0026quot; in Databricks, a global, canonical run_id and an id_in_job which starts at 1 for each job and increments by 1 for each of that job\u0026rsquo;s runs. Remember that the job definition can change between runs including important aspects such as the cluster definition, name, schedule, notebook, jar, etc. A job_id can be completely different from day to day and combining the point-in-time definition of a job along with the point-in-time cluster definition and run-times can be very powerful, and very hard to curate without Overwatch. Accounts Requires: Audit\nGold Entities: AccountModificationFact|AccountLoginFact\nThe accounts module is very simple and is meant to assist the IT Admin team in keeping track of users. The Accounts module includes some very useful auditing information as well as enables authorized users to tie user_ids back to user_emails. The Accounts module includes two key entities:\nuserLoginFact - identifies a user login, method, sourceIPAddress, and any authorizing groups. user - Slow changing dimension of a user entity through time including create, edit, delete. The user table also records the source IP address of any add/edit/delete action performed on a user. These tables (which are actually views) have been purposely maintained in the ETL database not the consumer database. This is because these are considerably sensitive tables and it\u0026rsquo;s best to simplify the security around these tables. If desired, views, can be created manually in the consumer database to publish these more broadly.\nOverwatch should not be used as single source of truth for any audit requirements.\nNotebooks Requires: Audit\nGold Entities: Notebook\nCurrently a very simple module that just enables the materialization of notebooks as slow changing dimensions.\nDBSQL Requires: Audit\nGold Entities: sqlQueryHistory|Warehouse\nThis module will contain all assets of DBSQL. Additional entities will be coming shortly to enable the estimation of costs in DBSQL\nSparkEvents Requires: Clusters\nGold Entities: SparkExecution|SparkJob|SparkStage|SparkTask|SparkExecutor|SparkStream|SparkJDBC\nDatabricks captures all \u0026ldquo;SparkEvents\u0026rdquo; captured by Open Source Spark (OSS) AND some additional events that are proprietary to Databricks. These events are captured and persisted in event log files when cluster logging is enabled. Below are screenshots of the cluster logs configuration and the resulting event log parent directories.\nWhen the SparkEvents module is enabled and a cluster is configured to capture cluster logs, Overwatch will dive into the cluster logging folder and find the event logs that were created since the previous run. This is why the clusters module is required for the SparkEvents module to function. Overwatch will remember which files it\u0026rsquo;s processed and only process new files captured within the relevant timeframe.\nIt\u0026rsquo;s best practice to configure cluster logs to be delivered to a location with time-to-live (TTL) properties set up (sometimes referred to as lifecycle properties). This allows for files to age out and not continue to eat up space. Files can be configured to be deleted or moved to cold storage after some defined timeframe. The raw files are very inefficient to store long-term and once the data has been captured by Overwatch, it\u0026rsquo;s stored in an efficient way; therefore, there\u0026rsquo;s little to no reason to keep the files in warm/hot storage for greater than some a few weeks. More information can be found on this topic in the Advanced Topics section.\nThe eventlog schema is very complex and implicit. Each event is categorized by \u0026ldquo;Event\u0026rdquo; and the columns populated are contingent upon the event. This method causes chaos when trying to analyze the logs manually and yet another reason Overwatch was created. The schema ultimately contains every single piece of information that can be found in the Spark UI and it can be accessed and analyzed programmatically. The most common data points have been curated and organized for consumption. The Overwatch team has grand plans to dig deeper and curate additional sources as they are created by Spark and Databricks.\nThe cluster_id is the primary method through which Spark data can be tied to Databricks metadata such as jobs, costs, users, core count, pools, workspaces, etc. Once the Spark operation data is combined with a cluster it\u0026rsquo;s relatively easy to tie it back to any dimension.\nCustom SparkEvents published through Spark Streaming and custom eventListeners can also be identified here for more advanced users. The custom events will not be curated for you but they can be found in the spark_events_bronze etl table\nSparkEvents Organization Tying the SparkEvents together and using only the data necessary is the key to success when working in these large datasets. It\u0026rsquo;s best to identify the highest level that has the measure you need and work your way up to the ClusterID and over to Databricks Environment metadata. The image below attempts to depict the relationships visually to simplify the interactions. A few bullet points have been provided below to help you digest the visual. Additionally, if you\u0026rsquo;re familiar with the SparkUI, the hierarchy and encapsulations are identical.\nAdditional clarification of the Spark Hierachy can be found on this Spark+AI Summit YT Video\nA SparkExecutionID is only present when the spark job[s] are spawned from a SparkSQL command. RDD and Delta metadata commands are not encapsulated by an ExecutionID; as such, JobGroupIDs and JobIDs can exist without the presence of an ExecutionID. A JobGroupID is present anytime a single spark DAG requires multiple Spark Jobs to complete the action. A JobGroupID has 0:m ExecutionIDs and 1:m JobIDs. Another very important note about the JobGroupID is that if the jobGroupID was launched from a Databricks Job, the Databricks JobID and Id_in_Job (i.e. Run 43) are captured as a portion of the jobGroupID. This means that any Spark Job can be directly tied back to a specific Databricks Job Run. A SparkJobID IS required for every spark job and has 1:m Spark StageIDs. The SparkJobID has a 0:1 to a Spark JobGroupID and a 1:m to StageID. SparkJobIDs are also special because they provide the Databricks metadata such as user and cluster_id. Spark_events_bronze also has many additional Databricks properties published in certain circumstances. A SparkStageID is required for all spark jobs and has a 1:1 relationship to JobID and a 1:m to TaskID. A SparkTaskID is the lowest level of a spark job, is the unit of work that is executed on a specific machine, and contains all the execution compute statistics for the tasks just as in the SparkUI. Every task much execute on one and only one SparkExecutorID. a TaskID has a 1:1 relationship to ExecutorID event though a task can be retried because a unique Task is also identified by the AttemptID; thus, when a task fails on due to node failure, the task + attempt is marked as failed and a new attempt is executed on another node. The ExecutorID is the node on which the work was executed. NotebookCommands Requires: ClusterEvents|Notebooks|notebookCommands\nGold Entities: NotebookCommands\nThis module enables the materialization of notebooks commands with estimated cost dimension related to the notebook commands. It requires verbose audit logging to be enabled in the workspace. NotebookCommands are not available for notebooks run on a SQL Warehouse yet. This feature will be added in a future release\n"
},
{
	"uri": "http://localhost:1313/overwatch/deployoverwatch/configureoverwatch/",
	"title": "Configure Overwatch",
	"tags": [],
	"description": "",
	"content": "This section provides details for:\nOverwatch Configurations Managing Custom Costs Security Considerations Validating the Configuration "
},
{
	"uri": "http://localhost:1313/overwatch/deployoverwatch/",
	"title": "Deploy Overwatch",
	"tags": [],
	"description": "",
	"content": "Overwatch is a pipeline that executes to aggregate and normalize all of the logs from all the supported sources and make them easy to interrogate for insights. The steps to deploying Overwatch are pretty simple but there are some specific details that may pertain to your deployment. Below are the top level steps that this section will walk you through the process.\nWhen wanting to monitor your Workspaces, you might want to:\nDeploy Overwatch for the first time Add a new workspace to an existing Overwatch deployment First Overwatch deployment Steps 1. Configure Cloud Infrastructure\nSelect your cloud to find the details for cloud configuration here. (AWS | Azure | GCP)\n2. Build / Apply Security\nReference Security Considerations page to help you build and apply a security model commensurate with your needs.\n3. Configure Overwatch inputs\nReference the configuration page to clarify the configuration details and help you get started.\n4. Run and Schedule the Job[s]\nDecide whether you\u0026rsquo;d like to execute Overwatch as a JAR or a NOTEBOOK and schedule a job to periodically execute the job. It\u0026rsquo;s recommended to run Overwatch as a JAR as it unifies the deployment and doesn\u0026rsquo;t depend on another asset (the notebook) to secure and ensure no one edits, moves, or deletes it.\n5. Productionize Your Deployment\nNow that you\u0026rsquo;re live it\u0026rsquo;s important to optimize everything and ensure the data is safe in case something unexpected happens \u0026ndash; see Productionizing\n6. Share/Utilize Overwatch\nNow you\u0026rsquo;re ready to onboard consumers across your workspaces. Details about how to do that can be found in the Sharing Overwatch page.\nAdd workspace to existing Overwatch deployment When adding a new workspace, all you have to do is add a new row in the configuration table for Overwatch containing the new workspace\u0026rsquo;s information. Please go through the previous steps for cloud infrastructure and security considerations to keep in mind to ensure the driver workspace can monitor the new workspace\nOther information pertaining to deployments Multi-Workspace Monitoring - Considerations As of 0.7.1.0 Overwatch is able to monitor remote workspaces but having one and only one global deployment often doesn\u0026rsquo;t meet customer needs and there are some requirements for a deployment to monitor remote workspaces. More details are available on the Security Considerations and Validations pages. The most important things to consider are:\nAccess to remote workspaces via API Token \u0026ndash; a token (PAT) authorizing access to the remote workspace is required to be provisioned. The Overwatch cluster must have direct read access (not via a mount point) to all cluster logging cloud storage If you have more than 50 mountpoints, please follow the docs here If some of your workspaces do not meet these requirements; that\u0026rsquo;s ok, a local deployment with a simpler config will need to be created for those outliers. It\u0026rsquo;s completely fine to have a primary deployment that manages 20 workspaces and a few workspaces that have to have their own Overwatch Job. The data from each deployment can be dropped into the same or multiple targets, this is all part of the configuration.\nOverwatch Landscapes An Example Scenario Assume you have 23 workspaces, 20 of which meet the criteria discussed above and 3 that need to be locally configured. All 23 workspaces output their data into the same database (or multiple) but this would require that Overwatch run on 4 workspaces, 1 to manage the 20 and 1 on each of the 3 with special requirements.\nProduction Production can hold the data for all 23 workspaces even if some of the workspaces are deemed \u0026ldquo;non-prod\u0026rdquo;. This is ideal to allow an analyst to identify efficiency gains and metrics globally from a single location.\nNon-Production A non-prod Overwatch deployment is also recommended so that when new versions come out and/or schema upgrades come out your team can review and test the upgrade and update any dependent dashboards before you upgrade all 23 workspaces.\nA non-prod Overwatch deployment typically consists of a subset of the 23 workspaces (usually 3-5 workspaces). This deployment will always be upgraded and validated before the global production deployment. This pattern enables Change Management best practices. Non-Prod and Prod can even be deployed on the same workspace just with two different configurations such that the non-prod has its own database names and storage locations.\n"
},
{
	"uri": "http://localhost:1313/overwatch/deployoverwatch/ucedeployment/cloudstorageaccessrequirements/gcp/",
	"title": "GCP",
	"tags": [],
	"description": "",
	"content": "Below are the requirement needed for Storage Access setup in GCP\nGoogle Service Account for Storage Credentials\nGoogle Service Account for Overwatch Job Cluster\nCluster Logging Locations Setup\nGoogle Service Account for Storage Credentials Unity Catalog Storage credentials should have the ability to read and write to a External Location(GCS bucket) by assigning appropriate IAM roles on that bucket to a Databricks-generated Google Cloud service account. Please refer the docs for detailed steps.\nGoogle Service Account for Overwatch Job Cluster After the Storage Credentials are created, the existing Service Account attached to the Overwatch Job cluster needs to be provisioned read/write access to the storage target for the Overwatch Output (which will ultimately become your external location). This Service Account can be added to Overwatch Job/Interactive Clusters.\nThe following steps needs to be performed in external locations GCS bucket permissions -\nGo to the Permissions tab of the external locations GCS bucket Click on Grant Access Add Service Account which is attached to the Overwatch Job cluster in the Add Principal section Add Service Account which is attached to the Overwatch Job cluster Add Roles - Storage Admin Storage Legacy Bucket Owner Cluster Logging Locations Setup When you create a workspace, Databricks on Google Cloud creates two Google Cloud Storage GCS buckets in your GCP project:\ndatabricks-\u0026lt;workspace-id\u0026gt; - stores system data that is generated as you use various Databricks features. This bucket includes notebook revisions, job run details, command results, and Spark logs databricks-\u0026lt;workspace-id\u0026gt;-system - contains workspaces root storage for the Databricks File System (DBFS). Your DBFS root bucket is not intended for storage of production data. Follow the databricks-docs to get more information on these buckets.\nIn order to fetch the cluster logs of the remote workspace, cluster should have access to the GCS bucket - databricks-\u0026lt;workspace-id\u0026gt;. This GCS Bucket is created in the Google Cloud project that hosts your Databricks workspace.\nThe following steps needs to be performed in databricks-\u0026lt;workspace-id\u0026gt; GCS bucket permissions -\nGo to the Permissions tab of the databricks-\u0026lt;workspace-id\u0026gt; GCS bucket Click on Grant Access Add Service Account which is attached to the Overwatch Job cluster in the Add Principal section Add Roles - Storage Admin Storage Legacy Bucket Owner GCP \u0026ndash; Remote Cluster Logs - Databricks on GCP, does not support mounted/GCS bucket locations. Customers must provide DBFS root path as a target for log delivery.\n"
},
{
	"uri": "http://localhost:1313/overwatch/deployoverwatch/ucedeployment/uceconfiguration/",
	"title": "UC Configuration Details",
	"tags": [],
	"description": "",
	"content": "Configuration changes required for UC Enablement There is no unique configuration process for UC; however, the format for the THREE CONFIGURATIONS below are specific for UC Enablement. For all other configurations, please follow the Configuration\netl_database_name - \u0026lt;catalog_name\u0026gt;.\u0026lt;etl_database_name\u0026gt; consumer_database_name - \u0026lt;catalog_name\u0026gt;.\u0026lt;consumer_database_name\u0026gt; storage_prefix - \u0026lt;UC External Location\u0026gt;/\u0026lt;storage_prefix\u0026gt; "
},
{
	"uri": "http://localhost:1313/overwatch/deployoverwatch/runningoverwatch/jar/",
	"title": "As A JAR",
	"tags": [],
	"description": "",
	"content": "Deploying Overwatch As A JAR On Databricks Workflows This deployment method requires Overwatch Version 0.7.1.0+\nMain Class The main class for job is com.databricks.labs.overwatch.MultiWorkspaceRunner\nDependent Library com.databricks.labs:overwatch_2.12:0.8.x.x\ncom.microsoft.azure:azure-eventhubs-spark_2.12:2.3.21 (Azure only - If not using system tables)\ncom.microsoft.azure:msal4j:1.10.1 (Azure Only - With AAD Auth For EH, if not using system tables)\nParameters As of 0.7.1.1 the config.csv referenced below can be any one of the following\n\u0026ldquo;dbfs:/path/to/config.csv\u0026rdquo; \u0026ndash; original config csv approach still works (must end with .csv) \u0026ldquo;dbfs:/path/to/deltaTable\u0026rdquo; \u0026ndash; path to a delta table containing the config \u0026ldquo;myDatabase.myConfigTable\u0026rdquo; \u0026ndash; name of delta table that contains the config Note: any of the paths in examples above may be on any supported storage, dbfs:/ is not required.\nJob can take upto 3 arguments\nArgs(0): Path of Config.csv (Mandatory) EX: [\u0026quot;dbfs:/path/to/config.csv\u0026quot;] Args(1): Number of threads to complete the task in parallel. Default == 4. (Optional) EX: [\u0026quot;dbfs:/path/to/config.csv\u0026quot;, \u0026quot;4\u0026quot;] Args(2): Pipelines to be executed. Default == \u0026ldquo;Bronze,Silver,Gold\u0026rdquo; If you wanted to split Bronze into one task and Silver/Gold into another task the arguments would look like the examples below. Bronze Only Task - [\u0026quot;dbfs:/path/to/config.csv\u0026quot;, \u0026quot;4\u0026quot;, \u0026quot;Bronze\u0026quot;] Silver/Gold Task - [\u0026quot;dbfs:/path/to/config.csv\u0026quot;, \u0026quot;4\u0026quot;, \u0026quot;Silver,Gold\u0026quot;] Running all the pipelines together will maximize cluster utilization but there are often reasons to split the pipelines thus we\u0026rsquo;ve added support. "
},
{
	"uri": "http://localhost:1313/overwatch/deployoverwatch/configureoverwatch/securityconsiderations/",
	"title": "Security Considerations",
	"tags": [],
	"description": "",
	"content": "API Access Overwatch utilizes several APIs to normalize the platform data. Overwatch leverages secret scopes and keys to acquire a token that is authorized to access the platform. The account that owns the token (i.e. dapi token) must have read access to the assets you wish to manage. If the token owner is a non-admin account the account must be granted read level access to the assets to be monitored.\nScope Access Required clusters Can Attach To \u0026ndash; all clusters clusterEvents Can Attach To \u0026ndash; all clusters pools Read \u0026ndash; all Pools jobs Read \u0026ndash; all Jobs dbsql Read \u0026ndash; all Warehouses accounts Token MUST BE ADMIN If the scope is not referenced above it does not require API access\nAccess To The Secrets The owner of the Overwatch Job must have read access to the secret scopes used to store any/all credentials used by the Overwatch pipeline.\nFor example, if John runs a notebook, John must have access to the secrets used in the config. The same is true if John creates a scheduled job and sets it to run on a schedule. This changes when it goes to production though and John sends it through production promotion process and the job is now owned by the etl-admin principle. Since the job is now owned by the etl-admin principle, etl-admin must have access to the relevant secrets.\nStorage Access In addition to API authentication / authorization there are some storage access that Overwatch requires.\nAudit Logs (AWS/GCP) Overwatch cluster must be able to read the audit logs\nOverwatch Target Location Overwatch cluster must be able to read/write from/to the output location (i.e. storage prefix)\nOverwatch, by default, will create a single database that, is accessible to everyone in your organization unless you specify a location for the database in the configuration that is secured. Several of the modules capture fairly sensitive data such as users, userIDs, etc. It is suggested that the configuration specify two databases in the configuration:\nETL database \u0026ndash; hold all raw and intermediate transform entities. This database can be secured allowing only the necessary data engineers direct access. Consumer database \u0026ndash; Holds views only and is easy to secure using Databricks\u0026rsquo; table ACLs (assuming no direct scala access). The consumer database holds only views that point to tables so additional security can easily be attributed at this layer. Additionally, when registering the Overwatch Databases on remote workspaces certain organization_ids can be specified so that only the data for those workspaces are present in this consumer Database. For more information on how to configure the separation of ETL and consumption databases, please reference the configuration page.\nAdditional steps can be taken to secure the storage location of the ETL entities as necessary. The method for securing access to these tables would be the same as with any set of tables in your organization.\nRecommended Authorizations Approach For AWS \u0026ndash; Create an IAM Role and provision it with the following authorizations and then enable an Instance Profile to utilize the IAM Role for the cluster read/write access to the Overwatch Output Storage read access to all locations where cluster logs are stored For GCP \u0026ndash; Create Google Service Account and provision it with the following authorizations and attach it to the cluster read/write access to the Overwatch Output Storage read access to all locations where cluster logs are stored For Azure \u0026ndash; Create an SPN and provision it with and then add the configuration below to authorize the cluster to use the SPN to access the storage locations. read/write access to the Overwatch Output Storage read access to all locations where cluster logs are stored Event Hub Access (AZURE ONLY) In Azure the audit logs must be acquired through an Event Hub stream. The details for configuring and provisioning access are detailed in the Azure Cloud Infrastructure Section\nAzure Storage Auth Config Fill out the following and add it to your cluster as a spark config. For more information please reference the Azure DOCS\nNote that there are two sets of configs, both are required as both spark on the executors and the driver must be authorized to the storage.\nfs.azure.account.auth.type OAuth fs.azure.account.oauth.provider.type org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider fs.azure.account.oauth2.client.id \u0026lt;application-id\u0026gt; fs.azure.account.oauth2.client.secret {{secrets/\u0026lt;SCOPE_NAME\u0026gt;/\u0026lt;KEY_NAME\u0026gt;}} fs.azure.account.oauth2.client.endpoint https://login.microsoftonline.com/\u0026lt;directory-id\u0026gt;/oauth2/token spark.hadoop.fs.azure.account.auth.type OAuth spark.hadoop.fs.azure.account.oauth.provider.type org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider spark.hadoop.fs.azure.account.oauth2.client.id \u0026lt;application-id\u0026gt; spark.hadoop.fs.azure.account.oauth2.client.secret {{secrets/\u0026lt;SCOPE_NAME\u0026gt;/\u0026lt;KEY_NAME\u0026gt;}} spark.hadoop.fs.azure.account.oauth2.client.endpoint https://login.microsoftonline.com/\u0026lt;directory-id\u0026gt;/oauth2/token Accessing The Data From DBSQL with UC enabled on the workspace To provide access to the views in DBSQL follow the steps below. Note, you will need an account admin to complete these steps. More details on these steps can be found in the Databricks documentation. These steps are necessary since the Overwatch tables are stored as external tables so direct access to the storage for the users must be provisioned through DBSQL.\nGrant user appropriate access to the schema / tables Create a storage credential for DBSQL Create an external location that points to the Overwatch Storage Credential Grant users access to the External Location "
},
{
	"uri": "http://localhost:1313/overwatch/dataengineer/",
	"title": "Data Engineering",
	"tags": [],
	"description": "",
	"content": "This section is meant for analysts and Data Engineers looking for more detail on managing the Overwatch Data Ingestion Process and also contains the data dictionary for all columns delivered to the presentation layer.\nData Dictionary (Latest) 0.7.1.x 0.6.1.x 0.5.x Overwatch Pipeline Details \u0026amp; Management Overview Ingesting \u0026amp; Incremental Loading Pipeline Report (PipReport) Overwatch Pipeline Time Module Flow \u0026amp; Dependencies Reloading Data Executing Specific Pipelines (i.e. Bronze/Silver/Gold) Upgrading Overwatch - Overview Productionizing Overwatch Unifying Cluster Logs Bronze Backup Alerting On Module Failures Externalize Optimize \u0026amp; Z-Order Overwatch ERD For full fidelity image, right click and save the image.\n"
},
{
	"uri": "http://localhost:1313/overwatch/deployoverwatch/runningoverwatch/",
	"title": "Running Overwatch",
	"tags": [],
	"description": "",
	"content": "An Overwatch deployment is simply a workload and as such can be run within a notebook or as a Job using a Main class\nAs of Version 0.7.1.0 To deploy Overwatch through the following methods it is required to have version 0710+\nRunning Overwatch as a Notebook Running Overwatch as a JAR Legacy Deployments As of version 0.8.0.0 Overwatch is sunsetting legacy deployment methods. Please reference the links above for a simpler and more advanced method for deploying Overwatch.\nMigrating From Legacy Deployments (versions \u0026lt;0710) If you\u0026rsquo;re ready to migrate from the legacy deployment to the new deployment method, all you need to do is:\nStop all the Overwatch jobs on all workspaces Follow the deployment guide here BEFORE YOU RUN DEPLOYMENT \u0026ndash; if you\u0026rsquo;re using an internal metastore, execute the drop database commands below This will drop your database schemas / metadata (NOT THE DATA) and ensure all the metadata paths line up. After you complete the subsequent Overwatch run the databases and all the data will reappear in your metastore. This will take your databases offline for a bit (on this workspace only) while the Overwatch job runs. drop database \u0026lt;etl_database_name\u0026gt; cascade; drop database \u0026lt;consumer_database_name\u0026gt; cascade; "
},
{
	"uri": "http://localhost:1313/overwatch/dataengineer/pipeline_management/",
	"title": "Pipeline_Management",
	"tags": [],
	"description": "",
	"content": "Overwatch Data Promotion Process Overwatch data is promoted from bronze - silver - gold - presentation to ensure data consistency and quality as the data is enriched between the stages. The presentation layer is composed of views that reference the latest schema version of the gold layer. This disconnects the consumption layer from the underlying data structure so that developers can transparently add and alter columns without user disruption. All tables in each layer (except consumption) are suffixed in the ETL database with _layer.\nNote, the ETL database and consumption database are usually different, determined by the configuration.\nCommon Terms \u0026amp; Concepts Snapshot A snapshot is a point in time image of a context. For example, Cluster and Job definitions come and go in a Databricks workspace but at the time of an Overwatch run, there is a state and the snapshots capture this state.\nSeveral snapshot tables can be found in the bronze layer such as clusters_snapshot_bronze. As a specific example, this snapshot is an entire capture of all existing clusters and definitions at the time of the Overwatch run. This is an important reference point, especially in the early days of running Overwatch, as depending on when the process and logging was begun, many of the existing clusters were created (and last edited) before the audit logs could capture it therefore, the existence and metadata of a cluster would be missing from Overwatch if not for the snapshot. The snapshots also serve as a good last resort lookup for any missing data through time, espeically as it pertains to assets that exist for a longer period of time.\nSnapshots should not be the first choice when looking for data regarding slow changing assets (such as clusters) as the resolution is expectedly low, it\u0026rsquo;s much better to hit the slow changing dimensions in gold, cluster, or even in silver such as cluster_spec_silver. If the raw data you require has not been enriched from raw state then you likely have no other choice than to go directly to the audit logs, audit_log_bronze, and filter down to the relevant service_name and action.\nJob Be weary of the term JOB when working with Overwatch (and Spark on Databricks in general). A Databricks Job is a very different entity than a Spark Job. The documentation attempts to be very clear, please file a ticket if you find a section in which the difference is not made clear.\nSpark Context Composition \u0026amp; Hierarchy Data Ingestion and Resume Process The specificities can vary slightly between cloud provider but the general methodology is the exact same.\nEach module is responsible for building certain entities at each layer, bronze, silver, gold, and presentation. The mapping between module and gold entity can be found in the Modules section. Each module is also tracked individually in the primary tracking tabe, Pipeline_Report.\nThe Overwatch Execution Process Each Overwatch Run takes a snapshot at a point in time and will be the uppermost timestamp for which any data will be captured. Each Overwatch Run also derives an associated GUID by which it can be globally identified throughout the run. Each record created during the run will have an associated Overwatch_RunID and Pipeline_SnapTS which can be used to determine exactly when a record was loaded.\nThe \u0026ldquo;from_time\u0026rdquo; will be derived dynamically for each module as per the previous run snapshot time for the given module plus one millisecond. If the module has never been executed, the primordialDateString will be used which is defined as current_date - 60 days, by default. This can be overridden in the config. Some modules can be very slow / time-consuming on the first run due to the data volume and/or trickle loads from APIs. Best practice is to set your primordialDateString (if desired \u0026gt; 60d) AND set \u0026ldquo;maxDaysToLoad\u0026rdquo; to a low number like 5. It\u0026rsquo;s important to balance sufficient data to fully build out the implicit schemas and small enough data to get through an initial valid test.\nMore best practices can be found in the Best Practices section; these are especially important to reference when getting started.\nThe Pipeline Report The Pipeline_Report table is the state table for the Overwatch pipeline. This table controls the start/stop points for each module and tracks the status of each run. Manipulating this table will change the way Overwatch executes so be sure to read the rest of this page before altering this table.\nThe Pipeline Report is very useful for identifying issues in the pipeline. Each module, each run is detailed here. The structure of the table is outlined below.\nPipeline_Report Structure SAMPLE\nKEY \u0026ndash; organization_id + moduleID + Overwatch_RunID\nWrite Mode \u0026ndash; Append\nFor the developers \u0026ndash; This output is created as a DataSet via the Case Class com.databricks.labs.overwatch.utils.ModuleStatusReport\nColumn Type Description organization_id string Canonical workspace id workspace_name string Customizable human-legible name of the workspace, should be globally unique within the organization moduleId int Module ID \u0026ndash; Unique identifier for the module moduleName string Name of the module primordialDateString string The date from which Overwatch will capture the details. The format should be yyyy-MM-dd ex: 2022-05-20 == May 20 2022 runStartTS long Snapshot time of when the Overwatch run begins runEndTS long Snapshot time of when the Overwatch run ends fromTS long The snapshot start time from which data will be loaded for the module untilTS long The snapshot final time from which data will be loaded for the module. This is also the epoch millis of Pipeline_SnapTS status string Terminal Status for the module run (SUCCEEDED, FAILED, ROLLED_BACK, EMPTY, etc). When status is failed, the error message and stack trace that can be obtained is also placed in this field to assist in finding issues writeOpsMetrics map Contains write metric for a particular chunk of data written to a table. It has information like number of files, Output Rows and Output bytes being written lastOptimizedTS long Epoch millis of the last delta optimize run. Used by Overwatch internals to maintain healthy and optimized tables. vacuumRetentionHours int Retention hours for delta \u0026ndash; How long to retain deleted data inputConfig struct Captured input of OverwatchParams config at time of module run parsedConfig struct The config that was parsed into OverwatchParams through the json deserializer Pipeline_SnapTS timestamp Timestamp when the module was completed as part of the pipeline. This timestamp is reported in server time. Overwatch_RunID string GUID for the overwatch run The PipReport View A pipReport view has been created atop the pipeline_report table to simplify reviewing the historical runs. The query most commonly used is below. If you are asked for the pipReport, please provide the output from the following\nSCALA\ntable(\u0026#34;overwatch_etl.pipReport\u0026#34;) .filter(\u0026#39;organization_id === \u0026#34;\u0026lt;workspace_id\u0026gt;\u0026#34;) // if you want to see the pipReport for only a specific workspace .orderBy(\u0026#39;Pipeline_SnapTS.desc) SQL\nselect * from overwatch_etl.pipReport where organization_id = \u0026#39;\u0026lt;workspace_id\u0026gt;\u0026#39; -- if you want to see the pipReport for only a specific workspace order by Pipeline_SnapTS desc The structure of the table is outlined below.\nPipReport Structure SAMPLE\nKEY \u0026ndash; organization_id + moduleID + Overwatch_RunID\nWrite Mode \u0026ndash; Append\nColumn Type Description organization_id string Canonical workspace id workspace_name string Customizable human-legible name of the workspace, should be globally unique within the organization module_id int Module ID \u0026ndash; Unique identifier for the module module_name string Name of the module primodialDate date The date from which Overwatch will capture the details. The format should be yyyy-MM-dd ex: 2022-05-20 == May 20 2022 fromTS long The snapshot start time from which data will be loaded for the module untilTS long The snapshot final time from which data will be loaded for the module. This is also the epoch millis of Pipeline_SnapTS status string Terminal Status for the module run (SUCCEEDED, FAILED, ROLLED_BACK, EMPTY, etc). When status is failed, the error message and stack trace that can be obtained is also placed in this field to assist in finding issues write_metrics string Contains write metric for a particular chunk of data written to a table. It has information like number of files, Output Rows and Output bytes being written. Pipeline_SnapTS timestamp Timestamp when the module was completed as part of the pipeline. This timestamp is reported in server time. Overwatch_RunID string GUID for the overwatch run Overwatch Time Overwatch will default timezone to that of the server / cluster on which it runs. This is a critical point to be aware of when aggregating Overwatch results from several workspaces across different regions.\nModule Dependencies Several modules depend on other modules as source input; as such, as you can imagine, we don\u0026rsquo;t want modules to progress if one of their dependencies is erroring out and not populating any data. To handle this, Modules have dependencies such that upstream failures will cause downstream modules to be ineligible to continue. There are several types of errors that result in a module failure, and those errors are written out in the pipeline_report when they occur. Sometimes, there will truly be no new data in an upstream Module, this is ok and will not stop the progression of the \u0026ldquo;until\u0026rdquo; time as no data is not considered an error. Below are the ETL Modules and their dependencies\nModule Dependencies Diagram Module ID Module Name Target Table Name Module ID Dependencies As Of 1001 Bronze_Jobs_Snapshot jobs_snapshot_bronze 0.6.0 1002 Bronze_Clusters_Snapshot clusters_snapshot_bronze 0.6.0 1003 Bronze_Pools pools_snapshot_bronze 0.6.0 1004 Bronze_AuditLogs audit_log_bronze 0.6.0 1005 Bronze_ClusterEventLogs cluster_events_bronze 1004 0.6.0 1006 Bronze_SparkEventLogs spark_events_bronze 1004 0.6.0 1007 Bronze_Libraries_Snapshot libs_snapshot_bronze 0.7.1.1 1008 Bronze_Policies_Snapshot policies_snapshot_bronze 0.7.1.1 1009 Bronze_Instance_Profile_Snapshot instance_profiles_snapshot_bronze 0.7.1.1 1010 Bronze_Token_Snapshot tokens_snapshot_bronze 0.7.1.1 1011 Bronze_Global_Init_Scripts_Snapshot global_inits_snapshot_bronze 0.7.1.1 1012 Bronze_Job_Runs_Snapshot* job_runs_snapshot_bronze 0.7.1.1 1013 Bronze_Warehouses_Snapshot warehouses_snapshot_bronze 0.7.2 2003 Silver_SPARK_Executors spark_executors_silver 1006 0.6.0 2005 Silver_SPARK_Executions spark_executions_silver 1006 0.6.0 2006 Silver_SPARK_Jobs spark_jobs_silver 1006 0.6.0 2007 Silver_SPARK_Stages spark_stages_silver 1006 0.6.0 2008 Silver_SPARK_Tasks spark_tasks_silver 1006 0.6.0 2009 Silver_PoolsSpec pools_silver 1003,1004 0.6.0 2010 Silver_JobsStatus job_status_silver 1004 0.6.0 2011 Silver_JobsRuns jobrun_silver 1004,2010,2014 0.6.0 2014 Silver_ClusterSpec cluster_spec_silver 1004 0.6.0 2016 Silver_AccountLogins accountlogin 1004 0.6.0 2017 Silver_ModifiedAccounts account_mods_silver 1004 0.6.0 2018 Silver_Notebooks notebook_silver 1004 0.6.0 2019 Silver_ClusterStateDetail cluster_state_detail_silver 1005 0.6.0 2020 Silver_SQLQueryHistory sql_query_history_silver 1004 0.7.0 2021 Silver_WarehouseSpec warehouse_spec_silver 1004,1013 0.7.2 3001 Gold_Cluster cluster_gold 2014 0.6.0 3002 Gold_Job job_gold 2010 0.6.0 3003 Gold_JobRun jobrun_gold 2011 0.6.0 3004 Gold_Notebook notebook_gold 2018 0.6.0 3005 Gold_ClusterStateFact clusterstatefact_gold 1005,2014 0.6.0 3007 Gold_AccountMod account_mods_gold 2017 0.6.0 3008 Gold_AccountLogin account_login_gold 2016 0.6.0 3009 Gold_Pools instancepool_gold 2009 0.6.0 3010 Gold_SparkJob sparkjob_gold 2006 0.6.0 3011 Gold_SparkStage sparkstage_gold 2007 0.6.0 3012 Gold_SparkTask sparktask_gold 2008 0.6.0 3013 Gold_SparkExecution sparkexecution_gold 2005 0.6.0 3014 Gold_SparkExecutor sparkexecutor_gold 2003 0.6.0 3016 Gold_SparkStream sparkstream_gold 1006,2005 0.7.0 3015 Gold_jobRunCostPotentialFact jobruncostpotentialfact_gold 3001,3003,3005,3010,3012 0.6.0 3017 Gold_Sql_QueryHistory sql_query_history_gold 2020 0.7.0 3018 Gold_Warehouse warehouse_gold 2021 0.7.2.1 3019 Gold_NotebookCommands notebookCommands_gold 1004,3004,3005 0.7.2.1 * Bronze_Job_Runs_Snapshot is experimental as of 0711. The module works as expected but the API can only pull 25 runs per API call; therefore, for some customers with many runs per day (i.e. thousands) this module can take several hours. If you truly want this module enabled you can enable it but additional optimization work may be required to make it efficient if your workspace executes thousands of job runs per day. TO ENABLE set the following as a cluster spark config overwatch.experimental.enablejobrunsnapshot true\nReloading Data CAUTION Be very careful when attempting to reload bronze data, much of the bronze source data expires to minimize storage costs. Reloading bronze data should be a very rare task (hopefully never necessary)\nThe need may arise to reload data. This can be easily accomplished by setting the \u0026ldquo;Status\u0026rdquo; column to \u0026ldquo;ROLLED_BACK\u0026rdquo; for the modules you wish to reload where:\nfromTime column \u0026gt; some point in time OR Overwatch_RunID === some Overwatch_Run_ID (useful if duplicates get loaded) Remember to specify the ModuleID for which you want to reload data.\nReloading Data Example Scenario Something has occurred and has corrupted the jobrun entity after 01-01-2021 but the issue wasn\u0026rsquo;t identified until 02-02-2021. To fix this, all data from 12-31-2020 forward will be rolled back. Depending on the details, the untilTime for the run on 12-31-2020 to ensure no data gaps or overlaps (for the examples below we\u0026rsquo;ll assume the untilTime was midnight on 12-31-2020); for this scenario we will go back an extra day and use Pipeline_SnapTS as the incremental time column. Since we\u0026rsquo;re using delta, we are safe to perform updates and deletes without fear of losing data (In the event of an issue the table could be restored to a previous version using delta time travel).\nReloading spark silver requires one extra step. If reloading spark silver data click the link.\nSteps \u0026ndash; Automatic Method As Of version 0.7.0 The following example will automatically detect the point in time at which the modules had a break point in the pipeline to ensure there are no data gaps or overlaps.\nimport com.databricks.labs.overwatch.utils.Helpers val workspace = Helpers.getWorkspaceByDatabase(\u0026#34;overwatch_etl\u0026#34;) val rollbackToTS = 1609416000000L // unix millis midnight Dec 31 2020 val modulesToRollback = Array(3005, 3015) // gold cost tables val isDryRun = true // switch to false when you\u0026#39;re ready to run val rollbackStatusText = \u0026#34;DataRepair_Example\u0026#34; // text to appear in the status column after the rollback val workspaceIDs = Array(\u0026#34;123\u0026#34;, \u0026#34;456\u0026#34;) // two workspaces to rollback the modules in // val workspaceIDs = Array(\u0026#34;global\u0026#34;) // use \u0026#34;global\u0026#34; to rollback all workspaces // Putting it all together Helpers.rollbackPipelineForModule(workspace, rollbackToTS, modulesToRollback, workspaceIDs, isDryRun, rollbackStatusText) Steps \u0026ndash; Manual Method: The method below is completely viable and the method used for years prior to the automated method above. It can be challenging to rollback each module to exactly the right timestamp to ensure no data gaps or overlaps. The method below assumes that the pipeline\u0026rsquo;s untilTime (from pipReport) for the run ended exactly at midnight to ensure zero gaps or overlaps.\nVerify Overwatch isn\u0026rsquo;t currently running and isn\u0026rsquo;t about to kick off for this workspace. Delete from jobRun_silver table where Pipeline_SnapTS \u0026gt;= \u0026ldquo;2020-12-31\u0026rdquo; spark.sql(\u0026#34;delete from overwatch_etl.jobRun_Silver where organization_id = \u0026#39;\u0026lt;ord_id\u0026gt;\u0026#39; and cast(Pipeline_SnapTS as date) \u0026gt;= \u0026#39;2020-12-31\u0026#39; \u0026#34;) Delete from jobRun_gold table where Pipeline_SnapTS \u0026gt;= \u0026ldquo;2020-12-31\u0026rdquo; spark.sql(\u0026#34;delete from overwatch_etl.jobRun_Gold where organization_id = \u0026#39;\u0026lt;ord_id\u0026gt;\u0026#39; and cast(Pipeline_SnapTS as date) \u0026gt;= \u0026#39;2020-12-31\u0026#39; \u0026#34;) Update the Pipeline_Report table set Status = \u0026ldquo;ROLLED_BACK\u0026rdquo; where moduleId in (2011,3003) and status in (\u0026lsquo;SUCCESS\u0026rsquo;, \u0026lsquo;EMPTY\u0026rsquo;). Modules 2011 and 3003 are the Silver and Gold modules for jobRun respectively Only for Status values of \u0026lsquo;SUCCESS\u0026rsquo; and \u0026lsquo;EMPTY\u0026rsquo; because we want to maintain any \u0026lsquo;FAILED\u0026rsquo; or \u0026lsquo;ERROR\u0026rsquo; statuses that may be present from other runs spark.sql(s\u0026#34;update overwatch_etl.pipeline_report set status = \u0026#39;ROLLED_BACK\u0026#39; where organization_id = \u0026#39;\u0026lt;ord_id\u0026gt;\u0026#39; and moduleID in (2011,3003) and (status = \u0026#39;SUCCESS\u0026#39; or status like \u0026#39;EMPT%\u0026#39;) \u0026#34;) After performing the steps above, the next Overwatch run will load the data from 12-31 -\u0026gt; current. Rebuilding Spark Bronze \u0026ndash; If you want to reprocess data in the spark modules, it\u0026rsquo;s strongly recommended that you never truncate / rebuild spark_events_bronze as the data is quite large and the source log files have probably been removed from source due to retention policies. If you must spark bronze you must also update one other table; spark_events_processedfiles in the etl database. If you\u0026rsquo;re rolling back the spark modules from Feb 02, 2021 to Dec 31, 2020, you would need to delete the tracked files where Pipeline_SnapTS \u0026gt;= Dec 31, 2020 and failed = false and withinSpecifiedTimeRange = true.\nFor smaller, dimensional, silver/gold entities and testing, it\u0026rsquo;s often easier to just drop the table than it is to delete records after some date. Remember, though, it\u0026rsquo;s critical to set the primordialDateString from time if rebuilding entities with more than 60 days.\nRunning Only Specific Layers As in the example above, it\u0026rsquo;s common to need to rerun only a certain layer for testing or reloading data. In the example above it\u0026rsquo;s inefficient to run the bronze layer repeatedly to reprocess two tables in silver and gold. If no new data is ingested into bronze, no modules outside of the two we rolled_back will have any new data and thus will essentially be skipped resulting in the re-processing of only the two tables we\u0026rsquo;re working on.\nThe main class parameters allows for a single pipeline (bronze, silver, or gold) or all to be run. Refer to the Getting Started - Run Via Main Class for more information on how to run a single pipeline from main class. The same can also be done via a notebook run, refer to Getting Started - Run Via Notebook for more information on this.\n"
},
{
	"uri": "http://localhost:1313/overwatch/deployoverwatch/ucedeployment/migratingtouc/",
	"title": "Migrating To UC",
	"tags": [],
	"description": "",
	"content": "Migrating Existing Deployment From Hive_Metastore To UC Migrating a deployment from Hive Metastore has been made very simple. The steps are\nComplete the UC Pre-Requisites Ensure storage is set up correctly Update the Overwatch Configuration appropriately for UC Use the Migration Notebook below to migrate the data from Hive to UC Resume the job Migration Notebook Migration Notebook ( HTML | DBC ) Details to Run are in the notebook "
},
{
	"uri": "http://localhost:1313/overwatch/deployoverwatch/sharingoverwatch/",
	"title": "Sharing Overwatch Data",
	"tags": [],
	"description": "",
	"content": "Granting Access Via DBSQL If your storage prefix is referencing a mount point, nothing should be necessary here as the mount point is already accessible to all workspace users. If using a direct path (i.e. s3://, abfss://, or gs://) you will need to configure the appropriate access in Admin Settings \u0026ndash;\u0026gt; SQL Warehouse Settings.\nIf using a Hive Metastore you may still need to review your grants to ensure users have access to the Overwatch tables / views. Additionally, you may need to grant select on any file to the appropriate user groups if direct path is used for overwatch instead of a mount point. GRANT SELECT ON ANY FILE TO `\u0026lt;user\u0026gt;@\u0026lt;domain-name\u0026gt;` If using Unity Catalog be sure to provision the appropriate access to the user groups on the Overwatch Schemas.\nProviding Access to Consumers On Remote Workspaces Now that Overwatch is deployed, it\u0026rsquo;s likely that you want to share some or all of the data with stakeholders. We\u0026rsquo;ve made that easy to do across workspaces with a single function and it\u0026rsquo;s parameters Helpers.registerRemoteOverwatchIntoLocalMetastore. This is a one time command that must be run on the workspace that wants to consumer the Overwatch Data.\nThis is only necessary when using multi-workspace deployment and not using a shared metastore.\nRegistering Overwatch On Remote Workspaces The command below is the simplest version and will publish the etl and consumer databases with all the data to the workspace on which the command is run. Note, this must be run on the target workspace, not the deployment workspace.\nThe following are just a few basic examples, please refer to the optional parameters below for a full list of options and details that can be passed to the function.\nAs an example let\u0026rsquo;s say we have two workspaces. Overwatch was deployed on workspace 123 (i.e. 123 is the workspace ID) and is configured to monitor workspaces 123 and 234. After initial deployment users on workspace 234 will not be able to see the databases. If you wish for the users on 234 to be able to see the database[s] simply run the function below based on your needs\nYou want users of workspace 234 to see all Overwatch data\nRun the following command on workspace 234 and the ETL and Consumer databases and all tables and all data will appear in the metastore for workspace 234.\nimport com.databricks.labs.overwatch.utils.Helpers Helpers.registerRemoteOverwatchIntoLocalMetastore(remoteStoragePrefix = \u0026#34;/mnt/overwatch_global/overwatch-5434\u0026#34;, remoteWorkspaceID = \u0026#34;123\u0026#34;) You want users of workspace 234 to see only their data\nIf you only want workspace 234 users to see their data (i.e. from workspace 234 not 123) then run the following command. This will register only the consumer database and only the records that pertain to workspace 234. The ETL database will not be published.\nimport com.databricks.labs.overwatch.utils.Helpers Helpers.registerRemoteOverwatchIntoLocalMetastore(remoteStoragePrefix = \u0026#34;/mnt/overwatch_global/overwatch-5434\u0026#34;, remoteWorkspaceID = \u0026#34;123\u0026#34;, workspacesAllowed = Array(\u0026#34;234\u0026#34;)) You want users of workspace 234 to see Overwatch data for several workspaces\nIf there were many workspaces and you wanted to share data from multiple workspaces it\u0026rsquo;s the same command just add to the additional workspaceIDs to the \u0026ldquo;workspacesAllowed\u0026rdquo; Array. Obviously this would require Overwatch to be configured for more workspaces but this is meant to extend the examples above.\nimport com.databricks.labs.overwatch.utils.Helpers Helpers.registerRemoteOverwatchIntoLocalMetastore(remoteStoragePrefix = \u0026#34;/mnt/overwatch_global/overwatch-5434\u0026#34;, remoteWorkspaceID = \u0026#34;123\u0026#34;, workspacesAllowed = Array(\u0026#34;234\u0026#34;, \u0026#34;345\u0026#34;, \u0026#34;456\u0026#34;)) Requirements Overwatch JAR - The cluster used to run the commands above on workspace 234 must have Overwatch JAR 0.7.1.0+ attached to the cluster. Security - Any compute that will be reading the data must have storage access to read the storage prefix, in the example above, \u0026ldquo;/mnt/overwatch_global/overwatch-5434\u0026rdquo;. If the path is mounted on workspace 234, this access will be available to all compute in the workspace. If only certain clusters should have access the cluster must be configured with an Instance Profile (AWS) or SPN permissions (AZURE) Database Names - The local database names don\u0026rsquo;t already exist. Unless overridden, these names will be the same as on workspace 123. This feature will not work with legacy deployments. The databases and views are registered by default during the Overwatch pipeline run. Consider upgrading if you would like to use this feature.\nFunction Argument Details This function currently support below arguments:\nKey Type Default Value Description remoteStoragePrefix String NA Remote storage prefix for the remote workspace where overwatch has been deployed remoteWorkspaceID String NA workSpaceID for the remoteworkspace where overwatch has been deployed. This will be used in getRemoteWorkspaceByPath() localETLDatabaseName String Same name as the ETL database on the deployment workspace ETLDatabaseName that user want to override. If not provided then by default etlDatabase name from the remoteWorkspace would be used as same for current workspace localConsumerDatabaseName String Same name as the Consumer database on the deployment workspace ConsumerDatabase that user want to override. If not provided then by default ConsumerDatabase name from the remoteWorkspace would be used as same for current workspace remoteETLDataPathPrefixOverride String \u0026quot;\u0026quot; Param to override StoragePrefix. If not provided then remoteStoragePrefix+\u0026quot;/global_share\u0026quot; would be used as StoragePrefix usingExternalMetastore Boolean false Used in case if user using any ExternalMetastore. workspacesAllowed Array[String] Array() If we want to populate the table for a specific workSpaceID. In that case only ConsumerDB would be visible to the user. How it Works The consumer database is just a database full of views, so when you pass in a list of workspaceIds into the \u0026ldquo;workspacesAllowed\u0026rdquo; parameter, it registers\nExample With Screenshots Below are the details of how the function works:\nWe have deployed Overwatch on one of our workspaces (workspace with id 123) with the below params: Now if we want to deploy the tables from this workspace to a new workspace, say workspaceID 234 to continue our previous example. Run the below command on workspace 234 Helpers.registerRemoteOverwatchIntoLocalMetastore(remoteStoragePrefix = \u0026#34;/mnt/overwatch_global/overwatch-5434\u0026#34;,remoteWorkspaceID = \u0026#34;123\u0026#34;, workspacesAllowed = Array()) As we can see the tables are loaded in new workspace. -- from workspace 234 use overwatch_dev_5434; show tables; "
},
{
	"uri": "http://localhost:1313/overwatch/deployoverwatch/configureoverwatch/validation/",
	"title": "Validation",
	"tags": [],
	"description": "",
	"content": "Deployment Validations Many validations are performed to minimize mistakes. The following section offers details on all validations done. Not all validation steps are done on all runs. All validations should be performed before a first run on any given workspace. For more information on executing a full validation, see below\nValidation rules Name Validation Rule Impacted columns Api Url Validation API URL should give some response with provided scope and key. api_url Primordial data validation Primordial Date must be in yyyy-MM-dd format (i.e. 2022-01-30 == Jan 30, 2022) and must be less than current date. primordial_date Excluded scope validation Excluded scope must be null or in colon-delimited format and include only the following audit:sparkEvents:jobs:clusters:clusterEvents:notebooks:pools:accounts:dbsql excluded_scopes Event Hub Shared Access Key (Azure Only) Audit Log data must be recognized as present and readable from the provided Event Hub Configuration fields. eh_name, eh_scope_key, secret_scope Event Hub AAD Auth (Azure Only) Audit Log data must be recognized as present and readable from the provided Event Hub Configuration fields. eh_name, eh_conn_string, aad_tenant_id, aad_client_id, aad_client_secret_key, aad_authority_endpoint Common consumer database name All workspaces must have a common consumer database name. consumer_database_name Common ETL database name All workspaces must have a common ETL database name. etl_database_name Storage prefix validation All workspaces must share a single storage prefix and the Overwatch cluster must have appropriate access to read/write. storage_prefix Cloud provider validation Either Azure or AWS. cloud Max days validation Must Be a Number max_days Secrete scope validation Secret scope must not be empty. secret_scope PAT key validation DBPAT must not be empty and must be able to authenticate to the workspace. secret_key_dbpat Audit log location validation (AWS/GCP ONLY) Audit logs must present immediately within the provided path and must be read accessible auditlogprefix_source_path Mount point validation Workspaces with more than 50 mount points need to provide a csv file which will contain the mount point to source mapping. click here for more details mount_mapping_path System Table Validation System table should enabled and must have data for the workspace. auditlogprefix_source_path Run the validation import com.databricks.labs.overwatch.MultiWorkspaceDeployment val configCsvPath = \u0026#34;dbfs:/FileStore/overwatch/workspaceConfig.csv\u0026#34; // Path to the config.csv // temp location which will be used as a temp storage. It will be automatically cleaned after each run. val tempLocation = \u0026#34;/tmp/overwatch/templocation\u0026#34; // number of workspaces to validate in parallel. Exceeding 20 may require larger drivers or additional cluster config considerations // If total workspaces \u0026lt;= 20 recommend setting parallelism == to workspace count val parallelism = 4 // Run validation MultiWorkspaceDeployment(configCsvPath, tempLocation) .validate(parallelism) Review The Report The validation report will be generated in \u0026lt;etl_storage_prefix\u0026gt;/report/validationReport as delta table. Run the below query to check the validation report. All records should say validated = true or action is necessary prior to deployment.\nNOTE The validation report maintains a full history of validations. If you\u0026rsquo;ve validated multiple times be sure to look at the snapTS and only review the validations relevant for the latest validation run.\n-- full report select * from delta.`\u0026lt;etl_storage_prefix\u0026gt;/report/validationReport` order by snapTS desc -- only validation error select * from delta.`\u0026lt;etl_storage_prefix\u0026gt;/report/validationReport` where validated = \u0026#39;false\u0026#39; order by snapTS desc display( spark.read.format(\u0026#34;delta\u0026#34;).load(\u0026#34;\u0026lt;etl_storage_prefix\u0026gt;/report/validationReport\u0026#34;) .orderBy(\u0026#39;snapTS.desc) ) "
},
{
	"uri": "http://localhost:1313/overwatch/dataengineer/upgrade/",
	"title": "Upgrades",
	"tags": [],
	"description": "",
	"content": "Sometimes upgrading from one version to the next requires a schema change. In these cases, the CHANGELOG will be explicit. Upgrades MUST be executed WITH the new library (jar) and before the pipeline is executed. Basic pseudocode can be found below as a reference. For actual version upgrade scripts please reference the upgrade scripts linked to your target version in the Changelog.\nWhen a schema upgrade is required between versions, this step cannot be skipped. Overwatch will not allow you to continue on a version that requires a newer schema.\nA sample notebook is provided below for reference.\nUpgrade.html / Upgrade.dbc\n"
},
{
	"uri": "http://localhost:1313/overwatch/changelog/",
	"title": "ChangeLog",
	"tags": [],
	"description": "",
	"content": "0.8.1.0 Upgrading There are no mandatory changes in this release, you can simply swap the jar, and Overwatch will run normally. Now, it is recommended that you run this script, given that the changes in this release try to mitigate changes in the source Databricks APIs that could lead to some data loss for some clusters details.\nUpgrade script: HTML | DBC\nAlso, remember to upgrade the jar version used in your Optimizer job :)\nBug Fixes Created a workaround for generating the Clusters snapshot table, since we were using the clusters/list API endpoint, which now has started enforcing limits to the number of clusters returned Improved logic to include late-arriving cluster events Revamped the logic for splitting cluster usage and dbu share per job tasks when they are ran in parallel on the same cluster Released May 8th, 2024 - Full Change Inventory\n0.8.0.1 (Patch) Fixed a bug when creating Warehouse dimension during the very first run\nReleased March 21st, 2024\n0.8.0.0 (Major Release) Upgrading There are no mandatory changes in this release, you can simply swap the jar if you\u0026rsquo;re not migrating Audit logs to read from system tables. We\u0026rsquo;ve created a script to help you in migrating your deployment configuration to system tables, if you choose to do so.\nUpgrade script: HTML | DBC\nMajor Feature First integration with System Tables! Now Overwatch can collect audit logs directly from System Tables. You can find the docs here Bug Fixes In this version there were many bug fixes, as well as the development of internal tooling to help us validate new releases. Some of the most notable bug fixes include:\nCreated a workaround for cases when job clusters are missing terminating cluster events. This fix is very important as it will improve the accuracy of the data. We recommend that you rollback and backprocess the data for modules 2019, 3005 and 3015. If you need a step by step guide on how to do this, follow the steps here Improve error handling for empty upstream sources for Gold_SparkStream, NotebookCommands, and SilverWarehouseSpec Fixed Helpers.getWorkspaceByDatabase which was broken for retrieving remote workspaces Performance improvements for NotebookCommands Added a notebook for troubleshooting configuration issues under Troubleshooting Released Jan 25, 2024 - Full Change Inventory\n0.7.2.2.1 (Patch) This patch addresses some critical fixes. If you ran versions 0721 or 0722, you need to run the upgrade script in order to repair the data.\nUpgrade script: HTML | DBC\nBug Fixes Fixed lookup to ensure NotebookCommands there are no NULL notebook_ids in NotebookCommands There are some new services available in EventHub that if activated, would send duplicated audit log columns. Improve performance of WarehouseSpec table and resolve schema discrepancies in Warehouse Gold which would lead to failures Improves the coverage of job_run_cluster_util by ensuring we can also compute it for jobs submitted using SparkSubmit (Dashboard bug) Fix to metric \u0026ldquo;DBU Spend by the top 3 expensive Interactive clusters\u0026rdquo; in Clusters dashboard Find the updated version here Released Oct 31st, 2023 - Full Change Inventory\n0.7.2.2 This release contains a change to how we query the Databricks Jobs API, to prevent failures, given that the API is changing from offset based pagination, to token based.\nTo Upgrade, if you\u0026rsquo;re currently running Overwatch v0.7.2.0.1 onwards, simply swap the JAR. If you\u0026rsquo;re in a previous version, contact your Databricks representative.\nBug Fixes Updated Jobs API queries to use token based pagination Fixed an issue in the Optimizer code that would show up if the driver workspace was not in the list of workspaces to be optimized Updated GCP instance details to the correct API Names Released Oct 5th, 2023 - Full Change Inventory\n0.7.2.1 Upgrading Although there are no breaking schema changes with this release, we recommend you run the upgrade script to:\nUpdate your instance details to the latest available data (GCP ONLY) Process all the remote workspaces cluster logs, which you weren\u0026rsquo;t able to do until this release. Upgrade script: HTML | DBC\nMajor Features Warehouse dimension now available Schema is available here NotebookCommands table now available Customers will now be able to see metrics for each command run on every notebook in your workspace, including estimated costs Support is limited to Notebooks run on clusters for now. Notebooks run on warehouses will be supported soon Schema is available here It requires Verbose Audit Logging to be enabled in each workspace that needs to be monitored Customers can now Snapshot, Restore, and Migrate their Overwatch data in a few clicks. Read more info about it here Updated Azure and GCP instanceDetails node types to include all new supported node types as of August 24, 2023. Released the first set of pre-canned dashboards You can find them here Key Bug Fixes (GCP) Capture cluster logs for remote workspaces Captured cluster changes when they are only resized when running Improved logic for detecting job/automated clusters in JRCP Ensured AutoOptimize is set for tables that needed Fixed bug where all error messages were printing to \u0026ldquo;API Call\u0026rdquo; incorrectly When Bronze_Clusters_Snapshot didn\u0026rsquo;t have data then Bronze_SparkEventLogs was failing Fixed bug when converting data from Struct to map where keys had multiple periods in it Released Sept 11th, 2023 - Full Change Inventory\n0.7.2.0.4 (Patch) To Upgrade, simply swap the JAR\nBug Fixes Fixed schema issue while merging in job_status_silver 0.7.2.0.3 (Deprecated) Pulled forward all bug fixes from 0715\nTo Upgrade, simply swap the JAR\nBug Fixes PVC - instanceId and containerId duplicate columns in audit_log_bronze handled Released Aug 3rd, 2023 - Full Change Inventory\n0.7.2.0.2 (Deprecated) Pulled forward all bug fixes from 0714\nTo Upgrade, simply swap the JAR\nFull Change Inventory\n0.7.2.0.1 (Patch) Bug Fix - mount_mapping_path was being ignored\nOnly customers using multi-workspace deployments with remote workspaces with \u0026gt; 50 mounts and using mount_mapping_path will be affected.\nTo Upgrade, simply swap the JAR\n0.7.2.0 Upgrading There were a few column name changes to the config file (multi-workspace). Please run this script to update your config file if upgrading from 071x multi-workspace deployment. HTML | DBC\nMajor Features Enablement for GCP Overwatch can now be deployed on Google Compute. The process is identical to AWS Enablement for Unity Catalog Customers can now deploy to either Hive Metastore or Unity Catalog (UC) To deploy to UC follow the guide for Deploying To Unity Catalog To migrate existing Hive Metastore deployments to a UC, follow the Migrating To UC guide Enablement for Event Hub Auth via AAD in multi-workspace deployments Search enabled for docs Updated instanceDetails node types to include all new supported node types as of April 12, 2023. Key Bug Fixes Single Tenant Multi-Workspace None.get issue for workspaceID Pipeline output showing 0 Output Rows for merged tables | Issue 692 (Azure) When Event Hub Library Dependency is missing / corrupted kill bronze pipeline properly Full Change Inventory\n0.7.1.5 (Patch) To Upgrade, simply swap the JAR\nBug Fixes Fixed schema issue while merging in job_status_silver PVC - instanceId and containerId duplicate columns in audit_log_bronze handled Released Aug 3rd, 2023 - Full Change Inventory\n0.7.1.4 (Deprecated) To Upgrade, simply swap the JAR\nFeatures Backported AAD Authentication for Event Hubs Added support for permanently deleted clusters Costs will now stop accumulating for permanently deleted clusters Bug Fixes api calls / job_status_silver - Added lag overlap to handle late arriving events getOrgID fix for single-tenant and PVC backported from 072x PVC - shardName duplicate column in audit_log_bronze handled Only promote crud events for successful events (clusters / jobs / notebooks) Spark Events Bronze - AWS Remote workspaces with s3:// direct logging paths empty Spark Executor Table - empty Spark Tasks \u0026ndash; end timestamp nulls Full Change Inventory\n0.7.1.3 Data Quality Fixes - primarily around cost and utilization Upgrade Process Swap the Jar \u0026ndash; that\u0026rsquo;s it \u0026ndash; all new data will be built with these corrections (Optional) Run Historical Data Repair script HTML | DBC If you want to rebuild historical data, run the script above and Overwatch\u0026rsquo;s subsequent run will rebuild the necessary tables (Optional) Since one of the bug fixes is changing the job run start timestamp, subsequent Overwatch runs can result in duplicate records for job runs. If you experience this you can run the following script once to eliminate duplicate runs. You only need to run this once per deployment (i.e. storage prefix) it will properly dedup all records from all workspaces in the targets. You only need to run this one time after you experience job run duplicates. Note that you will not experience duplicates until after you execute the 0713+ Overwatch Pipeline. The notebook has a check in it to determine whether you\u0026rsquo;ve experienced this issue. HTML | DBC Bug Fixes DBU Photon Costs for Automated workloads DBU multiplier updated Single Node DBUs \u0026amp; Cost Calculations Overwatch was showing 0 DBUs and $0 costs for workers but since the worker is the driver this is not accurate Job Task Run - Start Timestamp The task_runtime was referencing \u0026ldquo;submission_time\u0026rdquo; instead of \u0026ldquo;start_time\u0026rdquo; but it turns out that Databricks does not publish the task start time and all job tasks get submitted at the same time; thus start_time must be used. Job Run Cost Potential Fact - (JRCP) Cluster Utilization Due to the taskRun timestamp issue in the last bullet and a miscalculation for concurrent jobs running on a single cluster the cluster utilization in JRCP could be off \u0026ndash; this was resolved and should now be accurate. Cluster name / id imputes can incorrect in certain scenarios Full Change Inventory\n0.7.1.2 Fixes bugs from 0711 that may affect workflow If already using 0711 \u0026ndash; simply swap the JAR \u0026ndash; no other action necessary If \u0026lt; 0711, be sure to complete the 0711 upgrade script found below. HTML | DBC Full Change Inventory\n0.7.1.1 (Deprecated) USE 0712 Version Bug Fixes Plus a Few New Features\nUpgrade Script \u0026ndash; Simple but necessary script to be run once per Overwatch Deployment (i.e. storage prefix) the details of script are in the script.\nHTML | DBC Noteworthy Bug Fixes Significant Improvement in DBU / Cost calculations DBU calculation accuracy improvement (especially for Photon clusters) Significant Performance improvements Delayed log arrival now ingested properly Completeness of first-run record initialization Noteworthy New Features Remote workspaces with \u0026gt;50 mounts supported Several new bronze tables job_runs_snapshot_bronze \u0026ndash; Experimental To enable add spark config to cluster overwatch.experimental.enablejobrunsnapshot true more details here under the modules table libs_snapshot_bronze policies_snapshot_bronze instance_profiles_snapshot_bronze (AWS) tokens_snapshot_bronze global_inits_snapshot_bronze Multi-workspace config delta compatible now (instead of only CSV) Allows simple updates to config without download/upload of new config CSV Threaded spark session to avoid collisions Table locking for parallel workspace ingestion (avoids delta concurrent write errors) Full Change Inventory (0711)\n0.7.1.0 Multi-Workspace Deployments Are Here!!\nIf you\u0026rsquo;re tired of managing and maintaining Overwatch jobs on all your workspaces, be tired no more. Migrate your configurations to the new, enhanced configuration method and simplify your deployment with this release. Existing customers wanting to migrate to this deployment method, START HERE\nRequirements You must be running 0.7.0.x before upgrading to this version. No upgrade script is required but steps are required to migrate to the new deployment method.\nAdditional Changes Enhanced Documentation Enhanced API Manager Updated Default hourly DBUs and Node Pricing Upgrade to DBR 11.3LTS as default DBR Performance Improvements Photon Tested and Validated with significant efficiency gains (in most cases) Full Change Inventory\n0.7.0.0.5 (PATCH) Patch for 0.7.0.0.4. Fixes a regression introduced in 0.7.0.0.4 for clusterEventsBronze. This patch also promotes total_dbus field to JRCP table.\nIf you previously ran 0.7.0.0.4 you must run the following repair script to correct data in cluster_events_bronze. If you don\u0026rsquo;t you will receive schema merge errors in clusterEvents bronze module. If you did not run 07004, you do not need to run this script. HTML | DBC 0.7.0.0.4 (PATCH) This is a patch for 0.7.0.0. There were a few issues identified with 0.7.0 by our early movers. This patch is the same as 0700 with the bug fixes closed in PR 633.\nThis update includes all bug fixes published in 07003. No upgrade needed - the Jars may simply be swapped out \u0026ndash; do this on all workspaces. If you were affected by one of the issues fixed in the PR above and you would like to repair the historical data, please do so using this script. Instructions are in the notebook. HTML | DBC Bug Fixes Bug fix related to dbu utilization and cost calculation Issue 632. Bug fix related to sqlQueryHistory Issue 625. 0.7.0.0.3 (PATCH) This is a patch for 0.7.0.0. There were a few issues identified with 0.7.0 by our early movers. This patch is the same as 0700 with the bug fixes closed in PR 602.\nThis update includes all bug fixes published in 07002 No upgrade needed - the Jars may simply be swapped out \u0026ndash; do this on all workspaces If you were affected by one of the issues fixed and you would like to repair the historical data, please do so using this script. Instructions are in the notebook. HTML | DBC 0.7.0.0.2 (PATCH) This is a patch for 0.7.0.0. There were a few issues identified with 0.7.0 by our early movers. This patch is the same as 0700 with the bug fixes closed in PR 580.\nThis update includes all bug fixes published in 07001 No upgrade needed - the Jars may simply be swapped out \u0026ndash; do this on all workspaces If you were affected by one of the issues fixed in the PR above and you would like to repair the historical data, please do so using this script. Instructions are in the notebook. HTML | DBC 0.7.0.0.1 (PATCH) This is a patch for 0.7.0.0. There were a few issues identified with 0.7.0 by our early movers. This patch is the same as 0700 with the bug fixes closed in PR 559. 0.7.0.0 (Major Release) Please use PATCH 0.7.0.0.1 from maven. Everything in this release stands\n0700 Upgrade Script HTML | DBC\nMajor Changes Support for Multi-Task Jobs (MTJs) Prior to this release Overwatch was unable to properly analyze and display jobs with multiple tasks and, at times even struggled to properly display new-format single-task jobs As this is a major change, there are significant changes in the jobs tables in silver and gold Production Dashboards should be validated/updated in non-prod 0.7.0 deployment as there will likely be some changes to column naming, typing, and structures. For details on the new schema, see the Data Dictionary Support for Job Clusters Jobs 2.1 API Support Databricks Jobs team has migrated to API 2.1 and now so has Overwatch. This resolves some data gaps as well as API capacity issues Support for DBSQL Query History Including Metrics DBSQL Query History is our first step into first-party support for DBSQL. Warehouses are expected to follow shortly but was unable to make the cut for this release Be sure to enable the new scope \u0026ldquo;dbsql\u0026rdquo; or use the new 0.7.0 Runner Notebook to ensure you\u0026rsquo;re loading the DBSQL tables if your workspace[s] are using DBSQL. Limitation - Databricks does not publish Warehouse events yet and as such, explicit cost anlaysis is not yet possible for DBSQL. As soon as this is made available the Overwatch Dev team will begin work to integrate it. Photon Costs Attributed in CLSF and JRCP Photon cost calculations have been integrated into Overwatch. By default the Photon costs will begin being applied after the upgrade date. If you want to retroactively apply this change you can by rolling back and rebuilding the following tables as this is not automatically handled in the upgrade (some customers don\u0026rsquo;t want to see that change in their historical reports) cluster_spec_silver clusterstatefact_gold jobruncostpotentialfact_gold New API Manager The API management libraries were completely rebuilt from the ground up to maximize throughput, capabilities, and safety for your environment. The new API manager offers users much deeper control on how Overwatch uses the APIs. See the APIEnv Configs on the Configurations details page. Support for Authenticated PROXY Most customers are able to configure proxies with cluster configs and init scripts but Overwatch now offers deeper, first-party suppot for authenticated Proxy configs. See more details in the APIEnv Configs section. SparkJob User-Email Coverage \u0026amp; Accuracy User_Email in sparkJob table now has complete coverage but may require some action The user_email had incompletions and inaccuracies. Databricks was not publishing the user data for users using Python and R languages. This has now been resolved automatically for clusters (operational clusters not the Overwatch cluster) running DBR 11.3+. To enable this feature on \u0026lt;= DBR 11.2 clusters a config must be added to the cluster. See the FAQ 9 for details on how to ensure you\u0026rsquo;re getting complete coverage for this field. Multi-Workspace Deployment (DELAYED) Allows customers to deploy, manage, and run Overwatch from a single Workspace while still collecting data from several. Provides unified global configuration via Excel / Delta tables While many customers are eagerly awaiting this feature and the dev team thought it could be released as part of this major release; unfortunately, due to some complications this wasn\u0026rsquo;t possible but should be out in the coming weeks and isn\u0026rsquo;t expected to need any sort of \u0026ldquo;upgrade\u0026rdquo;, it should be just a JAR swap. More details and full documentation coming soon. Bug Fixes Biggest fix is with regard to data quality surrounding multi-task jobs and job-clusters. This should all be resolved at this point. Full list of bug fixes for this release can be found here\n0.6.1.1 (Maintenance Release) To upgrade from 0.6.1.0, simply swap the JAR from 0.6.1.0 to this version, no upgrade script necessary\nEnhancements\nExtreme performance improvement (~75% for large workspaces) for several of the largest silver/gold modules. Increased reliability on AQE for further perf improvements Perf improvements for CLSF windows Bronze workspace snapshot helper function created to backup bronze Duplicates hunter / dropper functions enabled Bug Fixes\nFull list of bug fixes for this release can be found here 0.6.1.0 (Upgrade Release) Upgrade Process Required for existing customers on 0.6.0.x The upgrade is small and quick but is very important. Some new Databricks features have resulted in some unsafe columns (i.e. complex unbound structs) in bronze; thus, these need to be converted to map types to reduce schema cardinality. There are three tables affected and the details can be found in the upgrade script linked below.\nYou are strongly urged to upgrade DBR version to 10.4LTS (from 9.1LTS). If you do not upgrade DBR to 10.4LTS, the upgrade can be quite compute intensive due to the need to fully rebuild one of the largest tables, spark_events_bronze. A new feature called, column mapping is available in 10.4LTS that doesn\u0026rsquo;t require a full table rebuild. The caveat is that all future reads/writes to this table require 10.4LTS+. The ONLY customers not upgrading to 10.4LTS should be those with a legitimate business blocker for upgrading to 10.4LTS.\nWhen upgrading to 0.6.1 using 10.4LTS all future reads/writes to bronze table spark_events_bronze will require a cluster with DBR 10.4LTS+\n0610 Upgrade Script - 10.4LTS HTML | DBC 0610 Upgrade Script - 9.1LTS HTML | DBC For questions / issues with the upgrade, please file a git ticket\nNoteworthy Features Add Overwatch databases and contents to a workspace not running Overwatch (i.e. remote only) Ability to specify custom temporary working directories Used to default to a dir in /tmp but due to some root policies allowing it to be overridden for more details see Configs Pipeline Management simplifications Major Fixes Data Quality Enhancements Pipeline edge case stability improvements The full list of fixes can be found on the 0610 Milestone\n0.6.0.4 (Maintenance Release) Bug fixes Issue 305 - Silver_jobStatus timeout_seconds schema type Issue 308 - Optimize job could get the wrong workspace credentials in some cases Issue 310 - SparkJob table, many nulled fields Issue 313 - Silver_ClusterStateDetail - no column autoscale 0.6.0.3 (Deprecated) If an issue is found in this version, please try the latest 0.6.x version, if the issue persists, please open a git ticket\nFixes regression found in 0602 Issue 297 If you are affected, please use the script linked below to cleanup the historical data. If you\u0026rsquo;d like assistance, please file a git ticket and someone will review the process with you. CLEANUP SCRIPT (DBC | HTML) Minimum Events for Azure audit log stream ingest \u0026ndash; improvement for very busy workspaces Issue 299 Resolves issue where \u0026ldquo;init_scripts\u0026rdquo; field is missing Workspaces that have never had a cluster with an init script could hit this issue Performance Improvement for Azure Audit log ingestion Improved Schema enforcement Fix for ArrayType minimum schema requirements 0.6.0.2 \u0026ndash; DEPRECATED \u0026ndash; DONT USE Contains data quality regression - See Issue 297 Several bug fixes - mostly edge cases\nA full list of resolutions can be found here\n0.6.0.1 (MAJOR UPGRADE/RELEASE) Existing Customers on 0.5.x \u0026ndash;\u0026gt; 0.6.x UPGRADE PROCESS REQUIRED\nUpgrade process detailed in linked upgrade script HTML | DBC Notice \u0026ndash; 0.6.0 had a concurrent merge conflict for some multi-workspace deployments described in issue 284. When upgrading to 0.6 please use 0.6.0.1 as the base version, 0.6.0 shouldn\u0026rsquo;t be used.\nNoteworthy Features\nCustom workspace names Upserts - Previous versions only appended data which meant that the run / clusterState / etc had to end before the Overwatch job would pick it up. Now Overwatch utilizes delta upserts/merges so all data comes into the pipeline anchored on start_time not end_time. Numerous and Significant Schema enhancements in Gold/Consumer layer Increases resiliency and simplifies analysis New Entities in Consumer Layer InstancePools Streams (preview) \u0026ndash; Releasing as preview to get more feedback and improve structure for use cases. DBSQL - slipped \u0026ndash; DBSQL entities did not make it to gold in 0.6.0 but are staged to be released in next minor release Separated node (compute) contract costs from dbu contract costs DBU costs by sku slow-changing-dimension added (dbuCostDetails) Old instanceDetails table greatly simplified. Name is still the same, only the structure changed Enabled externalized optimize \u0026amp; z-order pipelines This was and is baked into the Overwatch process but as of 0.6.0 users can decide to externalize this. This\nis powerful as it allows the Overwatch pipeline to be more efficient and the Optimize/Zorder job to be completed from a single workspace with the appropriate cluster size/configuration for the workload. Gold Table Initializations In previous version there were often significant gaps in early runs of Overwatch as the objects that create the slow-changing-dims weren\u0026rsquo;t created/edited thus were absent from the audit logs. The APIs are now used to get an initial full picture of the workspace and that is merged with the audit logs to jump-start all records for early runs of Overwatch Error Message Improvements The error messaging pipelines have been signficantly improved to provide more useful and specific errors All tables fully externalized to paths Enables finer-grained security for multi-workspace environments 0.6.0.1 also brings numerous data quality enhancements, performance optimization, and resolves several outstanding bugs\nPlease note that pipelines may run a bit longer in some cases as we are now executing merges instead of appends. Merges are a bit more compute intensive and future optimizations are already in the works to minimize the increase. The increases were marginal but noteworthy.\nA more complete list of features and improvements can be found on the closed features/bugs list of Milestone 0.6.0\nVersion 0.5.x NOTE If youre on 0.5.x and not ready to upgrade to 0.6.0 yet, and youre not on the latest version of 0.5.x please do upgrade to the latest 0.5.x as 0.5.x will no longer be receiving feature updates and is considered stable at the latest version. No upgrade necessary, just swap the JAR to the latest version of 0.5.x and everything will work even better.\nRegarding Deprecated Versions Please note that we\u0026rsquo;re a pretty small team trying to keep up with a wide customer base. If the version you\u0026rsquo;re on shows deprecated, please note thta it doesn\u0026rsquo;t mean that we won\u0026rsquo;t try to offer assistance, just that we may first ask that you swap the jar out to the latest version of 0.5.x before we do a deep dive as many of the issues you may face are already resolved in the later version. Please do upgrade to the latest release as soon as possible.\n0.5.0.6.1 Apologies for the insanity of the version number \u0026ndash; will be better in future\nPatch for Issue_278 Databricks published some corrupted log files. These log files had duplicate column names with upper and lower case references. This was promptly resolved on the Databricks side but the logs were not retroactively created; thus, if you hit the error below using 0506 or earlier, you will need to switch to this JAR for your pipeline to continue. AnalysisException: Found duplicate column(s) in the data schema: \u0026lt;column_name\u0026gt;\n0.5.0.6 Patch for Issue_235 Edge cases resulted in nulls for several values in clusterstatefact. 0.5.0.5 - deprecated Feature Feature_223 Adds package version to parsed config in pipeline_report Patch fix for Issue_210 Patch fix for Issue_214 Important fix for Spark Users \u0026ndash; this bug resulted in several spark logs not getting ingested Patch fix for Issue_218 Potential duplicates for api call \u0026ldquo;RunNow\u0026rdquo; Patch fix for Issue_221 Multi-workspace enhancements for validations Altered DBU Costs fix Patch fix for Issue_213 Reduced plan size for jobRunCostPotentialFact table build Patch fix for Issue_192 Some spark overrides in the Overwatch pipeline were not getting applied Patch fix for Issue_206 Incorrect default prices for several node types 0.5.0.4 - deprecated Patch fix for Issue_196 AZURE - Enhancement - Enable full EH configuration to be passed through the job arguments to the main class using secrets / scopes \u0026ndash; Issue_197 0.5.0.3 - deprecated BUG FOR NEW DEPLOYMENTS When using 0.5.0.3 for a brand new deployment, a duplicate key was left in a static table causing a couple of gold layer costing tables to error out. If this is your first run, the following is a fix until 0.5.0.4 comes out with the fix.\nInitialize the Environment \u0026ndash; note this is the standard getting started process. The only difference is to stop after Bronze(workspace) and not continue. Execute through \u0026ldquo;Bronze(workspace)\u0026rdquo; DO NOT RUN THE PIPELINE (i.e. Don\u0026rsquo;t use Bronze(workspace).run()), just initialize it. Execute the following command in SQL to delete the single offending row. Remember to replace \u0026lt;overwatch_etl\u0026gt; with your configured ETL database delete from \u0026lt;overwatch_etl\u0026gt;.instancedetails where API_Name = \u0026#39;Standard_E4as_v4\u0026#39; and On_Demand_Cost_Hourly = 0.1482 Resume standard workflow Bug Fixes\nIncremental input sources returning 0 rows when users run Overwatch multiple times in the same day Incremental clusterStateFactGold \u0026ndash; addresses orphaned events for long-running states across Overwatch Runs. null driver node types for pooled drivers Improved instance pool lookup logic Mixed instance pools enabled basic refactoring for sparkOverrides \u0026ndash; full refactor in 0.5.1 Improved logging Costing - node details lookups for edge cases \u0026ndash; there was a bug in the join condition for users running Overwatch multiple times per day in the same workspace 0.5.0.2 - deprecated If upgrading from Overwatch version prior to 0.5.0 please see schema upgrade requirements\nHotfix release to resolve Issue 179. clusterstatefact_gold incremental column was start_timestamp instead of end_timestamp. Meant to roll this into 0.5.0.1 but it got missed, sorry for the double release. 0.5.0.1 - deprecated If upgrading from Overwatch version prior to 0.5.0 please see schema upgrade requirements\nHotfix release to resolve Issue 170. Hotfix to resolve laggard DF lookups required to join across pipeline runs. Caused some events to not be joined with their lagging start events in the data model. Hotfix to enable non-json formatted audit logs (AWS) primarily for PVC edge cases 0.5.0 - deprecated Upgrading and Required Changes SCHEMA UPGRADE REQUIRED - If you are upgrading from 0.4.12+ please use Upgrade.UpgradeTo0420 If upgrading from a schema prior to 0.4.12, please first upgrade to 0.4.12 and then to 0.4.2. The upgrade process will be improving or going away soon, please bear with us while we improve this process. import com.databricks.labs.overwatch.utils.Upgrade val upgradeReport = Upgrade.upgradeTo042(\u0026lt;YOUR WORKSPACE OBJECT\u0026gt;) The schema versions have been tied to the release versions until 0.5.0, this divergence was intentional. A schema version should not == the binary release version as changes to the schema should be much more infrequent than binary releases. Rest assured binary v0.5.0 requires schema version 0.4.2. Please upgrade to 0.4.2 as noted above using the 0.5.0 binary but before the pipeline is executed.\nLaunch Config Change - Note the difference in the Initializer for 0.4.2. The following change is ONLY necessary when/if instantiating Overwatch from a notebook, the main class does not require this change. Sorry for the breaking change we will try to minimize these everywhere possible // 0.4.1 (OLD) val workspace = Initializer(Array(args), debugFlag = true) // 0.4.2+ (NEW) val workspace = Initializer(args, debugFlag = true) Major Features \u0026amp; Enhancements Below are the major feature and enhancements in 0.4.2\nGetting Started Example Notebooks Kitana Testing framework, currently meant for advanced usage, documentation will roll out asap. Intelligent scaling Audit Log Formats (AWS) For AWS the input format of the audit logs is now configurable to json (default), delta, parquet Contract costs through time Changes to the instanceDetails table now allows for the tracking of all costs through time including compute and DBU. To recalcuate associated costs in the gold layer, the jobRunCostPotentialFact and clusterStateFact table will need to be rolled back as per the documents in Pipeline_Management DBU Contract Costs for DatabricksSQL and JobsLight added Costing for additional SKUs were added to the configuration such that they can be tracked. Note that as of 0.4.2 release, no calculation changes in costing as it relates to sku have yet been incorporated. These recalculations for jobs light and DatabricksSQL are in progress. Enabled Main class execution to execute only a single layer of the pipeline such as bronze / silver / gold. Primarily enabled for future enablement with jobs pipelines and for development / testing purposes. User can now pass 2 arguments to the databricks job where the first is \u0026lsquo;bronze\u0026rsquo;, \u0026lsquo;silver\u0026rsquo;, or \u0026lsquo;gold\u0026rsquo; and the second is the escaped configuration string and only that pipeline layer will execute. Event Hub Checkpoint - Auto-Recovery (Azure) Previously, if an EH state was to get lost or corrupted, it was very challenging to resume. Overwatch will now automatically identify the latest state loaded and resume. Support non-dbfs file types such as abfss://, s3a://, s3n:// etc. for audit log source paths and output paths. All file types supported on Databricks should now be supported in Overwatch. Minor Bug Fixes / Enhancements Unsafe SSL allowed Issue 152. All API connections will first be attempted using standard SSL but will fallback on unsafe if there is a SSLHandshakeException. Targeted at specific scenarios. ERD Updates for accuracy Issue resolved where not all cluster logs were being loaded thus several spark log events were missing Various schema errors in edge use cases Docs updates and ehancements Additional Fixes non-dbfs uris for cluster logs (i.e. s3a://, abfss://) intelligent scaling calculation improved errors for access issues with api calls unsafeSSL fallback optimization and bug fix elementType implicit casting to target schemas within arrays aws - non-json formatted audit logs schema unification api schema scrubber improvements for addtional edge cases 0.4.2 - deprecated Deprecated release - please use 0.5.0 The 0.4.2 can be viewed as an RC for 0.5.0. The 0.5.0 release is the 0.4.2 release with several bug fixes 0.4.13 - deprecated Hotfix for Issue 138 An upgrade to this minor release is only necessary if you\u0026rsquo;re experience api limits and/or seeing 429 issues. Otherwise, this release can be skipped as the fix will be in 0.4.2+. New users should use this version until the next release is published.\n0.4.12 - deprecated SCHEMA UPGRADE REQUIRED\nHotfix for Issue 126. Hotfix for Issue 129. Schema upgrade capability enabled. Please follow the upgrade documentation to complete your upgrade Upgrade function name == Upgrade.upgradeTo0412 Corrected case sensitivity issue. When failure would occur in spark_events_bronze, there were column references to state tables with mismatched column name case sensitivity causing errors Corrected issue with missing quotes around update statement to track failed files in spark_events_processedFiles 0.4.11 - deprecated Hotfix for Issue 119. Issue was only present in edge cases. Edge cases include workspaces missing data and attempting to run modules for which data didn\u0026rsquo;t exist. Added additional guard rails to pipeline state checks to help clarify workspace / pipeline state issues for future users. 0.4.1 - deprecated Converted all delta targets from managed tables to external tables Better security and best practice Enabled etl data path prefix to simplify and protect the raw data across workspaces pipeline_report - enabled partitioning by \u0026ldquo;organization_id\u0026rdquo; Necessary to guard against write concurrency issues across multiple workspaces orgianization_id - Single Tenant AWS workspaces with DNS would return \u0026ldquo;0\u0026rdquo; for organization_id, updated to change organization_id to the workspace prefix. EXAMPLE: https://myorg_dev.cloud.databricks.com \u0026ndash;\u0026gt; myorg_dev instanceDetails - Converted from append mode to overwrite mode. Allows for multiple workspaces to create the compute pricing lookup if it didn\u0026rsquo;t already exist for the workspace Moved default table location from consumerDB to ETLDB and created mapped view to table in consumerDB Simplified the instanceDetails cost customization process (docs updated) Added proper filter to ensure ONLY current workspace compute costs are looked up during joins Converted Azure example Memory_GB to double to match data type from AWS Bug fix for JSON to Map that caused certain escaped json strings to break when being pulled from the new Jobs UI Bug fix for Issue 111 Bug fix for multi-workspace Azure, filter from audit_raw_land table was incomplete Bug fix to allow for consumerDB to be omitted from DataTarget in config 0.4.0 - deprecated Initial Public Release\n"
},
{
	"uri": "http://localhost:1313/overwatch/deployoverwatch/ucedeployment/",
	"title": "Deploying To Unity Catalog",
	"tags": [],
	"description": "",
	"content": " At this time Overwatch only supports Unity Catalog External Tables. Overwatch was designed to operate with external tables and this strategy has been mirrored to the UC deployments as well. We\u0026rsquo;re investigating options for enabling Overwatch on UC Managed Tables.\nDeploying Overwatch to a Unity Catalog is nearly exactly the same as deploying to a Hive Metastore with a few additional pre-requisites to configure auth for UC underlying storage, External Locations, Catalogs, and Schemas.\nThis guide assumes the reader has an understanding of Unity Catalog, Storage Credentials, and External Locations. For more details on how these components work together to provide fine-grained ACLs to data and storage please see the Databricks Docs for Unity Catalog\nUC Deployment Specific Differences As mentioned before the deployment process is nearly identical to deploying on hive metastore so all the docs in the Deployment Guide are very relevant and should guide your deployment with the specific deviations referenced below and detailed out in this section of the docs.\nStorage Requirements UC Pre-Requisites Configuration Details Migrating to UC from Hive Metastore If you are currently on 0720+ in Hive Metastore and would like to migrate your deployment to UC, the process and script for doing that can be found in the Migrating To UC section.\n"
},
{
	"uri": "http://localhost:1313/overwatch/dataengineer/productionizing/",
	"title": "Productionizing",
	"tags": [],
	"description": "",
	"content": "Moving To Production When you\u0026rsquo;re ready to move to production, there are a few things to keep in mind and best practices to follow to get the most out of Overwatch\nCluster Logging Simplify and Unify your cluster logging directories\nMany users forget to enable cluster logging and without it Overwatch cannot provide usage telemetry by notebook, job, user so it\u0026rsquo;s critical that all clusters have clusters logs enabled If users are allowed to create clusters/jobs without any governance, log files will be produced and stored all over the place. These will be very challenging to clean up and can eat up a significant amount of storage over time. Utilize cluster policies to ensure the logging location is always set and done so consistently Set up a lifecycle policy (i.e. TTL) on your cluster logging directory in your cloud storage so the logs don\u0026rsquo;t pile up indefinitely. Suggested time to live time is 30 days. Backups (This will be Deprecated from version 7.2.1.Please refer to the Snapshot section for the new way to do backups) Perform Bronze Backups I know we don\u0026rsquo;t hear a lot about backups in big data world but often times the cluster logs and / or the audit logs are transient (especially Azure deployments as Event Hub only maintains 7 days). This means that if something happened to the bronze data your Overwatch history could be lost forever. To guard against this it\u0026rsquo;s strongly recommended that you periodically backup the Bronze data. As of 0.6.1.1 this has been made very easy through the snapshot helper function.\nNow just schedule a notebook like this to run once a week and you\u0026rsquo;ll always have at 1 week of backups\nimport com.databricks.labs.overwatch.pipeline.Bronze import com.databricks.labs.overwatch.utils.Helpers val workspace = Helpers.getWorkspaceByDatabase(\u0026#34;overwatch_etl\u0026#34;) // can be used after pipeline is running successfully // alternative method for getting a workspace is // import com.databricks.labs.overwatch.pipeline.Initializer // val workspace = Initializer(\u0026#34;\u0026#34;\u0026#34;\u0026lt;compact config string\u0026gt;\u0026#34;\u0026#34;\u0026#34;) // simple method Bronze(workspace).snapshot(\u0026#34;/path/to/my/backups/sourceFolder\u0026#34;, true) // alternative method with more customized configs Bronze(workspace).snapshot( targetPrefix = \u0026#34;/path/to/my/backups/sourceFolder\u0026#34;, overwrite = true, excludes = Array(\u0026#34;audit_log_raw_events\u0026#34;, \u0026#34;clusters_snapshot_bronze\u0026#34;) // not recommended but here for example purposes ) Snapshot Process Snapshoting in Big Data World is very rare but often times the cluster logs and / or the audit logs are transient (especially Azure deployments as Event Hub only maintains 7 days). This means that if something happened to the bronze data your Overwatch history could be lost forever. To guard against this you now have the option to periodically snapshot the Overwatch data, as of version 0.7.2.1\nCurrently, There are two types of Snapshot process available in Overwatch: Full or Incremental:\nFull Snapshot This is normal Snapshot process where we take the backup of ETL Database in Batch method. Here we can do Deep Cloning of existing ETL database tables to target Snapshot RootPath.\nIncremental Snapshot Through Incremental_Snap we take the snapshot of the ETL tables incrementally i.e. through streaming operation.\nThis process will require us to maintain a checkpoint directory. Below is the screenshot of the snap location after snapshot process is done\n\u0026ldquo;snapshotRootPath/data\u0026rdquo; - Contains the Snapshot Data of Source Database Tables. \u0026ldquo;snapshotRootPath/clone_report\u0026rdquo; - Contains the data containing run metrics. \u0026ldquo;snapshotRootPath/checkpoint\u0026rdquo; - Checkpoint Location for Streaming Operation. How to run the Snapshot Process You can run the Snapshot process via a scheduled job, or a notebook.\nSnapshot through Databricks Notebook The code snippet to run the Snapshot process is as below:\nimport com.databricks.labs.overwatch.pipeline.Snapshot val sourceETLDB = \u0026#34;ow_bronze_snapshot_etl\u0026#34; val targetPrefix = \u0026#34;/mnt/overwatch_playground/721-snap/incremental\u0026#34; val pipeline = \u0026#34;Bronze,Silver,Gold\u0026#34; val snapshotType = \u0026#34;Incremental\u0026#34; val tablesToExclude = \u0026#34;notebook_silver,notebook_gold\u0026#34; Snapshot.process(sourceETLDB,targetPrefix,snapshotType,pipeline,tablesToExclude) We need below parameters to run the snapshot process through Databricks Notebook:\nParam Type Optional Default Value Description sourceETLDB String No NA Source Database Name for which Snapshot need to be done(note you should only snapshot your Overwatch etl database). targetPrefix String No NA Target path where Snapshot data will be saved to. snapshotType String No NA Type of Snapshot to be performed. \u0026ldquo;Full\u0026rdquo; for Full Snapshot, \u0026ldquo;Incremental\u0026rdquo; for Incremental Snapshot pipeline String Yes \u0026ldquo;Bronze,Silver,Gold\u0026rdquo; Define the Medallion Layers. Argument should be in form of \u0026ldquo;Bronze, Silver, Gold\u0026rdquo;(All 3 or any combination of them) cloneLevel String Yes \u0026ldquo;DEEP\u0026rdquo; Clone Level for Snapshot. By Default it is \u0026ldquo;DEEP\u0026rdquo;. You can also specify \u0026ldquo;SHALLOW\u0026rdquo; Clone. You can get more details on cloning here tablesToExclude String Yes \u0026quot;\u0026quot; Array of table names to exclude from the snapshot. Tables should be separated with \u0026ldquo;,\u0026rdquo;. This is the table name only - without the database prefix. The snapshot data will be in the \u0026ldquo;targetPrefix/data\u0026rdquo; path as shown below:\nIf you look into the targetPrefix folder you will find another subdirectory called \u0026ldquo;clone_report\u0026rdquo;. This folder contains the data regarding the status of your snapshot process for each table:\nSnapshot through Databricks Job You can also schedule a job to either run the notebook above, or use the SnapshotRunner class. For example:\nType - Jar Main class - com.databricks.labs.overwatch.pipeline.SnapshotRunner Parameters for the Job - Same as mentioned above for snapshot-through-databricks-notebook Validation Functionality This is a special module in Snapshot Process. Before running your Snapshot, you can validate that your configuration is correct by calling the Snapshot.isValid function. This function is also used internally when the Snapshot is kicked off.\nBelow is the code snippet for the validation function\nimport com.databricks.labs.overwatch.pipeline.Snapshot val sourceETLDB = \u0026#34;ow_bronze_snapshot_etl\u0026#34; val pipeline = \u0026#34;Bronze,Silver,Gold\u0026#34; val snapshotType = \u0026#34;Incremental\u0026#34; Snapshot.isValid(sourceETLDB,snapshotType,pipeline) This function will return true if the configuration is valid, and will also output relevant info for the fields checked. For example:\now_bronze_snapshot_etl is Overwatch Database and suitable for Snapshot. Snapshot Type is Suitable for Snapshot Process. Provided SnapshotType value is Incremental. cloneLevel Type is Suitable for Snapshot Process. Provided cloneLevel value is DEEP. Validation successful.You Can proceed with Snapshot process import com.databricks.labs.overwatch.pipeline.Snapshot res14: Boolean = true Restore Process You can use the Restore process to bring back snapshot data Similar to Snapshot, Restore can be run by both Databricks Job and Databricks Notebook.\nRestore through Databricks Notebook The code snippet to run the Restore process is as below:\nimport com.databricks.labs.overwatch.pipeline.Restore val sourcePrefix = \u0026#34;/mnt/overwatch_playground/721-snap/full\u0026#34; val targetPrefix = \u0026#34;/mnt/overwatch_playground/721-snap/restore\u0026#34; Restore.process(sourcePrefix,targetPrefix) We need below parameters to run the Restore process through Databricks Notebook:\nParam Type Optional Description sourcePrefix String No Source ETL Path Prefix from where restore need to be performed. targetPrefix String No Target ETL Path Prefix where restore data will be loaded to. After Restore process is done the tables from sourcePrefix will be restored in the targetPrefix/globalshare location Restore through Databricks Job In cases where you need to periodically run the Restore process, you can configure a job using the RestoreRunner class.\nType - Jar Main class - com.databricks.labs.overwatch.pipeline.RestoreRunner Parameters for the Job - Same as mentioned above for Restore through Databricks Notebook Validation Functionality Before running Restore, we recommend that you validate that your configuration is correct by calling the Restore.isValid function. This function is also used internally when the Restore process is kicked off\nBelow is the code snippet for the validation function\nimport com.databricks.labs.overwatch.pipeline.Restore val sourcePrefix = \u0026#34;/mnt/overwatch_playground/721-snap/full\u0026#34; val targetPrefix = \u0026#34;/mnt/overwatch_playground/721-snap/restore\u0026#34; Restore.isValid(sourcePrefix,targetPrefix) This function will return true if the configuration is valid, and will also output relevant info for the fields checked. For example:\nSourcePrefix Path Exists. Target Path /mnt/overwatch_playground/721-snap/restore is Empty and is Suitable for Restore Process Validation successful.You Can proceed with Restoration process import com.databricks.labs.overwatch.pipeline.Restore res43: Boolean = true Migration Process There are cases when you need to move your data altogether. During the Migration process, we will move your data to the new location, update the configuration to point to this new location, as well as delete the old database if everything looks good. These the steps required to Migrate your Overwatch data:\nStop Overwatch jobs (Need to be done by User) Run the Migration process (Use Migrate.process) Resume Overwatch jobs (Need to be done by User) Migration through Databricks Notebook The code snippet to run the Migration process is as below:\nimport com.databricks.labs.overwatch.pipeline.Migration val sourceETLDB = \u0026#34;ow_bronze_migration_etl\u0026#34; val migrateRootPath = \u0026#34;/mnt/overwatch_playground/721-snap/migration\u0026#34; val configPath = \u0026#34;abfss://overwatch-field-playground@overwatchglobalinternal.dfs.core.windows.net/sourav/configs/721-migration\u0026#34; val tablesToExclude = \u0026#34;notebook_silver:notebook_gold\u0026#34; Migration.process(sourceETLDB,migrateRootPath,configPath,tablesToExclude) We need below parameters to run the Migration process through Databricks Notebook:\nParam Type Optional Description sourceETLDB String No Source Database Name to migrate. Note that you can only use the Overwatch ETL database. migrateRootPath String No Target path to where migration need to be performed. configPath String No Configuration Path where the config file for the source is present. Path can be to a CSV file or path to a delta table, or simple a delta table name that contains your configuration. tablesToExclude String Yes Array of table names to exclude from the migration. This is the table name only - without the database prefix. By Default it is empty. After Migration process is done the tables from sourcePrefix will be restored in the migrateRootPath/globalshare location Migration through Databricks Job Below is the Configuration for the Databricks Job\nType - Jar Main class - com.databricks.labs.overwatch.pipeline.MigrationRunner Parameters for the Job Same as mentioned above for Migration through Databricks Notebook Validation Functionality Before running the Migration, we recommend that you validate that your configuration is correct by calling the Migration.isValid function. This function is also used internally when the Migration process is kicked off.\nBelow is the code snippet for the validation function\nimport com.databricks.labs.overwatch.pipeline.Migration val sourceETLDB = \u0026#34;ow_bronze_migration_etl\u0026#34; val configPath = \u0026#34;abfss://overwatch-field-playground@overwatchglobalinternal.dfs.core.windows.net/sourav/configs/721-migration\u0026#34; Migration.isValid(sourceETLDB,configPath) This function will return true if the configuration is valid, and will also output relevant info for the fields checked. For example\now_bronze_migration_etl is Overwatch Database and suitable for Migration Config file is properly configured and suitable for Migration Config source: delta path abfss://overwatch-field-playground@overwatchglobalinternal.dfs.core.windows.net/sourav/configs/721-migration Validation successful.You Can proceed with Migration process import com.databricks.labs.overwatch.pipeline.Migration res13: Boolean = true Alerting On Failures Overwatch modules are designed to fail softly. This means that if your silver jobs module fails the job will still succeed but the silver jobs module will not progress and neither will any downstream modules that depend on it. To be alerted when a specific module fails you need to configure a DBSQL alert to monitor the pipReport output and fire an alert when conditions are met.\nHow To Set Up Module Level Alerting Documentation in progress \u0026ndash; publishing soon\nEnsure Externalize Optimize is Enabled To ensure Overwatch is efficient it\u0026rsquo;s important to remove the optimize and z-order steps from the integrated pipeline. To do this, follow the instructions for Externalizing Optimize \u0026amp; Z-Order\n"
},
{
	"uri": "http://localhost:1313/overwatch/faq/",
	"title": "FAQ",
	"tags": [],
	"description": "",
	"content": "Using the FAQs The answers and examples to the questions below assume that a workspace is instantiated and ready to use. There are many ways to instantiate a workspace, below is a simple way but it doesn\u0026rsquo;t matter how you create the Workspace, just so long as the state of the workspace and/or pipeline can be referenced and utilized. A simple example is below. There are much more verbose details in the Advanced Topics Section if you\u0026rsquo;d like a deeper explanation.\nimport com.databricks.labs.overwatch.utils.Upgrade import com.databricks.labs.overwatch.pipeline.Initializer val params = OverwatchParams(...) val prodArgs = JsonUtils.objToJson(params).compactString // A more verbose example is available in the example notebooks referenced above val prodWorkspace = Initializer(prodArgs, debugFlag = true) val upgradeReport = Upgrade.upgradeTo042(prodWorkspace) display(upgradeReport) Q1: How To Clean Re-Deploy I just deployed Overwatch and made a mistake and want to clean up everything and re-deploy. I can\u0026rsquo;t seem to get a clean state from which to begin though.\nOverwatch tables are intentionally created as external tables. When the etlDataPathPrefix is configured (as recommended) the target data does not live underneath the database directory and as such is not deleted when the database is dropped. This is considered an \u0026ldquo;external table\u0026rdquo; and this is done by design to protect the org from someone accidentally dropping all data globally. For a FULL cleanup you may use the functions below but these are destructive, be sure that you review it and fully understand it before you run it.\nCRITICAL: CHOOSE THE CORRECT FUNCTION FOR YOUR ENVIRONMENT If Overwatch is configured on multiple workspaces, use the multi-workspace script otherwise, use the single workspace functions. Please do not remove the safeguards as they are there to protect you and your company from accidents.\nSINGLE WORKSPACE DEPLOYMENT DESTROY AND REBUILD\nsingle_workspace_cleanup.dbc | single_workspace_cleanup.html\nMULTI WORKSPACE DEPLOYMENT DESTROY AND REBUILD\nFirst off, don\u0026rsquo;t practice deploy a workspace to the global Overwatch data target as it\u0026rsquo;s challenging to clean-up; nonetheless, the script below will remove the data from this workspace from the global dataset and reset this workspace for a clean run.\nmulti_workspace_cleanup.dbc | multi_workspace_cleanup.html\nQ2: The incorrect costs were entered for a specific time range, I\u0026rsquo;d like to correct them, how should I do that? Please refer to the Configuring Custom Costs to baseline your understanding. Also, closely review the details of the instanceDetails table definition as that\u0026rsquo;s where costs are stored and this is what will need to be accurate. Lastly, please refer to the Pipeline_Management section of the docs as you\u0026rsquo;ll likely need to understand how to move the pipeline through time.\nNow that you have a good handle on the costs and the process for restoring / reloading data, note that there are ultimately two options after you reconfigure the instancedetails table to be accurate. There are two tables that are going to have inaccurate data in them:\njobruncostpotentialfact_gold clusterstatefact Options\nCompletely drop tables, delete their underlying data and rebuild them from the primordial date Delete all records where Pipeline_SnapTS \u0026gt;= the time where the data became inaccurate. Both options will require a corresponding appropriate update to the pipeline_report table for the two modules that write to those tables (3005, 3015). Here are several examples on ways to update the pipeline_report to change update the state of a module.\nQ3: What is the current guidance / state of customers who want to capture EC2 spot pricing? The easiest way is to identify an \u0026ldquo;average\u0026rdquo; SPOT price and apply it to all compute cost calcuations. Going a bit further, calculate average spot price by region, average spot price by region by node type and lastly down to a time resolution like hour of day. Use the AWS APIs to get the historical spot price at by these dims.This can be automated via a little script they build but it require proper access to the AWS account.\ndisplay( spark.table(\u0026#34;overwatch.clusterstatefact\u0026#34;) .withColumn(\u0026#34;compute_cost\u0026#34;, applySpotDiscount(\u0026#39;awsRegion, \u0026#39;nodeTypeID) ) where applySpotDiscount is the lookup function that either hits a static table of estimates or a live API endpoint from AWS.\nIt\u0026rsquo;s a very simple API call,just need the token to do it.It\u0026rsquo;s extremely powerful for shaping/optimizing job run timing, node types, AZs, etc\nSample Request:\n\u0026amp;StartTime=2020-11-01T00:00:00.000Z \u0026amp;EndTime=2020-11-01T23:59:59.000Z \u0026amp;AvailabilityZone=us-west-2a \u0026amp;AUTHPARAMS Sample Response:\n\u0026lt;requestId\u0026gt;59dbff89-35bd-4eac-99ed-be587EXAMPLE\u0026lt;/requestId\u0026gt; \u0026lt;spotPriceHistorySet\u0026gt; \u0026lt;item\u0026gt; \u0026lt;instanceType\u0026gt;m3.medium\u0026lt;/instanceType\u0026gt; \u0026lt;productDescription\u0026gt;Linux/UNIX\u0026lt;/productDescription\u0026gt; \u0026lt;spotPrice\u0026gt;0.287\u0026lt;/spotPrice\u0026gt; \u0026lt;timestamp\u0026gt;2020-11-01T20:56:05.000Z\u0026lt;/timestamp\u0026gt; \u0026lt;availabilityZone\u0026gt;us-west-2a\u0026lt;/availabilityZone\u0026gt; \u0026lt;/item\u0026gt; \u0026lt;item\u0026gt; \u0026lt;instanceType\u0026gt;m3.medium\u0026lt;/instanceType\u0026gt; \u0026lt;productDescription\u0026gt;Windows\u0026lt;/productDescription\u0026gt; \u0026lt;spotPrice\u0026gt;0.033\u0026lt;/spotPrice\u0026gt; \u0026lt;timestamp\u0026gt;2020-11-01T22:33:47.000Z\u0026lt;/timestamp\u0026gt; \u0026lt;availabilityZone\u0026gt;us-west-2a\u0026lt;/availabilityZone\u0026gt; \u0026lt;/item\u0026gt; \u0026lt;/spotPriceHistorySet\u0026gt; \u0026lt;nextToken/\u0026gt; \u0026lt;/DescribeSpotPriceHistoryResponse\u0026gt; More Details: https://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_DescribeSpotPriceHistory.html\nQ4: How to config the cluster logs path? The cluster logging paths are automatically acquired within Overwatch. There\u0026rsquo;s no need to tell Overwatch where to load those from. Click here for more details. Q5: Can I configure custom log4j appender settings Yes, Overwatch does not ingest from log4j output files; thus this can be edited as per customer needs without any impact on Overwatch. Overwatch utilizes the spark event logs from the cluster logs direction, not the stdout, stderr, or the log4j.\nQ6: I\u0026rsquo;d like to add or update workspace_name on my Overwatch deployment, is there an easy way to do that? Yes, follow the instructions in the script posted below. DBC | HTML\nQ7: I\u0026rsquo;d like to have cluster policies to ensure all future interactive and automated clusters have an appropriate logging location Use the Databricks Docs on Cluster Policies to define appropriate cluster policies as per your requirements. To enforce logging location on interactive, automated, or both use the examples below and apply them to your cluster policies appropriately.\nInteractive Cluster Policy Only Automated Cluster Policy Only Interactive AND Automated Cluster Policy Q8: Can I deploy Overwatch on Unity Catalog (UC) Yes!\nAs of v0.7.2.0 You can deploy Overwatch on your UC-enabled workspace. We have also provided a step-by-step migration guide to help you migrate your existing Overwatch data.\nhint: On top of the added security benefits of UC, deploying Overwatch onto it will significantly improve the deployment process for Azure customers in the future, since Overwatch will soon leverage System Tables as the bronze layer source of data when applicable.\nQ9: How to enable user-email tracking in sparkJob Table for DBR versions \u0026lt; 11.3 Until DBR 11.3+ the user_email field in sparkJob table was incomplete as Databricks did not begin publishing this in all cases until that version. To enable event log reporting for user_email in DBR versions \u0026lt; 11.3, set the following command to true in the cluster\u0026rsquo;s spark config. This can be done in the cluster config or through an init script. Any clusters running on DBR \u0026lt; 11.3 without this config will not report user_email for Python and R interactive workloads. spark.databricks.driver.enableUserContextForPythonAndRCommands\nTo enable this globally, it\u0026rsquo;s recommended to utilize cluster policies or global_init scripts.\nIf you set this property and someone in your org depends on one of these fields for their workflow this will overwrite their value. They should not be doing this and if they are they need to stop before 11.3 as this will be part of DBR by then. For example if a dev team is using something like \u0026ldquo;if not sc.getLocalProperty(\u0026ldquo;user\u0026rdquo;): doThing()\u0026rdquo; then this will now be set so their code would break.\nQ10: My view is giving me some strange error when I try to select from it, how do I fix it? Occasionally the views have an issue with their refresh and get stuck. If you\u0026rsquo;re having a strange error with your view, try the following code snipped to try and repair it.\nimport com.databricks.labs.overwatch.pipeline.Gold import com.databricks.labs.overwatch.utils.Helpers val myETLDB = \u0026#34;overwatch_etl\u0026#34; // CHANGE ME val nameOfMisbehavingView = \u0026#34;jobrun\u0026#34; // CHANGE ME spark.sql(s\u0026#34;drop view ${myETLDB}.${nameOfMisbehavingView}\u0026#34;) val workspace = Helpers.getWorkspaceByDatabase(myETLDB) val gold = Gold(workspace) gold.refreshViews() Q11: An API call is resulting in Job Aborted what can I do This can be due to a large resulting dataset. To validate download the appropriate logs and search for spark.rpc.message.maxSize. If you don\u0026rsquo;t find skip ahead, if you do find it, add the following parameter to the spark config of the Overwatch cluster. spark.rpc.message.maxSize 1024.\nIf the issue wasn\u0026rsquo;t the RPC size reduce the size of the \u0026ldquo;SuccessBatchSize\u0026rdquo; in the APIENV configuration (as of 0.7.0)\n"
},
{
	"uri": "http://localhost:1313/overwatch/dataengineer/advancedtopics/",
	"title": "AdvancedTopics",
	"tags": [],
	"description": "",
	"content": "Quick Reference Externalize Optimize \u0026amp; Z-Order Interacting With Overwatch and its State Optimizing Overwatch Maximizing First Run Potential Historical Loads Cluster Logs Ingest Details Joining With Slow Changing Dimensions (SCD) Optimizing Overwatch Expectation Check Note that Overwatch analyzes nearly all aspects of the Workspace and manages its own pipeline among many other tasks. This results in 1000s of spark job executions and as such, the Overwatch job will take some time to run. For small/medium workspaces, 20-40 minutes should be expected for each run. Larger workspaces can take longer, depending on the size of the cluster, the Overwatch configuration, and the workspace.\nThe optimization tactics discussed below are aimed at the \u0026ldquo;large workspace\u0026rdquo;; however, many can be applied to small / medium workspaces.\nLarge Workspace is generally defined \u0026ldquo;large\u0026rdquo; due to one or more of the following factors\n250+ clusters created / day \u0026ndash; common with external orchestrators such as airflow, when pools are heavily used, and many other reasons All clusters logging and 5000+ compute hours / day Optimization Limitations (Bronze) Note that there are two modules that simply cannot be linearly parallelized for reasons beyond the scope of this write-up. These limiting factors are only present in Bronze and thus one optimization is to utilize one cluster spec for bronze and another for silver/gold. To do this, utilize Databricks\u0026rsquo; multi-task jobs feature to run into three steps and specify two cluster. To split the Overwatch Pipeline into bronze/silver/gold steps refer to the jar or notebook deployment method, as pertintent\nBronze - small static (no autoscaling) cluster to trickle load slow modules Silver/Gold - slightly larger (autoscaling enabled as needed) - cluster to blaze through the modules as silver and gold modules are nearly linearly scalable. Overwatch developers will continue to strive for linear scalability in silver/gold. Below is a screenshot illustrating the job definition when broken apart into a multi-task job. Utilize External Optimize Externalizing the optimize and z-order functions is critical for reducing daily Overwatch runtimes and increasing efficiency of the optimize and z-order functions. Optimize \u0026amp; z-order are more efficient the larger the dataset and the larger the number of partitions to be optimized. Furthermore, optimize functions are very parallelizable and thus should be run on a larger, autoscaling cluster. Lastly, by externalizing the optimize functions, this task only need be carried out on a single workspace per Overwatch output target (i.e. storage prefix target).\nAdditional Optimization Considerations As of 0.7.0 Use DBR 11.3 LTS Photon can significantly improve runtimes. Mileage will vary by customer depending on data sizes and skews of certain modules, so feel free to turn it on and use Overwatch to determine whether Photon is the right answer for your needs. As of 0.6.1 disable Photon Photon will be the focus of optimization in DBR 11.xLTS+ 0.6.1+ Use DBR 10.4 LTS \u0026lt; 0.6.1 Use DBR 9.1 LTS This is critical as there are some regressions in 10.4LTS that have been handled in Overwatch 0.6.1 but not in previous versions (Azure) Follow recommendations for Event Hub. If using less than 32 partitions, increase this immediately. Ensure direct networking routes are used between storage and compute and that they are in the same region Overwatch target can be in separate region for multi-region architectures but sources should be co-located with compute. Using External Metastores If you\u0026rsquo;re using an external metastore for your workspaces, pay close attention to the Database Paths configured in the DataTarget. Notice that the workspaceID has been removed for the external metastore in the db paths. This is because when using internal metastores and creating databases in multiple workspaces you are actually creating different instances of databases with the same name; conversely, when you use an external metastore, the database truly is the same and will already exist in subsequent workspaces. This is why we remove the workspaceID from the path so that the one instance of the database truly only has one path, not one per workspace.\n// DEFAULT private val dataTarget = DataTarget( Some(etlDB), Some(s\u0026#34;${storagePrefix}/${workspaceID}/${etlDB}.db\u0026#34;), Some(s\u0026#34;${storagePrefix}/global_share\u0026#34;), Some(consumerDB), Some(s\u0026#34;${storagePrefix}/${workspaceID}/${consumerDB}.db\u0026#34;) ) // EXTERNAL METASTORE private val dataTarget = DataTarget( Some(etlDB), Some(s\u0026#34;${storagePrefix}/${etlDB}.db\u0026#34;), Some(s\u0026#34;${storagePrefix}/global_share\u0026#34;), Some(consumerDB), Some(s\u0026#34;${storagePrefix}/${consumerDB}.db\u0026#34;) ) Externalize Optimize \u0026amp; Z-Order (as of 0.6.0) The Overwatch pipeline has always included the Optimize \u0026amp; Z-Order functions. By default all targets are set to be optimized once per week which resulted in very different runtimes once per week. As of 0.6.0, this can be removed from the Overwatch Pipeline (i.e. externalized). Reasons to externalize include:\nLower job runtime variability and/or improve consistency to hit SLAs Utilize a more efficient cluster type/size for the optimize job Optimize \u0026amp; Zorder are extremely efficient but do heavily depend on local storage; thus Storage Optimized compute nodes are highly recommended for this type of workload. Azure: Standard_L*s series AWS: i3.*xLarge Allow a single job to efficiently optimize Overwatch for all workspaces Note that the scope of \u0026ldquo;all workspaces\u0026rdquo; above is within the scope of a data target. All workspaces that send data to the same dataset can be optimized from a single workspace. To externalize the optimize and z-order tasks:\nSet externalizeOptimize to true in the runner notebook config (All Workspaces) If running as a job don\u0026rsquo;t forget to update the job parameters with the new escaped JSON config string Create a job to execute the optimize on the workspace you wish (One Workspace - One Job) Type: JAR Main Class: com.databricks.labs.overwatch.Optimizer Dependent Libraries: com.databricks.labs:overwatch_2.12:\u0026lt;LATEST\u0026gt; Parameters: [\u0026quot;\u0026lt;Overwatch_etl_database_name\u0026gt;\u0026quot;] Cluster Config: Recommended DBR Version: 11.3LTS Photon: Optional Enable Autoscaling Compute with enough nodes to go as fast as you want. 4-8 is a good starting point AWS \u0026ndash; Enable autoscaling local storage Worker Nodes: Storage Optimize Driver Node: General Purpose with 16+ cores The driver gets very busy on these workloads - recommend 16 - 64 cores depending on the max size of the cluster Interacting With Overwatch and its State Use your production notebook (or equivallent) to instantiate your Overatch Configs. From there use JsonUtils.compactString to get the condensed parameters for your workspace.\nNOTE you cannot use the escaped string, it needs to be the compactString.\nBelow is an example of getting the compact string. You may also refer to the example runner notebook for your cloud provider.\nval params = OverwatchParams(...) print(JsonUtils.objToJson(params).compactString) Instantiating the Workspace As Of 0.6.0.4 Utilize Overwatch Helper functions get the workspace object for you. The workspace object returned will be the Overwatch config and state of the current workspace at the time of the last successful run. As of 0.6.1 this helper function is not expected to work across versions, meaning that if the last run was on 0.6.0.4 and the 0.6.1 JAR is attached, don\u0026rsquo;t expect this method to work\nimport com.databricks.labs.overwatch.utils.Helpers val prodWorkspace = Helpers.getWorkspaceByDatabase(\u0026#34;overwatch_etl\u0026#34;) // for olderVersions use the compact String // import com.databricks.labs.overwatch.pipeline.Initializer // Get the compactString from the runner notebook you used for your first run example BE SURE your configs are the same as they are in prod // val workspace = Initializer(\u0026#34;\u0026#34;\u0026#34;\u0026lt;compact config string\u0026gt;\u0026#34;\u0026#34;\u0026#34;) Using the Workspace Now that you have the workspace you can interact with Overwatch very intuitively.\nExploring the Config prodWorkspace.getConfig.* After the last period in the command above, push tab and you will be able to see the entire derived configuratin and it\u0026rsquo;s state.\nInteracting with the Pipeline When you instantiate a pipeline you can get stateful Dataframes, modules, and their configs such as the timestamp from which a DF will resume, etc.\nval bronzePipeline = Bonze(prodWorkspace) val silverPipeline = Silver(prodWorkspace) val goldPipeline = Gold(prodWorkspace) bronzePipeline.* Instantiating a pipeline gives you access to its public methods. Doing this allows you to navigate the pipeline and its state very naturally\nUsing Targets A \u0026ldquo;target\u0026rdquo; is essentially an Overwatch-defined table with a ton of helper methods and attributes. The attributes are closed off in 0.5.0 but Issue 164 will expose all reasonably helpful attributes making the target definition even more powerful.\nNote that the scala filter method below will be simplified in upcoming release, referenced in Issue 166\nval bronzeTargets = bronzePipeline.getAllTargets val auditLogTarget = bronzeTargets.filter(_.name === \u0026#34;audit_log_bronze\u0026#34;) // the name of a target is the name of the etl table // a workspace level dataframe of the table. It\u0026#39;s workspace-level because \u0026#34;global filters\u0026#34; are automatically // applied by defaults which includes your workspace id, thus this will return the dataframe of the audit logs // for this workspace val auditLogWorkspaceDF = auditLogTarget.asDF() // If you want to get the global dataframe val auditLogGlobalDF = auditLogTarget.asDF(withGlobalFilters = false) Getting a Strong First Run The first run is likely the most important as it initializes all the slow-changing dimensions and sets the stage for subsequent runs. Follow the steps below to get a strong start.\nTest First Let\u0026rsquo;s test before trying to dive right into loading a year+ of historical data. Set the primordial date to today - 5 days and run the pipeline. Then review the pipReport:\ndisplay( table(\u0026#34;overwatch_etl.pipReport\u0026#34;) .orderBy(\u0026#39;Pipeline_SnapTS.desc) ) and validate that all the modules are operating as expected. Once you\u0026rsquo;re certain all the modules are executing properly, run a full cleanup and update the parameters to load historical data and begin your production deployment.\nHistorical Loading Recommended max historical load is 30 days on AWS and 7 days on Azure.\nDISABLE SPOT INSTANCES \u0026ndash; when loading historical data, the last thing you want is to get nearly complete loading non-splittable sources just to have the node killed to spot instance termination and have to restart the process. It\u0026rsquo;s likely more expensive to try and complete your first run with spot instances enabled. This may also be true in production daily runs but it determines on your spot market availability and how often nodes are reclaimed during your daily run.\nIf you\u0026rsquo;re in AWS and planning to load a year of historical data because you have the audit logs to warrant it, that\u0026rsquo;s great but note that several of the critical APIs such as clusterEvents and sqlQueryHistory don\u0026rsquo;t store data for that long. ClusterEvents in particular is key for calculating utilization and costs; thus you need to determine if the historical data you\u0026rsquo;re going to get is really worth loading all of this history. Additionally, sparkEvents will only load so much history based on cluster log availability and will likely not load logs for historically transient clusters since Overwatch will look for the clusterEvents, they will be missing for a jobs cluster from 6 months ago and Overwatch will determine there\u0026rsquo;s no activity on the cluster and not scan its directories.\nLastly, note that several of the sources in bronze aren\u0026rsquo;t highly parallelizable due to their sources; this means that throwing more compute at this problem will only provide limited benefits.\nEmpty / Test / POC workspaces Aren\u0026rsquo;t A Valid Test If most of your scopes are empty (i.e. you\u0026rsquo;ve never run any jobs and only have 1 cluster without logging enabled) most of the modules will be skipped and provide no insight as to if Overwatch is configured correctly for production.\nCluster Logs Ingest Details For Overwatch to load the right-side of the ERD (Spark UI) the compute that executes the Spark Workloads must have cluster logging enabled and Overwatch must be able to load these logs. To do this, Overwatch uses its own data to determine the logging locations of all clusters and identifies clusters that have created logs within the Pipeline run\u0026rsquo;s time window (i.e. Jan 1 2023 \u0026ndash;\u0026gt; Jan 2 2023).\nHistorically (legacy deployments), this was fairly simple as all the logging locations were generally accessible from the workspace on which Overwatch Pipeline was running (since there was one Overwatch Pipeline on each workspace). There was massive demand to enable remote workspace monitoring such that Overwatch is only needed to be deployed on a single workspace and can monitor one or many local/remote workspaces from a centralized location.\nAs a result of this, the security requirements became a bit more involved to ensure the Overwatch cluster has access to all the storage where clusters persist their logs. An Overwatch cluster on workspace 1 cannot simply access logs on workspace 2 at some mounted location since the mounted location on workspace 2 may actually map to a different target than it does on workspace 1. To solve this, Overwatch uses a \u0026ldquo;search-mounts\u0026rdquo; api to translate remote mounts to fully-qualified storage paths; as such the Overwatch cluster must be authorized to directly access the fully-qualified storage path hence the need for additional security authorizations. If this is not something that is possible for your organization, you may still use the new deployment method and simply deploy Overwatch on each workspace similar to the traditional method and nothing changes.\nFor AWS \u0026ndash; ensure the Overwatch cluster has access to the remote s3:// paths to which clusters log\nFor Azure \u0026ndash; An SPN should be provisioned and authorized to read to all abfss:// storage accounts / containers to which cluters log. See Security Consideration for more details on how to add the SPN configuration to the Overwatch Cluster..\nCluster Logs Loading Scenarios Examples often help simplify complex details so let\u0026rsquo;s examine some scenarios. Given that Workspace 123 is where Overwatch is deployed and you would like to monitor workspaces 123 and 456 with Overwatch from the single 123 deployment. You deploy Overwatch on Workspace 123 and configure 456 as a remote but now you need to figure out how to get cluster on workspace 123 to access all the storage in the remote workspace[s]. This is where some governance and cluster policies come in handy \u0026ndash; if you\u0026rsquo;re organization is already forcing clusters to log to a limited number of storage accounts / buckets then provisioning access here isn\u0026rsquo;t too difficult.\nThe image below illustrates how the traffic flows along with several notes about how cluster log acquisition works for local and remote workspaces. Given the above diagram and a cluster logging path, below is a table of access scenarios. Notice the \u0026ldquo;Overwatch Reads Logs From\u0026rdquo; field which demonstrates from which path Overwatch will load the logs in the given scenario; the Overwatch cluster must have access to read from that location.\nWorkspace Cluster Logging Path Is Mounted Fully Qualified Storage Location Accessible From Deployment Workspace Overwatch Reads Logs From Comments 123 dbfs:/mnt/cluster_logs (AWS) True s3://myCompany/logsBucket/123 True dbfs:/mnt/cluster_logs Locally mounted locations are directly accessed on deployment workspace 123 dbfs:/mnt/cluster_logs (Azure) True abfss://mylogsContainer123@myComany\u0026hellip; True dbfs:/mnt/cluster_logs Locally mounted locations are directly accessed on deployment workspace 123 dbfs:/unmounted_local_path False dbfs:/unmounted_local_path True dbfs:/unmounted_local_path Locally mounted locations are directly accessed on deployment workspace 123 s3://myCompany/logsBucket/123 (AWS) False s3://myCompany/logsBucket/123 True* s3://myCompany/logsBucket/123 AWS - Direct s3 path access as configured in cluster logs 456 dbfs:/mnt/cluster_logs (AWS) True s3://myCompany/logsBucket/456 True* s3://myCompany/logsBucket/456 Remote mount point is translated to fully-qualified path and remote storage is access directly 456 dbfs:/mnt/cluster_logs (Azure) True abfss://mylogsContainer456@myComany\u0026hellip; True* abfss://mylogsContainer456@myComany\u0026hellip; Remote mount point is translated to fully-qualified path and remote storage is access directly 456 dbfs:/unmounted_local_path False dbfs:/unmounted_local_path False Inaccessible Root dbfs storage locations are never accessible from outside the workspace 456 s3://myCompany/logsBucket/456 (AWS) False s3://myCompany/logsBucket/456 True* s3://myCompany/logsBucket/456 AWS - Direct s3 path access as configured in cluster logs * (AWS) Assuming Instance Profile authroized to the fully qualified path location and configured\n* (AZURE) Assuming SPN authroized to the fully qualified path location and configured on the Overwatch cluster\nException - Remote Workspaces With 50+ Mounts In the image above, you likely noticed the alert. As of version 0.7.1.1, Overwatch can handle remote workspaces with more than 50 mounts through an alternative process.\nWhen a remote workspace has \u0026gt; 50 mounts and 1 or more clusters log to a mounted location, the following process must be followed to acquire the logs for that path.\nRemote Workspaces With 50+ Mounts Config Process The process below will allow you to bypass the API limitation and manually acquire all the mounts and their maps for a workspace as a CSV. Upload this to an accessible location on the deployment workspace (i.e. Workspace 123) and add it to the configuration for that workspace.\nContinuing the example from above, if Workspace 456 had \u0026gt; 50 mounts\nLog into the remote workspace (Workspace 456) Import and Execute this Notebook HTML | DBC Follow instruction in the notebook to download the results as CSV. Download the results (all of them \u0026ndash; follow closely if \u0026gt; 1000 mounts) Name the file something meaningful so you know it corresponds to Workspace 456 (i.e. mounts_mapping_456.csv) Upload to accessible location in deployment workspace (Workspace 123) such as dbfs:/Filestore/overwatch/mounts_mapping_456.csv Add the path to the mount_mapping_path in the multi-workspace config. Download Screenshot Example If more than 1000 mounts use follow the screenshot exactly otherwise just hit the down arrow to download Now Overwatch can bypass the API limitation of only being able to access 50 mounts for this remote workspace. Repeat this process for all workspaces that have \u0026gt;50 mounts. Leave the \u0026ldquo;mount_mapping_path\u0026rdquo; emtpy for the workspaces that do not have \u0026gt;50 mounts and Overwatch will use the api to automatically map the mount points to fully-qualified storage paths for you.\nWHY SO COMPLICATED The Databricks API does not support pagination and will only return a max of 50 results. This process is meant as a work-around method until Databricks can update this API to support higher max results, filters, and/or pagination at which time the subsequent release of Overwatch will omit this complexity.\nJoining With Slow Changing Dimensions (SCD) The nature of time throughout the Overwatch project has resulted in the need to for advanced time-series DataFrame management. As such, a vanilla version of databrickslabs/tempo was implemented for the sole purpose of enabling Scala time-series DataFrames (TSDF). TSDFs enable \u0026ldquo;asofJoin\u0026rdquo; and \u0026ldquo;lookupWhen\u0026rdquo; functions that also efficiently handle massive skew as is introduced with the partitioning of organization_id and cluster_id. Please refer to the Tempo documentation for deeper info. When Tempo\u0026rsquo;s implementation for Scala is complete, Overwatch plans to simply reference it as a dependency.\nThis is discussed here because these functionalities have been made public through Overwatch which means you can easily utilize \u0026ldquo;lookupWhen\u0026rdquo; and \u0026ldquo;asofJoin\u0026rdquo; when interrogating Overwatch. The details for optimizing skewed windows is beyond the scope of this documentation but please do reference the Tempo documentation for more details.\nIn the example below, assume you wanted the cluster name at the time of some event in a fact table. Since cluster names can be edited, this name could be different throughout time so what was that value at the time of some event in a driving table.\nThe function signature for \u0026ldquo;lookupWhen\u0026rdquo; is:\ndef lookupWhen( rightTSDF: TSDF, leftPrefix: String = \u0026#34;\u0026#34;, rightPrefix: String = \u0026#34;right_\u0026#34;, maxLookback: Long = Window.unboundedPreceding, maxLookAhead: Long = Window.currentRow, tsPartitionVal: Int = 0, fraction: Double = 0.1 ): TSDF = ??? import com.databricks.labs.overwatch.pipeline.TransformFunctions._ val metricDf = Seq((\u0026#34;cluster_id\u0026#34;, 10, 1609459200000L)).toDF(\u0026#34;partCol\u0026#34;, \u0026#34;metric\u0026#34;, \u0026#34;timestamp\u0026#34;) val lookupDf = Seq( (\u0026#34;0218-060606-rouge895\u0026#34;, \u0026#34;my_clusters_first_name\u0026#34;, 1609459220320L), (\u0026#34;0218-060606-rouge895\u0026#34;, \u0026#34;my_clusters_new_name\u0026#34;, 1609458708728L) ).toDF(\u0026#34;cluster_id\u0026#34;, \u0026#34;cluster_name\u0026#34;, \u0026#34;timestamp\u0026#34;) metricDf.toTSDF(\u0026#34;timestamp\u0026#34;, \u0026#34;cluster_id\u0026#34;) .lookupWhen( lookupDf.toTSDF(\u0026#34;timestamp\u0026#34;, \u0026#34;cluster_id\u0026#34;) ) .df .show(20, false) Using Overwatch Data to enrich realtime monitoring A supported TSDB (time-series database) is required to enable and integrate this. Databricks cannot provide support for deployment / configuration of TSDBs but can assist in integrating Databricks clusters with supported TSDBs. Supported TSDBs include:\nGraphite Prometheus LogAnalytics (Azure) Overwatch is often integrated with real-time solutions to enhance the data provided as raw Spark Metrics. For example, you may be monitoring jobs in real-time but want job_names instead of job_ids, Overwatch\u0026rsquo;s slow-changing dimensions can enhance this.\nReal-time monitoring usually comes from at least two different sources:\nspark via DropWizard (a default spark monitoring platform) machine metrics via collectd or some native cloud solutions such as CloudWatch or LogAnalytics. These real-time metrics can be captured as quickly as 5s intervals but it\u0026rsquo;s critical to note that proper historization is a must for higher intervals; furthermore, it\u0026rsquo;s critical to configure the metrics at the \u0026ldquo;correct\u0026rdquo; interval for your business needs. In other words, it can get quickly get expensive to load all metrics at 5s intervals. All of these details are likely common knowledge to a team that manages a time-series database (TSDB).\nBelow is a scalable, reference architecture using Graphite and Grafana to capture these metrics. Overwatch creates several daily JSON time-series compatible exports to Grafana JSON that provide slow-changing dimensional lookups between real-time keys and dimensional values through joins for enhanced dashboards.\nBelow is a link to a notebook offering samples for integrating Spark and machine metrics to some real-time infrastructure endpoint. The examples in this notebook offer examples to Prometheus, Graphite, and Log Analytics for Spark Metrics and collectD for machine metrics. Critical this is just a sample for review, this notebook is intended as a reference to guide you to creating your own implementation, you must create a script to be valid for your requirements and capture the right metrics and the right intervals for the namespaces from which you wish to capture metrics.\nRealtime Getting Started Reference Notebook HTML | DBC\nThe realtime reference architecture has been validated but 1-click delivery has not yet been enabled. The time-series database / infrastructure setup is the responsibility of the customer; Databricks can assist with integrating the spark metrics delivery with customer infrastructure but Databricks cannot offer much depth for standing up / configuring the real-time infrastructure itself.\nExample Realtime Architecture Example Realtime Dashboards Simple Spark Dashboard IO Dashboard Advanced Dashboard "
},
{
	"uri": "http://localhost:1313/overwatch/deployoverwatch/systemtableintegration/systemtableconfiguration/",
	"title": "System Table Configuration",
	"tags": [],
	"description": "",
	"content": "Configuration changes required for System Table Integration There is no unique configuration process for System Table. The only change required to the configuration for integration is below -\nauditlogprefix_source_path - Instead of adding a fully qualified path (s3 or GC) for auditlog, add keyword system in this column. This will enable the system table integration. For all other configurations, please follow the Configuration\nOnce the customer migrate to system table (version 0.8.0.0), they cannot revert back to legacy Deployment.\nFor Azure deployments since Eventhub is not required for System Table Integration, we can leave these fields blank - eh_name, Event hub name, eh_name, eh_conn_string, aad_tenant_id, aad_client_id, aad_client_secret_key, aad_authority_endpoint\nConfiguration changes required for Multi Account System Table Integration The process for achieving multi-account deployment is straightforward. In the configuration table, for each workspace in a different account from the one where Overwatch is run from, you\u0026rsquo;ll need to enter a value for the sql_endpoint column. In order to get this value, you can create a new warehouse or use an existing one on a workspace in the remote account. We encourage you to use a serverless warehouse for this purpose. You would then pass the warehouse endpoint details to Overwatch through the configuration in the sql_endpoint column. sql_endpoint column needs to be populated only for workspaces in a different account You can use the same warehouse sql_endpoint to collect the data for all the workspaces in the remote account.\nBelow are the details:\nsql_endpoint - this will be the http path from the sql warehouse. This can be found in the connection details table of the warehouse. See the below screenshot on how to find it auditlogprefix_source_path - Instead of adding a fully qualified path (s3 or GC) for auditlog, add keyword system in this column. This will enable the system table integration. For all other configurations, please follow the Configuration\n"
},
{
	"uri": "http://localhost:1313/overwatch/",
	"title": "Welcome To OverWatch",
	"tags": [],
	"description": "",
	"content": "Project Overview Overwatch was built to enable Databricks\u0026rsquo; customers, employees, and partners to quickly / easily understand operations within Databricks deployments. As enterprise adoption increases there\u0026rsquo;s an ever-growing need for strong governance. Overwatch means to enable users to quickly answer questions and then drill down to make effective operational changes. Common examples of operational activities Overwatch assists with are:\nCost tracking by various dimensions Governance at various dimensions Optimization Opportunities What If Experiments How Much Does It Cost Overwatch is currently a Databricks Labs project and has no direct cost associated with it other than the costs incurred with running the daily jobs. Audit logging is required and as such a Azure Databricks Premium SKU OR the equivalent AWS Premium Plan or above. As a reference, a cost analysis was performed at a large Databricks customer. This customer had \u0026gt;1000 named users with \u0026gt;400 daily active users with a contract price with Databricks over $2MM/year. Overwatch costs them approximately $8 USD / day to run which includes storage, compute, and DBUs. $8 / day == approximately $3,000 / year or 0.15% of Databricks contract price. This is just a single reference customer but cost monitoring for your Overwatch job should be the same as any other job you run. At the time of this writing Overwatch has only been deployed as a pilot and has already proven several million dollars in value. The savings customers found through these efficiency improvements allowed them to accelerate their backlogs, permanently lower the total cost of ownership, and even take on additional exploratory efforts.\nHow It Works Overwatch amalgamates and unifies all the logs produced by Spark and Databricks via a periodic job run (typically 1x/day). The Overwatch job then enriches this data through various API calls to the Databricks platform and, in some cases, the cloud provider. Finally, all the data is pulled together into a data model that enables simplified queries for powerful insights and governance. Below are a few example reports produced by Overwatch. The batch / historical data is typically used to identify the proper boundaries that can later be used to set up monitoring / alerts in the real-time modules.\nHot Notebooks Heavy User Breakdown You can also use some of the Overwatch data to enrich real time Spark monitoring data. In order to see how to do this, please refer to the information here\nData Process Flow "
},
{
	"uri": "http://localhost:1313/overwatch/deployoverwatch/systemtableintegration/systemtableprereq/",
	"title": "System Table Pre-Requisites",
	"tags": [],
	"description": "",
	"content": "This section will walk you through the steps necessary as a prerequisite for System Table Integration with Overwatch.\nWorkspace should be UC enabled and System Table enabled. Enabling system tables is straightforward, you do need to have at least one unity-catalog enabled workspace. System tables must be enabled by an account admin. You can enable system tables using the Unity Catalog REST API. Once enabled, the tables will appear in a catalog called system, which is included in every Unity Catalog metastore. In the system catalog youll see schemas such as access and billing that contain the system tables. Follow the documents to enable System Tables\nOverwatch Pipeline Cluster must be UC enabled (single user and Databricks runtime version \u0026gt;= 11.3).\nFor multi account deployment Overwatch Pipeline Cluster must be UC enabled (single user and Databricks runtime version \u0026gt;= 13.3).\nOverwatch latest version(0.8.0.0) should be deployed in the workspace\nPrincipal (user/SP) executing the Overwatch Pipeline must have access to the catalog - system with privileges:\nUSE CATALOG USE SCHEMA SELECT Principal (user/SP) executing the Overwatch Pipeline must have access to the schema - access with privileges:\nUSE SCHEMA SELECT Principal (user/SP) executing the Overwatch Pipeline must have access to the table - audit with privileges:\nSELECT Other overwatch prerequisites can be found here\nOnce all System Table Integration Pre-requisites are completed, please continue to Deploy Overwatch section.\n"
},
{
	"uri": "http://localhost:1313/overwatch/deployoverwatch/systemtableintegration/",
	"title": "System Table Integration",
	"tags": [],
	"description": "",
	"content": "Databricks system tables offer a robust mechanism for managing and understanding the intricacies of a Databricks environment. From metadata exploration to performance optimization and security auditing, these tables provide a comprehensive foundation for users and administrators to derive valuable insights and make informed decisions in a collaborative and data-driven environment. Follow these documents to know more about System Tables Databricks Docs for System Tables\nIn our continuous efforts to enhance the efficiency and robustness of Overwatch, we have transitioned to a system table integration for audit logs. This strategic move brings forth numerous advantages, further optimizing the monitoring and logging capabilities of the tool. Currently, our integration focuses on leveraging system table audit logs, but in future releases, we will be expanding our integration to include other system tables as well. Follow these documents to know more about System Table Audit Logs\nAdvantages of Moving to System Table Integrations:\nSeamless Integration with Overwatch: System table integration seamlessly aligns with Overwatch. The synergy between system tables and Overwatch ensures a harmonious integration, providing a unified platform for comprehensive monitoring and analysis.\nSimplified Setup within Overwatch: Integration with system tables leads to a simplified setup within Overwatch. The need to configure audit log settings independently within the Overwatch configurations is eliminated. This simplicity ensures a more user-friendly experience for both setup and ongoing management.\nEffortless Migration: The migration process to system tables is designed to be effortless within the Overwatch framework. The transition is smooth, minimizing disruptions and ensuring continuous monitoring capabilities without requiring extensive reconfigurations.\nExtended Data Retention for Azure Deployments in Overwatch: Particularly advantageous for Overwatch Azure deployments, the use of system tables extends the audit log retention period beyond 30 days for audit log bronze. This eliminates the previous dependency on Events Hub\u0026rsquo;s retention period.\nSystem Table Integration Specific Configuration Details As mentioned earlier, the deployment process is almost the same as other deployments. Therefore, the Deployment Guide contains relevant information that will guide your deployment. Specific differences are referenced below and detailed in this section of the documentation.\nSystem Table Integration Pre-Requisites Configuration Details Migrating to System Table Once the deployment is migrated to System Tables, it cannot be reverted back to source the audit logs from S3, GC Bucket or Azure Event Hubs i.e Legacy Deployment.\nThe migration to system tables is straightforward. Organizations can effortlessly transition from existing configurations to system table integrations, minimizing downtime and ensuring a smooth migration experience. Follow the below steps to migrate to System Table Integration: Configuration Details\nFetching data from Multi Account System Table With the integration of System Table in Overwatch, fetching system table data from multiple different accounts becomes easy, enabling seamless multi-account deployments. This setup requires minimal adjustments in configuration and additional infrastructure setup in the workspaces of other accounts. Follow the link to configure Multi Account System Table Integration\n"
},
{
	"uri": "http://localhost:1313/overwatch/deployoverwatch/cloudinfra/gcp/",
	"title": "GCP",
	"tags": [],
	"description": "",
	"content": "Configuring Overwatch on GCP - Databricks Reach out to your Databricks representative to help you with these tasks as needed.\nThere are two primary sources of data that need to be configured:\nAudit Logs-GCP These will be delivered to the configured bucket. These buckets are configured on a per-workspace basis and can be delivered to the same target bucket, just ensure that the prefixes are different to avoid collisions. We don\u0026rsquo;t want multiple workspaces delivering into the same prefix. The audit logs contain data for every interaction within the environment and are used to track the state of various objects through time along with which accounts interacted with them. This data is relatively small and delivery occurs infrequently which is why it\u0026rsquo;s rarely of any consequence to deliver audit logs to buckets even outside of the control plane region. Cluster Logs - Crucial to get the most out of Overwatch Cluster logs delivery location is configured in the cluster spec \u0026ndash;\u0026gt; Advanced Options \u0026ndash;\u0026gt; Logging. These logs can get quite large and they are stored in a very inefficient format for query and long-term storage. As such, it\u0026rsquo;s crucial to store these logs in the same region as the worker nodes for best results. Additionally, using dedicated buckets provides more flexibility when configuring TTL (time-to-live) to minimize long-term, unnecessary costs. It\u0026rsquo;s not recommended to store these on DBFS directly (dbfs mount points are ok). Best Practice - Multi-Workspace \u0026ndash; When multiple workspaces are using Overwatch within a single region it\u0026rsquo;s best to ensure that each are going to their own prefix, even if sharing a bucket. This greatly reduces Overwatch scan times as the log files build up. To enable Cluster Logs on Multiworkspace - follow this link GCP \u0026ndash; Remote Cluster Logs - Databricks on GCP, does not support mounted/GCS bucket locations. Customers must provide DBFS root path as a target for log delivery.\nReference Architecture As of 0.7.1 Overwatch can be deployed on a single workspace and retrieve data from one or more workspaces. For more details on requirements see Multi-Workspace Consideration. There are many cases where some workspaces should be able to monitor many workspaces and others should only monitor themselves. Additionally, co-location of the output data and who should be able to access what data also comes into play, this reference architecture can accommodate all of these needs. To learn more about the details walk through the deployment steps\nUsing System tables Using GCS Delivery "
},
{
	"uri": "http://localhost:1313/overwatch/dashboards/",
	"title": "Dashboards",
	"tags": [],
	"description": "",
	"content": "We have created a set of dashboards containing some essential, pre-defined metrics, to help you get started on your Overwatch journey. These are meant to be a learning resource for you to understand the data model, as well as a practical resource to help you get value out of Overwatch right away. As a first iteration, these are notebook-based dashboards, in the future we\u0026rsquo;ll have these available in DBSQL as well.\nAvailable Dashboards Workspace Start here, this is your initial overall view of the state of the workspaces you monitor with Overwatch\nMetrics available in this dashboard Daily cluster spend chart Compute Time of scheduled jobs on each workspace Cluster spend on each workspace Node type count by azure workspace DBU Cost vs Compute Cost Node type count by AWS workspace Cluster spend by type on each workspace Node type cost by azure workspace Cluster count by type on each workspace Node type cost by AWS workspace Count of scheduled jobs on each workspace Workspace Tags count by workspace Clusters This dashboard will deep dive into cluster-specific metrics\nMetrics available in this dashboard DBU Spend by cluster category Total DBU Incurred by top spending clusters per category DBU spend by the most expensive cluster per day per workspace Percentage of Autoscaling clusters per category Top spending clusters per day Scale up time of clusters (with \u0026amp; without pools) by cluster category DBU Spend by the top 3 expensive Interactive clusters (without auto-termination) per day Cluster Failure States and count of failures Cluster count in each category Cost of cluster failures per Failure States per workspace Cluster node type breakdown Cluster Failure States and failure count distribution Cluster node type breakdown by potential Interactive cluster restarts per day per cluster Cluster node potential breakdown by cluster category Job This dashboard will deep dive into job/workload specific metrics\nMetrics available in this dashboard Daily Cost on Jobs Daily Job status distribution Job Count by workspace Number of job Runs (Succeeded vs Failed) Jobs running in Interactive Clusters (Top 20 workspaces) Compute Time of Run Failures By Workspace Notebook In this dashboard we will cover metrics that could help in the tuning of workloads by analyzing the code run in Notebooks.\nMetrics available in this dashboard Data Throughput Per Notebook Path Largest shuffle explosions Longest Running Notebooks Total Spills per notebook run Data Throughput Per Notebook Path Largest shuffle explosions Top notebooks returning a lot of data to the UI Processing Speed (MB/sec) Spark Actions (count) Longest Running Failed Spark Jobs Notebooks with the largest records Serialization/deserialization time (ExecutorDeserializeTime + ResultSerializationTime) Task count by task type Notebook compute hours Large Tasks Count (\u0026gt; 400MB) Most popular (distinct users) notebooks per path depth Jobs Executing on Notebooks (count) Top notebooks returning a lot of data to the UI Processing Speed (MB/sec) Spark Actions (count) Longest Running Failed Spark Jobs Notebooks with the largest records Serialization/deserialization time (ExecutorDeserializeTime + ResultSerializationTime) Task count by task type Notebook compute hours Large Tasks Count (\u0026gt; 400MB) Most popular (distinct users) notebooks per path depth Jobs Executing on Notebooks (count) DBSQL A generic view at the performance of your DBSQL-specific queries\nMetrics available in this dashboard Global query duration and Global Query Core Hours Core hours by users (Top 20) Query count through time Distinct user count my warehouse Query count by warehouse Core hours by date Core hours by warehouse Core hours by Is Serverless Dashboard files Please download the dbc file and import it into your workspace, read through the readme, and you should be able to get them running right away.\nVersion 1 - DBC Released September 11, 2023 Version 1.1 - DBC Released October 31, 2023 Fix to \u0026ldquo;DBU Spend by the top 3 expensive Interactive clusters\u0026rdquo; metric in Clusters dashboard. "
},
{
	"uri": "http://localhost:1313/overwatch/troubleshooting/",
	"title": "Troubleshooting",
	"tags": [],
	"description": "",
	"content": "This section is meant to help customers identify what might be causing an issue with a deployment or with the pipeline itself.\nMost of the issues customers face are related to the configuration of the different cloud artifacts required to run Overwatch. We\u0026rsquo;ve created a notebook to help you troubleshoot:\nReadiness review notebook 0.8.x HTML | DBC\nTo help our support team, please download, import, and execute the following notebook in the environment having the issue. If you\u0026rsquo;re monitoring multiple workspaces from a single deployment, please execute the notebook on the workspace that is running Overwatch. The report has been anonymized to protect your privacy but still deliver the information necessary to troubleshoot the issue.\nGeneric Anonymized Diagnostics Report HTML | DBC\nRun the script and download the notebook in HTML format. (In the notebook: File -\u0026gt; Export -\u0026gt; HTML)\nOnce you have this HTML file you can either [open a GitHub ticket]((https://github.com/databrickslabs/overwatch/issues/new?assignees=\u0026amp;labels=user_question\u0026amp;template=overwatch-issue.md\u0026amp;title=), or contact your Databricks representative and attach the report to the email.\n"
},
{
	"uri": "http://localhost:1313/overwatch/deployoverwatch/cloudinfra/azure/",
	"title": "Azure",
	"tags": [],
	"description": "",
	"content": "Fast Travel Configuring Overwatch on Azure Databricks Reference Architecture Configuring Audit Log Delivery Through Event Hub Setting up Storage Accounts Mount Storage Accounts Configuring Overwatch on Azure Databricks Reach out to your Databricks representative to help you with these tasks as needed. To get started, we suggest you deploy a single workspace end to end so that you can figure out the steps involved and you can then apply these for the other workspaces to be deployed.\nThere are two primary sources of data that need to be configured:\nAudit Logs The audit logs contain data for every interaction within the environment and are used to track the state of various objects through time along with which accounts interacted with them. This data is relatively small and it certainly doesn\u0026rsquo;t contain any large data sets like cluster/spark logs. For ingesting this data, you have the option of using System tables (RECOMMENDED) or set up the delivery through Event Hubs Cluster Logs - Crucial to get the most out of Overwatch Cluster logs delivery location is configured in the cluster spec \u0026ndash;\u0026gt; Advanced Options \u0026ndash;\u0026gt; Logging. These logs can get quite large and they are stored in a very inefficient format for query and long-term storage. This is why it\u0026rsquo;s crucial to create a dedicated storage account for these and ensure TTL (time-to-live) is enabled to minimize long-term, unnecessary costs. It\u0026rsquo;s not recommended to store these on DBFS directly (dbfs mount points are ok). Best Practice - Multi-Workspace \u0026ndash; When multiple workspaces are using Overwatch within a single region it\u0026rsquo;s best to ensure that each are going to their own prefix, even if sharing a storage account. This greatly reduces Overwatch scan times as the log files build up. If scan times get too long, the TTL can be reduced or additional storage accounts can be created to increase read IOPS throughput (rarely necessary) intra-region. Reference Architecture As of 0.7.1 Overwatch can be deployed on a single workspace and retrieve data from one or more workspaces. For more details on requirements see Multi-Workspace Consideration. There are many cases where some workspaces should be able to monitor many workspaces and others should only monitor themselves. Additionally, co-location of the output data and who should be able to access what data also comes into play, this reference architecture can accommodate all of these needs. To learn more about the details walk through the deployment steps\nUsing System tables Using EventHubs Audit Log Delivery There are two ways for Overwatch to read the Audit logs:\nUsing System Tables (see System Table Configuration Details ) or Through EventHubs via Azure Diagnostic Logging. Overwatch will consume the events as a batch stream (Trigger.Once) once/period when the job runs. To configure Eventhub to deliver these logs, follow the steps below. LIMIT If Event Hub subscription is Standard, max of 10 Hubs per Namespace. If more than 10 workspaces are deployed in the same region, plan to distribute your Event Hubs and Namespaces appropriately.\nConfiguring Event Hub Step 1 Create or reuse an Event Hub namespace. The Event Hub Namespace MUST be in the same location as your control plane\nIf you select the \u0026ldquo;Basic\u0026rdquo; Pricing Tier, just note that you will be required to ensure at least two successful Overwatch runs per day to ensure there won\u0026rsquo;t be data loss. This is because message retention is 1 day meaning that data expires out of EH every 24 hours. \u0026ldquo;Standard\u0026rdquo; Pricing tier doesn\u0026rsquo;t cost much more but it will give you up to 7 days retention which is much safer if you grow dependent on Overwatch reports.\nThroughput Units In almost all cases, 1 static throughput unit is sufficient for Overwatch. Cases where this may not be true are include cases where there are many users across many workspaces sharing a single EH Namespace. Review the Throughput Units sizing and make the best decision for you.\nInside the namespace create an Event Hub\nALERT!!\nIf your Event Hub is behind a VNET, you must \u0026ldquo;Allow Trusted MSFT Services through the Firewall\u0026rdquo; or configure specific firewall rules. This should be done in the Event Hub Namespace \u0026ndash;\u0026gt; Networking Tab; see screenshot below.\nStep 2 Create an Event Hub inside your chosen (or created) EH Namespace. Every Workspace must have its own Event Hub \u0026ndash; They cannot be shared. Event Hub namespaces (up to 10 for Standard) can be shared but the Event Hub must be different. This is to save you time and money since Event Hubs act like topics and allow us to sub-select data without scanning / streaming it all in just to filter it out. If you try to share an Event Hub, you will be reloading data as there will be several issues.\nPartitions The maximum number of cores (parallelism) that can simultaneously pull data from your event hub == EH partitions. In most cases this should be set to 32. It can be fewer, just be sure that Overwatch uses an autoscaling cluster to minimize costs while waiting to pull the data from EH. For example, if Overwatch is pulling 30 million events but only has 8 partitions, any worker cores over 8 will sit idle while all 30 million events are retrieved. In a situation like this, the minimum autoscaling compute size should approximately equal the number of partitions to minimize waste.\nOverwatch needs access to the Event Hub. There are two methods for provisioning access, either a SAS Policy with keys in the connection stringOR AAD SPN. Chose your method and follow the docs below.\nStep 2.1 Authorizing Access Via SAS Policy Once created, get the connection string from the SAS Policy for the Event Hub, find the following path in the Azure portal below.\neh-namespace \u0026ndash;\u0026gt; eventhub \u0026ndash;\u0026gt; shared access policies \u0026ndash;\u0026gt; Connection String-primary key\nThe connection string should begin with Endpoint=sb://. Note that the policy only needs the Listen permission\nClick Add button and select Listen option for generate policies Copy the Connection string-primary key and create a secret using Key vault\nStep 2.2 Authorizing Access Via AAD SPN Navigate either to the EH Namespace or the Event Hub (whichever is appropriate for you), click on \u0026ldquo;Access Control (IAM)\u0026rdquo; and then click add \u0026ndash;\u0026gt; Add Role Assignment. Choose the Role \u0026ldquo;Azure Event Hubs Data Receiver\u0026rdquo; and add the principal you would like to provision \u0026ndash;\u0026gt; review and assign.\nNow the Principal has access to the EH or EHNS, now you just need to capture the details of the SPN to provide to Overwatch configs. These can be found by going to the SPN Overview Active Directory \u0026ndash;\u0026gt; App Registrations \u0026ndash;\u0026gt; click the principal. Now capture the details in the screenshot below.\nNow a secret needs to be created (if one doesn\u0026rsquo;t already exist), so from the previous screen click \u0026ldquo;Certificates \u0026amp; Secrets\u0026rdquo; \u0026ndash;\u0026gt; \u0026ldquo;New Client Secret\u0026rdquo;. Be sure to capture the secret as it won\u0026rsquo;t be visible later. Create a Databricks Secret and place the SPN secret in the Databricks secret. This is the value needed to use the AAD SPN to complete the rest of the AAD required configs.\nThe last thing you need is your connection string and that can be found by navigating back to your Event Hub ** be sure to get to the Event Hub not the Event Hub Namespace (i.e EHNS \u0026ndash;\u0026gt; Event Hubs \u0026ndash;\u0026gt; Event Hub). Then in the Overview section you will see \u0026ldquo;Namespace\u0026rdquo;, we will use this to construct the connection string.\nEndpoint=sb://\u0026lt;namespace\u0026gt;.servicebus.windows.net Step 3 With your Event Hub Namespace and Named Event Hub created, Navigate to your Azure Databricks workspace[s] (in the portal) for which you\u0026rsquo;d like to enable Overwatch. Under Monitoring section \u0026ndash;\u0026gt; Diagnostics settings \u0026ndash;\u0026gt; Add diagnostic setting. Configure your log delivery similar to the example in the image below.\nAdditionally, ensure that the Overwatch account has sufficient privileges to read data from the Event Hub[s] created above. A common method for providing Overwatch access to the Event Hub is to simply capture the connection string and store it as a secret. Proper ways for accessing this will be in the getting started and configuration sections. There are many methods through which to authorize Overwatch, just ensure it has the access to read from the Event Hub Stream.\nDO NOT leave Event hub name empty! Even though Azure doesn\u0026rsquo;t require an event hub name, you must create an event hub underneath the event hub namespace and give it a name. Reference the name here.\nStep 4: Validate Messages Are Flowing Now that you have configured you Overwatch EventHub Namespace, named Event Hub inside the namespace, and pointed diagnostic logging to the EventHub it\u0026rsquo;s time to validate that messages are flowing. You may have to wait several minutes to begin to see messages flowing depending on how busy the workspace is. There are two things that are commonly missed, please double-check the two bullet points below, there are images to help clarify as well.\nA named Event Hub has been created within the Namespace. Messages are flowing into the named event hub Setting up Storage Accounts The following steps are meant to be a baseline reference for setting up the storage accounts for Overwatch targets. With regard to security, we cannot make recommendations as to what is best for your organization in a global forum like this. For specifics regarding security implications of your storage account, please contact your Databricks sales representative or talk with your Databricks / Cloud administrators.\nThat said, the minimum technical requirement for Overwatch to function is that the storage account exist and be able to be access (read/write for Overwatch output, read for cluster logs) the storage from the Databricks workspace. All security consideration above this basic technical requirement should be commensurate with your organizational policies/standards.\nData locality is important for cluster logs. Cluster logs can be large and plentiful and as such, sending these to a storage account in a different geographical location can have significant performance impact. Keep this in mind and note that the reference architecture recommends at least one storage account per region to minimize latency and maximize bandwidth.\nOVERVIEW Barring the security and networking sections, once you\u0026rsquo;re setup is complete, your configuration should look similar the image below. Step 1 Select Storage Account from your Azure Portal and hit create\nStep 2 Enter your Subscription and Resource Group in Basics Tab Locally-Redundant Storage specified here but choose which is right for your organization. Standard storage is fine, the required IOPS here are relatively low and, while premium storage will work, it\u0026rsquo;s at an added, unnecessary cost. Step 3 Recommendations\nSecurity \u0026ndash; commensurate with your organization policies but must be accessible from the Databricks Workspace. Hot access tier Not tested on cold and cold will suffer significant performance loss Hierarchical namespace While hierarchical namespace is technically option, it\u0026rsquo;s STRONGLY recommended that this be enabled. Networking, similar to security, this should be configured commensurate with your corporate policies. The only technical requirement is that the data in the storage account is accessible from the Databricks workspace. Step 4 You may choose any options you prefer here. Note that soft-deletion of BLOBs cannot be mounted to the Databricks workspace and is considered an experimental feature. It\u0026rsquo;s recommended that you do NOT enable BlOB soft deletes at this time for the Overwatch storage account as it has not been fully tested. Container soft deletes are fine. Step 5 Add relevant tags and create Using the Storage Account The overwatch output storage account may be accessed directly via:\nabfss:// wasbs:// (not supported) A mounted filed system within Databricks. More information on mounting storage accounts can be found here. "
},
{
	"uri": "http://localhost:1313/overwatch/deployoverwatch/cloudinfra/aws/",
	"title": "AWS",
	"tags": [],
	"description": "",
	"content": "Configuring Overwatch on AWS - Databricks Reach out to your Customer Success Engineer (CSE) to help you with these tasks as needed.\nThere are two primary sources of data that need to be configured:\nAudit Logs-AWS The audit logs contain data for every interaction within the environment and are used to track the state of various objects through time along with which accounts interacted with them. This data is relatively small and delivery occurs infrequently which is why it\u0026rsquo;s rarely of any consequence to deliver audit logs to buckets even outside of the control plane region. For ingesting this data, you have the option of using System tables (RECOMMENDED) or you can configure the audit logs to be delivered to the configured bucket. These buckets are configured on a per-workspace basis and can be delivered to the same target bucket, just ensure that the prefixes are different to avoid collisions. We don\u0026rsquo;t want multiple workspaces delivering into the same prefix. Cluster Logs - Crucial to get the most out of Overwatch Cluster logs delivery location is configured in the cluster spec \u0026ndash;\u0026gt; Advanced Options \u0026ndash;\u0026gt; Logging. These logs can get quite large and they are stored in a very inefficient format for query and long-term storage. As such, it\u0026rsquo;s crucial to store these logs in the same region as the worker nodes for best results. Additionally, using dedicated buckets provides more flexibility when configuring TTL (time-to-live) to minimize long-term, unnecessary costs. It\u0026rsquo;s not recommended to store these on DBFS directly. Best Practice - Multi-Workspace \u0026ndash; When multiple workspaces are using Overwatch within a single region it\u0026rsquo;s best to ensure that each are going to their own prefix, even if sharing a storage account. This greatly reduces Overwatch scan times as the log files build up. If scan times get too long, the TTL can be reduced or additional storage accounts can be created to increase read IOPS throughput (rarely necessary) intra-region. Reference Architecture As of 0.7.1 Overwatch can be deployed on a single workspace and retrieve data from one or more workspaces. For more details on requirements see Multi-Workspace Consideration. There are many cases where some workspaces should be able to monitor many workspaces and others should only monitor themselves. Additionally, co-location of the output data and who should be able to access what data also comes into play, this reference architecture can accommodate all of these needs. To learn more about the details walk through the deployment steps\nUsing System tables Using S3 Delivery With Databricks Billable Usage Delivery Logs/billing system table Detailed costs data directly from Databricks. This data can significantly enhance deeper level cost metrics, and t\u0026rsquo;s easy to join these up with the Overwatch data for cost validation and / or exact cost break down by dimensions not supported in the usage logs/billing system table.\n"
},
{
	"uri": "http://localhost:1313/overwatch/dataengineer/definitions/061x/",
	"title": "Data Dictionary - 0.6.1.x",
	"tags": [],
	"description": "",
	"content": " Consumption Layer Column Descriptions ETL Tables Bronze Silver Gold Consumption Layer \u0026ldquo;Tables\u0026rdquo; (Views) All end users should be hitting consumer tables first. Digging into lower layers gets significantly more complex. Below is the data model for the consumption layer. The consumption layer is often in a stand-alone database apart from the ETL tables to minimize clutter and confusion. These entities in this layer are actually not tables at all (with a few minor exceptions such as lookup tables) but rather views. This allows for the Overwatch development team to alter the underlying columns, names, types, and structures without breaking existing transformations. Instead, view column names will remain the same but may be repointed to a newer version of a column, etc.\nETL should not be developed atop the consumption layer views but rather the gold layer. Before Overwatch version upgrades, it\u0026rsquo;s important that the engineering team review the change list and upgrade requirements before upgrading. These upgrades may require a remap depending on the changes. As of version 1.0 release, all columns in the gold layer will be underscored with their schema version number, column changes will reference the later release version but the views published with Overwatch will almost always point to the latest version of each column and will not include the schema suffix to simplify the data model for the average consumer.\nData Organization The large gray boxes in the simplified ERD below depict the two major, logical sections of the data model:\nDatabricks Platform - Metadata captured by the Databricks platform that can be used to assist in workspace governance. This data can also be enriched with the Spark data enabling in-depth analyses. The breadth of metadata is continuing to grow, stay tuned for additional capabilities. Spark UI The spark UI section is derived from the spark event logs and essentially contains every single piece of data from the Spark UI. There are a few sections that are not included in the first release but the data is present in spark_events_bronze albeit extremely complex to derive. The Overwatch development team is working tirelessly to expose additional SparkUI data and will publish as soon as it\u0026rsquo;s ready. Column Descriptions Complete column descriptions are only provided for the consumption layer. The entity names are linked below.\ncluster clusterStateFact instanceDetails job jobrun jobRunCostPotentialFact notebook instancePool dbuCostDetail accountLogin accountMod sparkExecution sparkExecutor sparkJob sparkStage sparkTask sparkStream **preview Common Meta Fields There are several fields that are present in all tables. Instead of cluttering each table with them, this section was created as a reference to each of these. Most tables below provide a data SAMPLE for reference. You may either click to view it or right click the SAMPLE link and click saveTargetAs or saveLinkAs and save the file. Note that these files are TAB delimited, so you will need to view as such if you save to local file. The data in the files were generated from an Azure, test deployment created by Overwatch Developers.\nCluster SAMPLE\nKEY \u0026ndash; organization_id + cluster_id + unixTimeMS\nIncremental Columns \u0026ndash; unixTimeMS\nPartition Columns \u0026ndash; organization_id\nWrite Mode \u0026ndash; Append\nColumn Type Description cluster_id string Canonical Databricks cluster ID (more info in Common Meta Fields) action string create, edit, or snapImpute \u0026ndash; depicts the type of action for the cluster \u0026ndash; **snapImpute is used on first run to initialize the state of the cluster even if it wasn\u0026rsquo;t created/edited since audit logs began timestamp timestamp timestamp the action took place cluster_name string user-defined name of the cluster driver_node_type string Canonical name of the driver node type. node_type string Canonical name of the worker node type. num_workers int The number of workers defined WHEN autoscaling is disabled autoscale struct The min/max workers defined WHEN autoscaling is enabled auto_termination_minutes int The number of minutes before the cluster auto-terminates due to inactivity enable_elastic_disk boolean Whether autoscaling disk was enabled or not is_automated booelan Whether the cluster is automated (true if automated false if interactive) cluster_type string Type of cluster (i.e. Serverless, SQL Analytics, Single Node, Standard) security_profile struct Complex type to describe secrity features enabled on the cluster. More information Below cluster_log_conf string Logging directory if configured init_script array Array of init scripts custom_tags string User-Defined tags AND also includes Databricks JobID and Databricks RunName when the cluster is created by a Databricks Job as an automated cluster. Other Databricks services that create clusters also store unique information here such as SqlEndpointID when a cluster is created by \u0026ldquo;SqlAnalytics\u0026rdquo; cluster_source string Shows the source of the action **(TODO \u0026ndash; checking on why null scenario with BUI) spark_env_vars string Spark environment variables defined on the cluster spark_conf string custom spark configuration on the cluster that deviate from default acl_path_prefix string Automated jobs pass acl to clusters via a path format, the path is defined here instance_pool_id string Canononical pool id from which workers receive nodes driver_instance_pool_id string Canononical pool id from which driver receives node instance_pool_name string Name of pool from which workers receive nodes driver_instance_pool_name string Name of pool from which driver receives node spark_version string DBR version - scala version idempotency_token string Idempotent jobs token if used ClusterStateFact SAMPLE\nKEY \u0026ndash; organization_id + cluster_id + state + unixTimeMS_state_start\nIncremental Columns \u0026ndash; state_start_date + unixTimeMS\nPartition Columns \u0026ndash; organization_id + state_start_date\nZ-Order Columns \u0026ndash; cluster_id + unixTimeMS_state_start\nWrite Mode \u0026ndash; Merge\nCosts and state details by cluster at every state in the cluster lifecycle.\nNOTE This fact table is not normalized on time. Some states will span multiple days and must be smoothed across days (i.e. divide by days_in_state) when trying to calculate costs by day. All states are force-terminated at the end of the Overwatch run to the until-timestamp of the run. If the state was still active at this time, it will be updated on the subsequent run.\nColumn Type Description cluster_id string Canonical Databricks cluster ID (more info in Common Meta Fields) cluster_name string Name of cluster at beginning of state custom_tags string JSON string of key/value pairs for all cluster associated custom tags give to the cluster *_state_start various timestamp reference column at the time the state began *_state_end various timestamp reference column at the time the state ended state string state of the cluster \u0026ndash; full list HERE current_num_workers long number of workers in use by the cluster at the start of the state target_num_worers long number of workers targeted to be present by the completion of the state. Should be equal to current_num_workers except during RESIZING state uptime_since_restart_S double Seconds since the cluster was last restarted / terminated uptime_in_state_S double Seconds the cluster spent in current state uptime_in_state_H double Hours the cluster spent in current state driver_node_type_id string KEY of driver node type to enable join to instanceDetails node_type_id string KEY of worker node type to enable join to instanceDetails cloud_billable boolean All current known states are cloud billable. This means that cloud provider charges are present during this state databricks_billable boolean State incurs databricks DBU costs. All states incur DBU costs except: INIT_SCRIPTS_FINISHED, INIT_SCRIPTS_STARTED, STARTING, TERMINATING, CREATING, RESTARTING isAutomated boolean Whether the cluster was created as an \u0026ldquo;automated\u0026rdquo; or \u0026ldquo;interactive\u0026rdquo; cluster dbu_rate double Effective dbu rate used for calculations (effective at time of pipeline run) state_dates array Array of all dates across which the state spanned days_in_state int Number of days in state worker_potential_core_H double Worker core hours available to execute spark tasks core_hours double All core hours of entire cluster (including driver). Nodes * cores * hours in state driver_compute_cost double Compute costs associated with driver runtime driver_dbu_cost double DBU costs associated with driver runtime worker_compute_cost double Compute costs associated with worker runtime worker_dbu_cost double DBU costs associated with cumulative runtime of all worker nodes total_driver_cost double Driver costs including DBUs and compute total_worker_cost double Worker costs including DBUs and compute total_compute_cost double All compute costs for Driver and Workers total_dbu_cost double All dbu costs for Driver and Workers total_cost double Total cost from Compute and DBUs for all nodes (including Driver) driverSpecs struct Driver node details workerSpecs struct Worker node details Cost Functions Explained EXPECTATIONS \u0026ndash; Note that Overwatch costs are derived. This is good and bad. Good as it allows for costs to be broken down by any dimension at the millisecond level. Bad because there can be significant differences between the derived costs and actual costs. These should generally be very close to equal but may differ within margin of error by as much as 10%. To verify the cost functions and the elements therein feel free to review them in more detail. If your costs are off by a large marine, please review all the components of the cost function and correct any configurations as necessary to align your reality with the Overwatch config. The default costs are list price and often do not accurately reflect a customer\u0026rsquo;s costs.\ndriver_compute_cost: when cloudBillable \u0026ndash;\u0026gt; Driver Node Compute Contract Price Hourly (instanceDetails) * Uptime_In_State_H \u0026ndash;\u0026gt; otherwise 0 worker_compute_cost: when cloudBillable \u0026ndash;\u0026gt; Worker Node Compute Contract Price Hourly (instanceDetails) * Uptime_In_State_H * target_num_workers \u0026ndash;\u0026gt; otherwise 0 target_num_workers used here is ambiguous. Assuming all targeted workers can be provisioned, the calculation is most accurate; however, if some workers cannot be provisioned the worker_compute_cost will be slightly higher than actual while target_num_workers \u0026gt; current_num_workers. target_num_workers used here because the compute costs begin accumulating as soon as the node is provisioned, not at the time it is added to the cluster. driver_dbu_cost: when databricks_billable \u0026ndash;\u0026gt; driver_hourly_dbus (instancedetails.hourlyDBUs) * houry_dbu_rate for dbu type (dbuCostDetails.contract_price) * uptime_in_state_H \u0026ndash;\u0026gt; otherwise 0 worker_dbu_cost: when databricks_billable \u0026ndash;\u0026gt; driver_hourly_dbus (instancedetails.hourlyDBUs) * houry_dbu_rate for dbu type (dbuCostDetails.contract_price) * current_num_workers * uptime_in_state_H \u0026ndash;\u0026gt; otherwise 0 current_num_workers used here as dbu costs do not begin until the node able to receive workloads (i.e. node is moved from target_worker to current_worker / \u0026ldquo;upsize_complete\u0026rdquo; state) cloudBillable: Cluster is in a running state GAP: Note that cloud billable ends at the time the cluster is terminated even though the nodes remain provisioned in the cloud provider for several more minutes; these additional minutes are not accounted for in this cost function. InstanceDetails AWS Sample | AZURE_Sample\nKEY \u0026ndash; Organization_ID + API_name\nIncremental Columns \u0026ndash; Pipeline_SnapTS\nPartition Columns \u0026ndash; organization_id\nWrite Mode \u0026ndash; Append\nThis table is unique and it\u0026rsquo;s purpose is to enable users to identify node specific contract costs associated with Databricks and the Cloud Provider through time. Defaults are loaded as an example by workspace. These defaults are meant to be reasonable, not accurate by default as there is a wide difference between cloud discount rates and prices between regions / countries. Everytime Overwatch runs, it validates the presence of this table and whether it has any data present for the current workspace, if it does not it creates and appends the relevant data to it; otherwise no action is taken. This gives the user the ability to extend / customize this table to fit their needs by workspace. Each organization_id (workspace), should provide complete cost data for each node used in that workspace. If you decide to completely customize the table, it\u0026rsquo;s critical to note that some columns are required for the ETL to function; these fields are indicated below in the table with an asterisk.\nThe organization_id (i.e. workspace id) is automatically generated for each workspace if that organization_id is not present in the table already (or the table is not present at all). Each workspace (i.e. organization_id) often has unique costs, this table enables you to customize compute pricing.\nIMPORTANT This table must be configured such that there are no overlapping costs (by time) and no gaps (by time) in costs for any key (organization_id + API_name) between primordial date and current date. This means that for a record to be \u0026ldquo;expired\u0026rdquo; the following must be true:\noriginal key expired by setting activeUntil == expiry date original key must be created with updated information and must: have activeFrom == expiry date of previous record (no gap, no overlap) have activeUntil == lit(null).cast(\u0026ldquo;date\u0026rdquo;) Azure VM Pricing Page\nAWS EC2 Pricing Page\nColumn Type Description instance string Common name of instance type API_name* string Canonical KEY name of the node type \u0026ndash; use this to join to node_ids elsewhere vCPUs* int Number of virtual cpus provisioned for the node type Memory_GB double Gigabyes of memory provisioned for the node type Compute_Contract_Price* double Contract price for the instance type as negotiated between customer and cloud vendor. This is the value used in cost functions to deliver cost estimates. It is defaulted to equal the on_demand compute price On_Demand_Cost_Hourly double On demand, list price for node type DISCLAIMER \u0026ndash; cloud provider pricing is dynamic and this is meant as an initial reference. This value should be validated and updated to reflect actual pricing Linux_Reserved_Cost_Hourly double Reserved, list price for node type DISCLAIMER \u0026ndash; cloud provider pricing is dynamic and this is meant as an initial reference. This value should be validated and updated to reflect actual pricing Hourly_DBUs* double Number of DBUs charged for the node type is_active boolean whether the contract price is currently active. This must be true for each key where activeUntil is null activeFrom* date The start date for the costs in this record. NOTE this MUST be equal to one other record\u0026rsquo;s activeUntil unless this is the first record for these costs. There may be no overlap in time or gaps in time. activeUntil* date The end date for the costs in this record. Must be null to indicate the active record. Only one record can be active at all times. The key (API_name) must have zero gaps and zero overlaps from the Overwatch primordial date until now indicated by null (active) dbuCostDetails KEY \u0026ndash; Organization_ID + sku\nIncremental Columns \u0026ndash; activeFrom\nPartition Columns \u0026ndash; organization_id\nWrite Mode \u0026ndash; Append\nSlow-changing dimension to track DBU contract costs by workspace through time. This table should only need to be edited in very rare circumstances such as historical cost correction. Note that editing these contract prices will not retroactively modify historical pricing in the costing table such as clusterStateFact or jobRunCostPotentialFact. For prices to be recalculated, the gold pipeline modules must be rolled back properly such that the costs can be rebuilt with the updated values.\nColumn Type Description sku string One of automated, interactive, jobsLight, sqlCompute contract_price double Price paid per DBU on the sku is_active boolean whether the contract price is currently active. This must be true for each key where activeUntil is null activeFrom* date The start date for the costs in this record. NOTE this MUST be equal to one other record\u0026rsquo;s activeUntil unless this is the first record for these costs. There may be no overlap in time or gaps in time. activeUntil* date The end date for the costs in this record. Must be null to indicate the active record. Only one record can be active at all times. The key (API_name) must have zero gaps and zero overlaps from the Overwatch primordial date until now indicated by null (active) Job SAMPLE\nKEY \u0026ndash; organization_id + job_id + unixTimeMS + action + request_id\nIncremental Columns \u0026ndash; unixTimeMS\nPartition Columns \u0026ndash; organization_id\nWrite Mode \u0026ndash; Append\nColumn Type Description organization_id string Canonical workspace id job_id string Databricks job id action string Action type defined by the record. One of: create, reset, update, delete, resetJobAcl, changeJobAcl. More information about these actions can be found here timestamp timestamp timestamp the action took place job_name string User defined name of job. NOTE, all jobs created through the UI are initialized with the name, \u0026ldquo;Untitled\u0026rdquo; therefore UI-created-named jobs will have an edit action to set the name. The cluster is also set to automated and defaulted on UI create as well job_type string ?? TBD ?? \u0026ndash; there is a job_task_type but that\u0026rsquo;s in Run_id timeout_seconds string null unless specified, default == null. Timeout seconds specified in UI or via api schedule string JSON - quartz cron expression of scheduled job and timezone_id notebook_path string null if job task does not point to a notebook task. If job points to notebook for execution, this is path to that notebook new_settings dynamic struct job action with \u0026ldquo;reset\u0026rdquo; or \u0026ldquo;update\u0026rdquo; where settings were changed. Includes complex type of cluster. JobSettings Structure Found Here cluster dyanmic struct Where relevant, contains the \u0026ldquo;new_cluster\u0026rdquo; spec when cluster definition is \u0026ldquo;new_cluster\u0026rdquo; or automated. If job definition points to existing cluster the cluster_id can be found here aclPermissionSet string Predefined aclPermissionsSet such as \u0026ldquo;Admin\u0026rdquo; or \u0026ldquo;Owner\u0026rdquo;. More information on these can be found HERE grants string JSON Array of explicit grants given to explicit user list target_user_id string Databricks canonical user id to which the aclPermissionSet is to be applied session_id string session_id that requested the action request_id string request_id of the action user_agent string request origin such as browser, terraform, api, etc. response struct response of api call including errorMessage, result, and statusCode (HTTP 200,400, etc) source_ip_address string Origin IP of action requested JobRun SAMPLE\nKEY \u0026ndash; organization_id + run_id + startEpochMS\nIncremental Columns \u0026ndash; startEpochMS\nPartition Columns \u0026ndash; organization_id\nWrite Mode \u0026ndash; Merge\nInventory of every canonical job run executed in a databricks workspace.\nColumn Type Description organization_id string Canonical workspace id run_id int Incremental Canonical run ID for the workspaced run_name string Name of run (if named) job_runtime struct Complex type with all, standard, runtime information regarding the runtime of the job. The start begins from the moment the job run start is requested (including cluster create time if relevant) and the end time marks the moment the workspace identifies the run as terminated job_id int Canonical ID of job id_in_job int Run instance of the job_id. This can be seen in the UI in a job spec denoted as \u0026ldquo;Run N\u0026rdquo;. Each Job ID has first id_in_job as \u0026ldquo;Run 1\u0026rdquo; and is incrented and canonical ONLY for the job_id. This field omits the \u0026ldquo;Run \u0026quot; prefix and results in an integer value of run instance within job. job_cluster_type string Either new OR existing. New == automated and existing == interactive cluster type job_task_type string Job Task Type - such as Notebook, Python, etc. See JobTask job_terminal_state string Result state of run such as SUCCESS, FAILED, TIMEDOUT, CANCELLED. See RunResultState job_trigger_type string Type of trigger: PERIODIC, ONE_TIME, RETRY. See TriggerType cluster_id string Canonical workspace cluster id notebook_params string JSON A map of (String, String) parameters sent to notebook parameter overrides. See ParamPair libraries string JSON Array of Libraries in the format defined HERE children array Array of structs that show all children of this job workflow_context string ?? REVIEW ?? task_detail struct Unified location for JobTask contingent upon jobrun task. See JobTask cancellation_detail struct All cancellation request detail and status time_detail struct All time dimensions tied to the run. runBeginTime == Time the run began to execute on the cluster, SubmissionTime == Time the run request was received by the endpoint (before cluster build/start), completionTime == time the workspace denoted the runID was terminal state started_by struct Run request received from user \u0026ndash; email is recorded here \u0026ndash; usage == started_by.email request_detail struct Complete request detail received by the endpoint JobRunCostPotentialFact SAMPLE\nKEY \u0026ndash; organization_id + run_id + startEpochMS\nIncremental Columns \u0026ndash; startEpochMS\nPartition Columns \u0026ndash; organization_id\nWrite Mode \u0026ndash; Merge\nThis fact table defines the job, the cluster, the cost, the potential, and utilization (if cluster logging is enabled) of a cluster associated with a specific Databricks Job Run.\nDimensionality Note that this fact table is not normalized by time but rather by job run and cluster state. Costs are not derived from job runs but from clusters thus the state[s] of the cluster are what\u0026rsquo;s pertinent when tying to cost. This is extremely important in the case of long running jobs, such as streaming.\nSCENARIO: Imagine a streaming job with 12 concurrent runs on an existing cluster that run for 20 days at the end of which the driver dies for some reason causing all runs fail and begin retrying but failing. When the 20 days end, the cost will be captured solely on that date and even more importantly, not only will all 20 days be captured at that date but the cost associated will be cluster runtime for 20 days * number of runs. Overwatch will automatically smooth the costs across the concurrent runs but not the days running since this fact table is not based by on an equidistant time axis.\nPotential: Total core_milliseconds for which the cluster COULD execute spark tasks. This derivation only includes the worker nodes in a state ready to receive spark tasks (i.e. Running). Nodes being added or running init scripts are not ready for spark jobs thus those core milliseconds are omitted from the total potential. Cost: Derived from the instanceDetails table and DBU configured contract price (see Configuration for more details). The compute costs in instanceDetails table are taken from the \u0026ldquo;Compute_Contract_Price\u0026rdquo; values associated with the instance type in instanceDetails. Utilization: Utilization is a function of core milliseconds used during spark task execution divided by the total amount of core milliseconds available given the cluster size and state. (i.e. spark_task_runtime_H / worker_potential_core_H) Cluster State: The state[s] of a cluster during a run. As the cluster scales and morphs to accommodate the run\u0026rsquo;s needs, the state changes. The number of state changes are recorded in this table as \u0026ldquo;run_cluster_states\u0026rdquo;. Run State: Advanced Topic for data engineers and developers. This topic is discussed in considerable detail in the Advanced Topics section. Given a cluster state, the run state is a state of all runs on a cluster at a given moment in time. This is the measure used to calculate shared costs across concurrent runs. A run state cannot pass the boundaries of a cluster state, a run that continues across cluster-state lines will result in a new run state. Column Type Description organization_id string Canonical workspace id job_id long Canonical ID of job id_in_job long Run instance of the job_id. This can be seen in the UI in a job spec denoted as \u0026ldquo;Run N\u0026rdquo;. Each Job ID has first id_in_job as \u0026ldquo;Run 1\u0026rdquo; and is incrented and canonical ONLY for the job_id. This field omits the \u0026ldquo;Run \u0026quot; prefix and results in an integer value of run instance within job. job_runtime struct Time details of job start/end in epoch millis and timestamps cluster_id string Canonical workspace cluster id cluster_name string Name of cluster at time of run cluster_type string Either new OR existing. New == automated and existing == interactive cluster type custom_tags string JSON string of key/value pairs for all cluster associated custom tags give to the cluster run_terminal_state string Final state of the job such as \u0026ldquo;Succeeded\u0026rdquo;, \u0026ldquo;Failed\u0026rdquo; or \u0026ldquo;Cancelled\u0026rdquo; run_trigger_type string How the run was triggered (i.e. cron / manual) run_task_type string Type of task in the job (i.e. notebook, jar, spark-submit, etc.) driver_node_type_id string KEY of driver node type to enable join to instanceDetails node_type_id string KEY of worker node type to enable join to instanceDetails dbu_rate double Effective DBU rate at time of job run used for calculations based on configured contract price in instanceDetails at the time of the Overwatch Pipeline Run running_days array Array (or list) of dates (not strings) across which the job run executed. This simplifies day-level cost attribution, among other metrics, when trying to smooth costs for long-running / streaming jobs avg_cluster_share double Average share of the cluster the run had available assuming fair scheduling. This DOES NOT account for activity outside of jobs (i.e. interactive notebooks running alongside job runs), this measure only splits out the share among concurrent job runs. Measure is only calculated for interactive clusters, automated clusters assume 100% run allocation. For more granular utilization detail, enable cluster logging and utilize \u0026ldquo;job_run_cluster_util\u0026rdquo; column which derives utilization at the spark task level. avg_overlapping_runs double Number of concurrent runs shared by the cluster on average throughout the run max_overlapping_runs long Highest number of concurrent runs on the cluster during the run run_cluster_states long Count of cluster states during the job run worker_potential_core_H double cluster core hours capable of executing spark tasks, \u0026ldquo;potential\u0026rdquo; driver_compute_cost double Compute costs associated with driver runtime driver_dbu_cost double DBU costs associated with driver runtime worker_compute_cost double Compute costs associated with worker runtime worker_dbu_cost double DBU costs associated with cumulative runtime of all worker nodes total_driver_cost double Driver costs including DBUs and compute total_worker_cost double Worker costs including DBUs and compute total_compute_cost double All compute costs for Driver and Workers total_dbu_cost double All dbu costs for Driver and Workers total_cost double Total cost from Compute and DBUs for all nodes (including Driver) spark_task_runtimeMS long Spark core execution time in milliseconds (i.e. task was operating/locking on core) spark_task_runtime_H double Spark core execution time in Hours (i.e. task was operating/locking on core) job_run_cluster_util double Cluster utilization: spark task execution time / cluster potential. True measure by core of utilization. Only available when cluster logging is enabled. Notebook SAMPLE\nKEY \u0026ndash; organization_id + notebook_id + request_id + action + unixTimeMS\nIncremental Columns \u0026ndash; unixTimeMS\nPartition Columns \u0026ndash; organization_id\nWrite Mode \u0026ndash; Append\nColumn Type Description notebook_id string Canonical notebook id notebook_name string Name of notebook at time of action requested notebook_path string Path of notebook at time of action requested cluster_id string Canonical workspace cluster id action string action recorded timestamp timestamp timestamp the action took place old_name string When action is \u0026ldquo;renameNotebook\u0026rdquo; this holds notebook name before rename old_path string When action is \u0026ldquo;moveNotebook\u0026rdquo; this holds notebook path before move new_name string When action is \u0026ldquo;renameNotebook\u0026rdquo; this holds notebook name after rename new_path string When action is \u0026ldquo;moveNotebook\u0026rdquo; this holds notebook path after move parent_path string When action is \u0026ldquo;renameNotebook\u0026rdquo; notebook containing, workspace path is recorded here user_email string Email of the user requesting the action request_id string Canonical request_id response struct HTTP response including errorMessage, result, and statusCode InstancePool KEY \u0026ndash; organization_id + instance_pool_id + timestamp\nIncremental Columns \u0026ndash; timestamp\nPartition Columns \u0026ndash; organization_id\nWrite Mode \u0026ndash; Merge\nColumn Type Description instance_pool_id string Canonical notebook id instance_pool_name string Name of notebook at time of action requested actionName string action recorded timestamp long timestamp the action took place node_type_id string Type of node in the pool idle_instance_autotermination_minutes long Minutes after which a node shall be terminated if unused min_idle_instances long Minimum number of hot instances in the pool max_capacity long Maximum number of nodes allowed in the pool preloaded_spark_versions string Spark versions preloaded on nodes in the pool Account Tables Not exposed in the consumer database. These tables contain more sensitive information and by default are not exposed in the consumer database but held back in the ETL database. This is done purposely to simplify security when/if desired. If desired, this can be exposed in consumer database with a simple vew definition exposing the columns desired.\nFor deeper insights regarding audit, please reference auditLogSchema. This is simplified through the use of the ETL_DB.audit_log_bronze and filter where serviceName == accounts for example. Additionally, you may filter down to specific actions using \u0026ldquo;actionName\u0026rdquo;. An example query is provided below:\nspark.table(\u0026#34;overwatch.audit_log_bronze\u0026#34;) .filter(\u0026#39;serviceName === \u0026#34;accounts\u0026#34; \u0026amp;\u0026amp; \u0026#39;actionName === \u0026#34;createGroup\u0026#34;) .selectExpr(\u0026#34;*\u0026#34;, \u0026#34;requestParams.*\u0026#34;).drop(\u0026#34;requestParams\u0026#34;) Slow changing dimension of user entity through time. Also used as reference map from user_email to user_id\nColumn Type Description organization_id string Canonical workspace id user_id string Canonical user id for which the action was requested (within the workspace) (target) user_email string User\u0026rsquo;s email for which the action was requested (target) action string Action requested to be performed added_from_ip_address string Source IP of the request added_by string Authenticated user that made the request user_agent string request origin such as browser, terraform, api, etc. AccountMod SAMPLE\nKEY \u0026ndash; organization_id + acton + mod_unixTimeMS + request_id\nIncremental Columns \u0026ndash; mod_unixTimeMS\nPartition Columns \u0026ndash; organization_id\nWrite Mode \u0026ndash; Append\nTODO\nAccountLogin SAMPLE\nKEY \u0026ndash; organization_id + login_type + login_unixTimeMS + from_ip_address\nIncremental Columns \u0026ndash; login_unixTimeMS\nPartition Columns \u0026ndash; organization_id\nWrite Mode \u0026ndash; Append\nNot exposed in the consumer database. This table contains more sensitive information and by default is not exposed in the consumer database but held back in the etl datbase. This is done purposely to simplify security when/if desired. If desired, this can be exposed in consumer database with a simple vew definition exposing the columns desired.\nColumn Type Description user_id string Canonical user id (within the workspace) user_email string User\u0026rsquo;s email login_type string Type of login such as web, ssh, token ssh_username string username used to login via SSH groups_user_name string ?? To research ?? account_admin_userID string ?? To research ?? login_from_ip_address struct Details about the source login and target logged into user_agent string request origin such as browser, terraform, api, etc. The following sections are related to Spark. Everything that can be seend/found in the SparkUI is visibel in the spark tables below. A reasonable understanding of the Spark hierarchy is necessary to make this section simpler. Please reference Spark Hierarchy For More Details for more details.\nSparkExecution SAMPLE\nKEY \u0026ndash; organization_id + spark_context_id + execution_id + date + unixTimeMS\nIncremental Columns \u0026ndash; date + unixTimeMS\nPartition Columns \u0026ndash; organization_id\nWrite Mode \u0026ndash; Merge\nColumn Type Description organization_id string Canonical workspace id spark_context_id string Canonical context ID \u0026ndash; One Spark Context per Cluster cluster_id string Canonical workspace cluster id execution_id long Spark Execution ID description string Description provided by spark details string Execution StackTrace sql_execution_runtime struct Complete runtime detail breakdown SparkExecutor SAMPLE\nKEY \u0026ndash; organization_id + spark_context_id + executor_id + date + unixTimeMS\nIncremental Columns \u0026ndash; date + unixTimeMS\nPartition Columns \u0026ndash; organization_id\nWrite Mode \u0026ndash; Merge\nColumn Type Description organization_id string Canonical workspace id spark_context_id string Canonical context ID \u0026ndash; One Spark Context per Cluster cluster_id string Canonical workspace cluster id executor_id int Executor ID executor_info string Executor Detail removed_reason string Reason executor was removed executor_alivetime struct Complete lifetime detail breakdown SparkJob SAMPLE\nKEY \u0026ndash; organization_id + spark_context_id + job_id + unixTimeMS\nIncremental Columns \u0026ndash; date + unixTimeMS\nPartition Columns \u0026ndash; organization_id + date\nZ-Order Columns \u0026ndash; cluster_id\nWrite Mode \u0026ndash; Merge\nColumn Type Description organization_id string Canonical workspace id spark_context_id string Canonical context ID \u0026ndash; One Spark Context per Cluster cluster_id string Canonical workspace cluster id job_id string Spark Job ID job_group_id string Spark Job Group ID \u0026ndash; NOTE very powerful for many reasons. See SparkEvents execution_id string Spark Execution ID stage_ids array[long] Array of all Spark Stage IDs nested within this Spark Job notebook_id string Canonical Databricks Workspace Notebook ID notebook_path string Databricks Notebook Path user_email string email of user that owned the request, for Databricks jobs this will be the job owner db_job_id string Databricks Job Id executing the Spark Job db_id_in_job string \u0026ldquo;id_in_job\u0026rdquo; such as \u0026ldquo;Run 10\u0026rdquo; without \u0026ldquo;Run \u0026quot; prefix. This is a critical join column when working looking up Databricks Jobs metadata job_runtime string Complete job runtime detail breakdown job_result struct Job Result and Exception if present SparkStage SAMPLE\nKEY \u0026ndash; organization_id + spark_context_id + stage_id + stage_attempt_id + unixTimeMS\nIncremental Columns \u0026ndash; date + unixTimeMS\nPartition Columns \u0026ndash; organization_id + date\nZ-Order Columns \u0026ndash; cluster_id\nWrite Mode \u0026ndash; Merge\nColumn Type Description organization_id string Canonical workspace id spark_context_id string Canonical context ID \u0026ndash; One Spark Context per Cluster cluster_id string Canonical workspace cluster id stage_id string Spark Stage ID stage_attempt_id string Spark Stage Attempt ID stage_runtime string Complete stage runtime detail stage_info string Lineage of all accumulables for the Spark Stage SparkTask SAMPLE\nKEY \u0026ndash; organization_id + spark_context_id + task_id + task_attempt_id + stage_id + stage_attempt_id + host + unixTimeMS\nIncremental Columns \u0026ndash; date + unixTimeMS\nPartition Columns \u0026ndash; organization_id + date\nZ-Order Columns \u0026ndash; cluster_id\nWrite Mode \u0026ndash; Merge\nUSE THE PARTITION COLUMN (date) and Indexed Column (cluster_id) in all joins and filters where possible. This table can get extremely large, select samples or smaller date ranges and reduce joins and columns selected to improve performance.\nColumn Type Description organization_id string Canonical workspace id workspace_name string Customizable human-legible name of the workspace, should be globally unique within the organization spark_context_id string Canonical context ID \u0026ndash; One Spark Context per Cluster cluster_id string Canonical workspace cluster id task_id string Spark Task ID task_attempt_id string Spark Task Attempt ID stage_id string Spark Stage ID stage_attempt_id string Spark Stage Attempt ID executor_id string Spark Executor ID host string Internal IP address of node task_runtime string Complete task runtime detail task_metrics string Lowest level compute metrics provided by spark such as spill bytes, read/write bytes, shuffle info, GC time, Serialization, etc. task_info string Lineage of all accumulables for the Spark Task task_type string Spark task Type (i.e. ResultTask, ShuffleMapTask, etc) task_end_reason string Task end status, state, and details plus stake trace when error SparkStream_preview KEY \u0026ndash; organization_id + spark_context_id + cluster_id + stream_id + stream_run_id + stream_batch_id + stream_timestamp\nIncremental Columns \u0026ndash; date + stream_timestamp\nPartition Columns \u0026ndash; organization_id + date\nZ-Order Columns \u0026ndash; cluster_id\nWrite Mode \u0026ndash; Merge\nRemains in preview through version 0.6.0 as more feedback is requested from users and use-cases before this table structure solidifes.\nColumn Type Description spark_context_id string Canonical context ID \u0026ndash; One Spark Context per Cluster cluster_id string Canonical workspace cluster id stream_id string GUID ID of the spark stream stream_name string Name of stream if named stream_run_id string GUID ID of the spark stream run stream_batch_id long GUID ID of the spark stream run batch stream_timestamp long Unix time (millis) the stream reported its batch complete metrics streamSegment string Type of event from the event listener such as \u0026lsquo;Progressed\u0026rsquo; streaming_metrics dynamic struct All metrics available for the stream batch run execution_ids array Array of execution_ids in the spark_context. Can explode and tie back to sparkExecution and other spark tables Common Meta Fields Column Type Description organization_id string Workspace / Organization ID on which the cluster was instantiated cluster_id string Canonical workspace cluster id unixTimeMS long unix time epoch as a long in milliseconds timestamp string unixTimeMS as a timestamp type in milliseconds date string unixTimeMS as a date type created_by string last_edited_by string last user to edit the state of the entity last_edited_ts string timestamp at which the entitiy\u0026rsquo;s sated was last edited deleted_by string user that deleted the entity deleted_ts string timestamp at which the entity was deleted event_log_start string Spark Event Log BEGIN file name / path event_log_end string Spark Event Log END file name / path Pipeline_SnapTS string Snapshot timestmap of Overwatch run that added the record Overwatch_RunID string Overwatch canonical ID that resulted in the record load ETL Tables The following are the list of potential tables, the module with which it\u0026rsquo;s created and the layer in which it lives. This list consists of only the ETL tables created to facilitate and deliver the consumption layer The gold and consumption layers are the only layers that maintain column name uniformity and naming convention across all tables. Users should always reference Consumption and Gold layers unless the data necessary has not been curated.\nBronze Table Scope Layer Description audit_log_bronze audit bronze Raw audit log data full schema audit_log_raw_events audit bronze (azure) Intermediate staging table responsible for coordinating intermediate events from azure Event Hub cluster_events_bronze clusterEvents bronze Raw landing of dataframe derived from JSON response from cluster events api call. Note: cluster events expire after 30 days of last termination. (reference) clusters_snapshot_bronze clusters bronze API snapshot of existing clusters defined in Databricks workspace at the time of the Overwatch run. Snapshot is taken on each run jobs_snapshot_bronze jobs bronze API snapshot of existing jobs defined in Databricks workspace at the time of the Overwatch run. Snapshot is taken on each run pools_snapshot_bronze pools bronze API snapshot of existing pools defined in Databricks workspace at the time of the Overwatch run. Snapshot is taken on each run spark_events_bronze sparkEvents bronze Raw landing of the master sparkEvents schema and data for all cluster logs. Cluster log locations are defined by cluster specs and all locations will be scanned for new files not yet captured by Overwatch. Overwatch uses an implicit schema generation here, as such, a lack of real-world can cause unforeseen issues. spark_events_processedfiles sparkEvents bronze Table that keeps track of all previously processed cluster log files (spark event logs) to minimize future file scanning and improve performance. This table can be used to reprocess and/or find specific eventLog files. pipeline_report NA tracking Tracking table used to identify state and status of each Overwatch Pipeline run. This table is also used to control the start and end points of each run. Altering the timestamps and status of this table will change the ETL start/end points. Silver Table Scope Layer Description account_login_silver accounts silver Login events account_mods_silver accounts silver Account modification events cluster_spec_silver clusters silver Slow changing dimension used to track all clusters through time including edits but excluding state change. cluster_state_detail_silver clusterEvents silver State detail for each cluster event enriched with cost information job_status_silver jobs silver Slow changing dimension used to track all jobs specifications through time jobrun_silver jobs silver Historical run of every job since Overwatch began capturing the audit_log_data notebook_silver notebooks silver Slow changing dimension used to track all notebook changes as it morphs through time along with which user instigated the change. This does not include specific change details of the commands within a notebook just metadata changes regarding the notebook. pools_silver pools silver Slow changing dimension used to track all changes to instance pools spark_executions_silver sparkEvents silver All spark event data relevant to spark executions spark_executors_silver sparkEvents silver All spark event data relevant to spark executors spark_jobs_silver sparkEvents silver All spark event data relevant to spark jobs spark_stages_silver sparkEvents silver All spark event data relevant to spark stages spark_tasks_silver sparkEvents silver All spark event data relevant to spark tasks Gold Table Scope Layer Description account_login_gold accounts gold Login events account_mods_gold accounts gold Account modification events cluster_gold clusters gold Slow-changing dimension with all cluster creates and edits through time. These events DO NOT INCLUDE automated cluster resize events or cluster state changes. Automated cluster resize and cluster state changes will be in clusterstatefact_gold. If user changes min/max nodes or node count (non-autoscaling) the event will be registered here AND clusterstatefact_gold. clusterStateFact_gold clusterEvents gold All cluster event changes along with the time spent in each state and the core hours in each state. This table should be used to find cluster anomalies and/or calculate compute/DBU costs of some given scope. job_gold jobs gold Slow-changing dimension of all changes to a job definition through time jobrun_gold jobs gold Dimensional data for each job run in the databricks workspace notebook_gold notebooks gold Slow changing dimension used to track all notebook changes as it morphs through time along with which user instigated the change. This does not include specific change details of the commands within a notebook just metadata changes regarding the notebook. instancepool_gold pools gold Slow changing dimension used to track all changes to instance pools sparkexecution_gold sparkEvents gold All spark event data relevant to spark executions sparkexecutor_gold sparkEvents gold All spark event data relevant to spark executors sparkjob_gold sparkEvents gold All spark event data relevant to spark jobs sparkstage_gold sparkEvents gold All spark event data relevant to spark stages sparktask_gold sparkEvents gold All spark event data relevant to spark tasks sparkstream_gold sparkEvents gold All spark event data relevant to spark streams "
},
{
	"uri": "http://localhost:1313/overwatch/dataengineer/definitions/071x/",
	"title": "Data Dictionary - 0.7.1.x",
	"tags": [],
	"description": "",
	"content": "ERD The \u0026ldquo;ERD\u0026rdquo; below is a visual representation of the consumer layer data model. Many of the joinable lines have been omitted to reduce chaos and complexity in the visualization. All columns with the same name are joinable (even if there\u0026rsquo;s not a line from one table to the other). The relations depicted are to call the analyst\u0026rsquo;s attention to less obvious joins.\nThe goal is to present a data model that unifies the different parts of the platform. The Overwatch team will continue to work with Databricks platform teams to publish and simplify this data. The gray boxes annotated as \u0026ldquo;Backlog/Research\u0026rdquo; are simply a known gap and a pursuit of the Overwatch dev team, it does NOT mean it\u0026rsquo;s going to be released soon but rather that we are aware of the missing component and we hope to enable gold-level data here in the future.\nConsumption Layer \u0026ldquo;Tables\u0026rdquo; (Views) All end users should be hitting consumer tables first. Digging into lower layers gets significantly more complex. Below is the data model for the consumption layer. The consumption layer is often in a stand-alone database apart from the ETL tables to minimize clutter and confusion. These entities in this layer are actually not tables at all (with a few minor exceptions such as lookup tables) but rather views. This allows for the Overwatch development team to alter the underlying columns, names, types, and structures without breaking existing transformations. Instead, view column names will remain the same but may be repointed to a newer version of a column, etc.\nETL should not be developed atop the consumption layer views but rather the gold layer. Before Overwatch version upgrades, it\u0026rsquo;s important that the engineering team review the change list and upgrade requirements before upgrading. These upgrades may require a remap depending on the changes. As of version 1.0 release, all columns in the gold layer will be underscored with their schema version number, column changes will reference the later release version but the views published with Overwatch will almost always point to the latest version of each column and will not include the schema suffix to simplify the data model for the average consumer.\nData Organization The large gray boxes in the simplified ERD below depict the two major, logical sections of the data model:\nDatabricks Platform - Metadata captured by the Databricks platform that can be used to assist in workspace governance. This data can also be enriched with the Spark data enabling in-depth analyses. The breadth of metadata is continuing to grow, stay tuned for additional capabilities. Spark UI The spark UI section is derived from the spark event logs and essentially contains every single piece of data from the Spark UI. There are a few sections that are not included in the first release but the data is present in spark_events_bronze albeit extremely complex to derive. The Overwatch development team is working tirelessly to expose additional SparkUI data and will publish as soon as it\u0026rsquo;s ready. Column Descriptions Complete column descriptions are only provided for the consumption layer. The entity names are linked below.\ncluster clusterStateFact instanceDetails job jobrun jobRunCostPotentialFact sqlQueryHistory notebook instancePool dbuCostDetail accountLogin accountMod sparkExecution sparkExecutor sparkJob sparkStage sparkTask sparkStream warehouse Common Meta Fields There are several fields that are present in all tables. Instead of cluttering each table with them, this section was created as a reference to each of these. Most tables below provide a data SAMPLE for reference. You may either click to view it or right click the SAMPLE link and click saveTargetAs or saveLinkAs and save the file. Note that these files are TAB delimited, so you will need to view as such if you save to local file. The data in the files were generated from an Azure, test deployment created by Overwatch Developers.\nCluster SAMPLE\nKEY \u0026ndash; organization_id + cluster_id + unixTimeMS\nIncremental Columns \u0026ndash; unixTimeMS\nPartition Columns \u0026ndash; organization_id\nWrite Mode \u0026ndash; Append\nColumn Type Description cluster_id string Canonical Databricks cluster ID (more info in Common Meta Fields) action string create, edit, or snapImpute \u0026ndash; depicts the type of action for the cluster \u0026ndash; **snapImpute is used on first run to initialize the state of the cluster even if it wasn\u0026rsquo;t created/edited since audit logs began timestamp timestamp timestamp the action took place cluster_name string user-defined name of the cluster driver_node_type string Canonical name of the driver node type. node_type string Canonical name of the worker node type. num_workers int The number of workers defined WHEN autoscaling is disabled autoscale struct The min/max workers defined WHEN autoscaling is enabled auto_termination_minutes int The number of minutes before the cluster auto-terminates due to inactivity enable_elastic_disk boolean Whether autoscaling disk was enabled or not is_automated booelan Whether the cluster is automated (true if automated false if interactive) cluster_type string Type of cluster (i.e. Serverless, SQL Analytics, Single Node, Standard) security_profile struct Complex type to describe secrity features enabled on the cluster. More information Below cluster_log_conf string Logging directory if configured init_script array Array of init scripts custom_tags string User-Defined tags AND also includes Databricks JobID and Databricks RunName when the cluster is created by a Databricks Job as an automated cluster. Other Databricks services that create clusters also store unique information here such as SqlEndpointID when a cluster is created by \u0026ldquo;SqlAnalytics\u0026rdquo; cluster_source string Shows the source of the action **(TODO \u0026ndash; checking on why null scenario with BUI) spark_env_vars string Spark environment variables defined on the cluster spark_conf string custom spark configuration on the cluster that deviate from default acl_path_prefix string Automated jobs pass acl to clusters via a path format, the path is defined here instance_pool_id string Canononical pool id from which workers receive nodes driver_instance_pool_id string Canononical pool id from which driver receives node instance_pool_name string Name of pool from which workers receive nodes driver_instance_pool_name string Name of pool from which driver receives node spark_version string DBR version - scala version idempotency_token string Idempotent jobs token if used ClusterStateFact SAMPLE\nKEY \u0026ndash; organization_id + cluster_id + state + unixTimeMS_state_start\nIncremental Columns \u0026ndash; state_start_date + unixTimeMS\nPartition Columns \u0026ndash; organization_id + state_start_date\nZ-Order Columns \u0026ndash; cluster_id + unixTimeMS_state_start\nWrite Mode \u0026ndash; Merge\nUnsupported Scenarios A few scenarios are not yet supported by Overwatch; they are called out here. Please stay tuned for updates as it\u0026rsquo;s our intention to include everything we can as soon as possible after Databricks product GAs new features but there will be a delay.\nCosts for DBSQL clusters Even though DBSQL warehouses may show up here (they normally will not), the costs for these cannot be calculated at this time. Databricks doesn\u0026rsquo;t yet publish the warehouse event logs (i.e. start/stop/scale) and until that is available, we cannot estimate costs for warehouses like we do for traditional clusters. Costs and state details by cluster at every state in the cluster lifecycle. The Cost Functions are detailed below the definitions of this table.\nAny static clusters spanning 90 days without any state changes will never get a state closure and result in costs increasing forever. This should be a VERY rare circumstance and usually only happens in extreemely stable, small streams. This max days for clsf will be externalized as an override config in the future but for now it\u0026rsquo;s static.\nThis fact table is not normalized on time. Some states will span multiple days and must be smoothed across days (i.e. divide by days_in_state) when trying to calculate costs by day. All states are force-terminated at the end of the Overwatch run to the until-timestamp of the run. If the state was still active at this time, it will be updated on the subsequent run.\nColumn Type Description cluster_id string Canonical Databricks cluster ID (more info in Common Meta Fields) cluster_name string Name of cluster at beginning of state custom_tags string JSON string of key/value pairs for all cluster associated custom tags give to the cluster *_state_start various timestamp reference column at the time the state began *_state_end various timestamp reference column at the time the state ended state string state of the cluster \u0026ndash; full list HERE current_num_workers long number of workers in use by the cluster at the start of the state target_num_workers long number of workers targeted to be present by the completion of the state. Should be equal to current_num_workers except during RESIZING state uptime_since_restart_S double Seconds since the cluster was last restarted / terminated uptime_in_state_S double Seconds the cluster spent in current state uptime_in_state_H double Hours the cluster spent in current state driver_node_type_id string KEY of driver node type to enable join to instanceDetails node_type_id string KEY of worker node type to enable join to instanceDetails cloud_billable boolean All current known states are cloud billable. This means that cloud provider charges are present during this state databricks_billable boolean State incurs databricks DBU costs. All states incur DBU costs except: INIT_SCRIPTS_FINISHED, INIT_SCRIPTS_STARTED, STARTING, TERMINATING, CREATING, RESTARTING isAutomated boolean Whether the cluster was created as an \u0026ldquo;automated\u0026rdquo; or \u0026ldquo;interactive\u0026rdquo; cluster dbu_rate double Effective dbu rate used for calculations (effective at time of pipeline run) excluding dbu increases due to photon \u0026ndash; photon uplifts included in dbu_totals runtime_engine string One of STANDARD or PHOTON. When PHOTON, pricing is adjusted when deriving the dbu_costs state_dates array Array of all dates across which the state spanned days_in_state int Number of days in state worker_potential_core_H double Worker core hours available to execute spark tasks core_hours double All core hours of entire cluster (including driver). Nodes * cores * hours in state driver_compute_cost double Compute costs associated with driver runtime driver_dbu_cost double DBU costs associated with driver runtime worker_compute_cost double Compute costs associated with worker runtime worker_dbu_cost double DBU costs associated with cumulative runtime of all worker nodes total_driver_cost double Driver costs including DBUs and compute total_worker_cost double Worker costs including DBUs and compute total_compute_cost double All compute costs for Driver and Workers total_dbu_cost double All dbu costs for Driver and Workers total_cost double Total cost from Compute and DBUs for all nodes (including Driver) driverSpecs struct Driver node details workerSpecs struct Worker node details Cost Functions Explained EXPECTATIONS \u0026ndash; Note that Overwatch costs are derived. This is good and bad. Good as it allows for costs to be broken down by any dimension at the millisecond level. Bad because there can be significant differences between the derived costs and actual costs. These should generally be very close to equal but may differ within margin of error by as much as 10%. To verify the cost functions and the elements therein feel free to review them in more detail. If your costs are off by a large margin, please review all the components of the cost function and correct any configurations as necessary to align your reality with the Overwatch config. The default costs are list price and often do not accurately reflect a customer\u0026rsquo;s costs.\ncloudBillable: Cluster is in a running state GAP: Note that cloud billable ends at the time the cluster is terminated even though the nodes remain provisioned in the cloud provider for several more minutes; these additional minutes are not accounted for in this cost function. driver_compute_cost: when cloudBillable \u0026ndash;\u0026gt; Driver Node Compute Contract Price Hourly (instanceDetails) * Uptime_In_State_H \u0026ndash;\u0026gt; otherwise 0 worker_compute_cost: when cloudBillable \u0026ndash;\u0026gt; Worker Node Compute Contract Price Hourly (instanceDetails) * Uptime_In_State_H * target_num_workers \u0026ndash;\u0026gt; otherwise 0 target_num_workers used here is ambiguous. Assuming all targeted workers can be provisioned, the calculation is most accurate; however, if some workers cannot be provisioned the worker_compute_cost will be slightly higher than actual while target_num_workers \u0026gt; current_num_workers. target_num_workers used here because the compute costs begin accumulating as soon as the node is provisioned, not at the time it is added to the cluster. photon_kicker: WHEN runtime_engine == \u0026ldquo;Photon\u0026rdquo; and sku != \u0026ldquo;SqlCompute\u0026rdquo; and isAutomated THEN 2.9 WHEN runtime_engine == \u0026ldquo;Photon\u0026rdquo; and sku != \u0026ldquo;SqlCompute\u0026rdquo; and !isAutomated THEN 2 otherwise 1 worker_dbus: WHEN databricks_billable and !SingleNode THEN current_num_workers * driver_hourly_dbus (instancedetails.hourlyDBUs) * uptime_in_state_H * photon_kicker NOTE: current_num_workers == current_worker, not target. current_num_workers only includes worker nodes after they have become ready and able to receive workloads. otherwise 0 driver_dbus: when databricks_billable \u0026ndash;\u0026gt; driver_hourly_dbus (instancedetails.hourlyDBUs) * uptime_in_state_H * photon_kicker \u0026ndash;\u0026gt; otherwise 0 NOTE: In Single Node Clusters \u0026ndash; only the driver will have dbus worker_dbu_cost: houry_dbu_rate for sku (dbuCostDetails.contract_price) * worker_dbus driver_dbu_cost: houry_dbu_rate for sku (dbuCostDetails.contract_price) * driver_dbus Cost may not appear for a cluster until a state change is observed (i.e. starting/terminating/expanded_disk/resizing/etc). This means that Overwatch may not recognize costs for a cluster until at least one state change has been observed by Overwatch since the primordial date (or first run date - 30d whichever is greater).\nInstanceDetails AWS Sample | AZURE_Sample\nKEY \u0026ndash; Organization_ID + API_name\nIncremental Columns \u0026ndash; Pipeline_SnapTS\nPartition Columns \u0026ndash; organization_id\nWrite Mode \u0026ndash; Append\nThis table is unique and it\u0026rsquo;s purpose is to enable users to identify node specific contract costs associated with Databricks and the Cloud Provider through time. Defaults are loaded as an example by workspace. These defaults are meant to be reasonable, not accurate by default as there is a wide difference between cloud discount rates and prices between regions / countries. Everytime Overwatch runs, it validates the presence of this table and whether it has any data present for the current workspace, if it does not it creates and appends the relevant data to it; otherwise no action is taken. This gives the user the ability to extend / customize this table to fit their needs by workspace. Each organization_id (workspace), should provide complete cost data for each node used in that workspace. If you decide to completely customize the table, it\u0026rsquo;s critical to note that some columns are required for the ETL to function; these fields are indicated below in the table with an asterisk.\nThe organization_id (i.e. workspace id) is automatically generated for each workspace if that organization_id is not present in the table already (or the table is not present at all). Each workspace (i.e. organization_id) often has unique costs, this table enables you to customize compute pricing.\nIMPORTANT This table must be configured such that there are no overlapping costs (by time) and no gaps (by time) in costs for any key (organization_id + API_name) between primordial date and current date. This means that for a record to be \u0026ldquo;expired\u0026rdquo; the following must be true:\noriginal key expired by setting activeUntil == expiry date original key must be created with updated information and must: have activeFrom == expiry date of previous record (no gap, no overlap) have activeUntil == lit(null).cast(\u0026ldquo;date\u0026rdquo;) Azure VM Pricing Page\nAWS EC2 Pricing Page\nColumn Type Description instance string Common name of instance type API_name* string Canonical KEY name of the node type \u0026ndash; use this to join to node_ids elsewhere vCPUs* int Number of virtual cpus provisioned for the node type Memory_GB double Gigabyes of memory provisioned for the node type Compute_Contract_Price* double Contract price for the instance type as negotiated between customer and cloud vendor. This is the value used in cost functions to deliver cost estimates. It is defaulted to equal the on_demand compute price On_Demand_Cost_Hourly double On demand, list price for node type DISCLAIMER \u0026ndash; cloud provider pricing is dynamic and this is meant as an initial reference. This value should be validated and updated to reflect actual pricing Linux_Reserved_Cost_Hourly double Reserved, list price for node type DISCLAIMER \u0026ndash; cloud provider pricing is dynamic and this is meant as an initial reference. This value should be validated and updated to reflect actual pricing Hourly_DBUs* double Number of DBUs charged for the node type is_active boolean whether the contract price is currently active. This must be true for each key where activeUntil is null activeFrom* date The start date for the costs in this record. NOTE this MUST be equal to one other record\u0026rsquo;s activeUntil unless this is the first record for these costs. There may be no overlap in time or gaps in time. activeUntil* date The end date for the costs in this record. Must be null to indicate the active record. Only one record can be active at all times. The key (API_name) must have zero gaps and zero overlaps from the Overwatch primordial date until now indicated by null (active) dbuCostDetails KEY \u0026ndash; Organization_ID + sku\nIncremental Columns \u0026ndash; activeFrom\nPartition Columns \u0026ndash; organization_id\nWrite Mode \u0026ndash; Append\nSlow-changing dimension to track DBU contract costs by workspace through time. This table should only need to be edited in very rare circumstances such as historical cost correction. Note that editing these contract prices will not retroactively modify historical pricing in the costing table such as clusterStateFact or jobRunCostPotentialFact. For prices to be recalculated, the gold pipeline modules must be rolled back properly such that the costs can be rebuilt with the updated values.\nColumn Type Description sku string One of automated, interactive, jobsLight, sqlCompute contract_price double Price paid per DBU on the sku is_active boolean whether the contract price is currently active. This must be true for each key where activeUntil is null activeFrom* date The start date for the costs in this record. NOTE this MUST be equal to one other record\u0026rsquo;s activeUntil unless this is the first record for these costs. There may be no overlap in time or gaps in time. activeUntil* date The end date for the costs in this record. Must be null to indicate the active record. Only one record can be active at all times. The key (API_name) must have zero gaps and zero overlaps from the Overwatch primordial date until now indicated by null (active) Job SAMPLE\nThe below columns closely mirror the APIs listed below by action. For more details about these fields and their structures please reference the relevant Databricks Documentation for the action.\nNote \u0026ndash; Databricks has moved to API2.1 for all jobs-related functions and in-turn, Databricks has moved several fields from the root level to a nested level to support multi-task jobs. These root level fields are still visible in Overwatch as some customers are still using legacy APIs and many customers have historical data by which this data was generated using the legacy 2.0 APIs. These fields can be identified by the prefix, \u0026ldquo;Legacy\u0026rdquo; in the Description and have been colored red on the ERD.\nAction API SnapImpute Only created during the first Overwatch Run to initialize records of existing jobs not present in the audit logs. These jobs are still available in the UI but have not been modified since the collection of audit logs begun thus no events have been identified and therefore must be imputed to maximize coverage Create \u0026ldquo;Create New Job API\u0026rdquo; Update \u0026ldquo;Partially Update a Job\u0026rdquo; Reset \u0026ldquo;Overwrite All Settings for a Job\u0026rdquo; Delete \u0026ldquo;Delete A Job\u0026rdquo; ChangeJobAcl \u0026ldquo;Update Job Permissions\u0026rdquo; ResetJobAcls \u0026ldquo;Replace Job Permissions\u0026rdquo; \u0026ndash; Not yet supported KEY \u0026ndash; organization_id + job_id + unixTimeMS + action + request_id\nIncremental Columns \u0026ndash; unixTimeMS\nPartition Columns \u0026ndash; organization_id\nWrite Mode \u0026ndash; Append\nColumn Type Description organization_id string Canonical workspace id workspace_name string Customer defined name of the workspace or workspace_id (default) job_id long Databricks job id action string Action type defined by the record. One of: create, reset, update, delete, resetJobAcl, changeJobAcl. More information about these actions can be found here date date Date of the action for the key timestamp timestamp Timestamp the action took place job_name string User defined name of job. tags map The tags applied to the job if they exist tasks array The tasks defined for the job job_clusters array The job clusters defined for the job libraries array LEGACY \u0026ndash; Libraries defined in the job \u0026ndash; Nested within tasks as of API 2.1 timeout_seconds string Job-level timeout seconds. Databricks supports timeout seconds at both the job level and the task level. Task level timeout_seconds can be found nested within tasks max_concurrent_runs long Job-level \u0026ndash; maximum concurrent executions of the job max_retries long LEGACY \u0026ndash; Max retries for legacy jobs \u0026ndash; Nested within tasks as of API 2.1 retry_on_timeout boolean LEGACY \u0026ndash; whether or not to retry if a job run times out \u0026ndash; Nested within tasks as of API 2.1 min_retry_interval_millis long LEGACY \u0026ndash; Minimal interval in milliseconds between the start of the failed run and the subsequent retry run. The default behavior is that unsuccessful runs are immediately retried. \u0026ndash; Nested within tasks as of API 2.1 schedule struct Schedule by which the job should execute and whether or not it is paused existing_cluster_id string LEGACY \u0026ndash; If compute is existing interactive cluster the cluster_id will be here \u0026ndash; Nested within tasks as of API 2.1 new_cluster struct LEGACY \u0026ndash; The cluster_spec identified as an automated cluster for legacy jobs \u0026ndash; Can be found nested within tasks now but ONLY for direct API Calls, editing legacy jobs, AND sparkSumbit tasks (as they cannot use job_clusters), otherwise, new_clusters defined through the UI will be defined as \u0026ldquo;job_clusters\u0026rdquo; and referenced by a \u0026ldquo;job_cluster_key\u0026rdquo; in the tasks field. git_source struct Specification for a remote repository containing the notebooks used by this job\u0026rsquo;s notebook tasks. task_detail_legacy struct LEGACY \u0026ndash; The job execution details used to be defined at the root level for API 2.0 as of API 2.1 they have been nested within tasks. The logic definition will be defined here for legacy jobs only (or new jobs created using the 2.0 jobs API) is_from_dlt boolean Whether or not the job was created from DLT \u0026ndash; Unsupported as OW doesn\u0026rsquo;t yet support DLT but left here as a reference in case it can be helpful aclPermissionSet struct Only populated for \u0026ldquo;ChangeJobAcl\u0026rdquo; actions. Defines the new ACLs for a job target_user_id string Databricks canonical user id to which the aclPermissionSet is to be applied session_id string session_id that requested the action request_id string request_id of the action user_agent string request origin such as browser, terraform, api, etc. response struct response of api call including errorMessage, result, and statusCode (HTTP 200,400, etc) source_ip_address string Origin IP of action requested created_by string Email account that created the job created_ts long Timestamp the job was created deleted_by string Email account that deleted the job \u0026ndash; will be null if job has not been deleted deleted_ts long Timestamp the job was deleted \u0026ndash; will be null if job has not been deleted last_edited_by string Email account that made the previous edit \u0026ndash; defaults to created by if no edits made last_edited_ts long Timestamp the job was last edited JobRun SAMPLE\nDatabricks has moved to \u0026ldquo;multi-task jobs\u0026rdquo; (MTJs) and each run now refers to the run of a task not a job. This migration will likely cause a lot of confusion so please read this carefully.\nEach record references the full lifecycle of a single task run with some legacy fields to accommodate historical job-level runs (and jobs/runs still being created/launched from the deprecated Jobs 2.0 API). Since the inception of multi-task jobs and Databricks jobs API 2.1, all run logic has been migrated from the job-level to the task-level. Overwatch must accommodate both as many customers have historical data that is still important. As such, some of the fields seem redundant and the analyst must apply the correct logic based on the circumstances. Please carefully review the field descriptions to understand the rules.\nUnsupported Scenarios A few scenarios are not yet supported by Overwatch; they are called out here. Please stay tuned for updates as it\u0026rsquo;s our intention to include everything we can as soon as possible after Databricks product GAs new features but there will be a delay.\nDLT details \u0026ndash; Delta Live tables aren\u0026rsquo;t yet supported even though the run_ids may show up here Runs that failed to launch due to error in the launch request \u0026ndash; these never actually create a run and never receive a run_id therefore they will not be present in this table at this time. Runs executing for more than 30 Days \u0026ndash; This is a limitation for performance. This will be an externalized config at a later time but for now the hard limit is 30 days. The progress of this feature can be tracked in Issue 528 TODO \u0026ndash; clarify the taskRunId vs jobRunId confusion from the UI\nKEY \u0026ndash; organization_id + run_id + startEpochMS\nIncremental Columns \u0026ndash; startEpochMS\nPartition Columns \u0026ndash; organization_id\nWrite Mode \u0026ndash; Merge\nInventory of every canonical task run executed by databricks workspace.\nColumn Type Description organization_id string Canonical workspace id workspace_name string Customer defined name of the workspace or workspace_id (default) job_id long ID of the job job_name string Name of the runName if run is named, otherwise it will be job name job_trigger_type string One of \u0026ldquo;cron\u0026rdquo; (automated scheduled), \u0026ldquo;manual\u0026rdquo;, \u0026ldquo;repair\u0026rdquo; terminal_state string State of the task run at the time of Overwatch pipeline execution run_id long The lowest level of the run_id (i.e. legacy jobs may not have a task_run_id, in this case, it will be the job_run_id). run_name string The name of the run if the run is named (i.e. in submitRun) otherwise this is set == taskKey multitask_parent_run_id long If the task belongs to a multi-task job the job_run_id will be populated here, otherwise it will be null job_run_id long The run id of the job, not the task task_run_id long The run id of the task except for legacy and repair_id long If the task or job was repaired, the repair id will be present here and the details of the repair will be in repair_details task_key string The name of the task is actually a key and must be unique within a job, this field specifies the task that was executed in this task_run_id cluster_type string Type of cluster used in the execution, one of \u0026ldquo;new\u0026rdquo;, \u0026ldquo;job_cluster\u0026rdquo;, \u0026ldquo;existing\u0026rdquo;, \u0026ldquo;SQL Warehouse\u0026rdquo;, null \u0026ndash; will be null for DLT pipelines and/or in situations where the type is not provided from Databricks cluster_id string The cluster ID of the compute used to execute the task run. If task executed on a SQL Warehouse, the warehouse_id will be populated here. cluster_name string The name of the compute asset used to execute the task run job_cluster_key string When the task compute is a job_cluster the name of the job_cluster will be provided here job_cluster struct When the task compute is a job_cluster, the cluster_definition of the job_cluster used to execute the task new_cluster struct LEGACY + SparkSubmit jobs \u0026ndash; new clusters are no longer used for tasks except for sparkSubmit jobs as they cannot use job_clusters. Job_clusters are used everywhere else tags map Job tags at the time of the run task_detail struct The details of the task logic such as notebook_task, sql_task, spark_python_task, etc. task_dependencies array The list of tasks the task depends on to be successful in order to run task_runtime struct The runtime of the task from launch to termination (including compute spin-up time) task_execution_runtime struct The execution time of the task (excluding compute spin-up time) task_type string Type of task to be executed \u0026ndash; this should mirror the \u0026ldquo;type\u0026rdquo; selected in the \u0026ldquo;type\u0026rdquo; drop down in the job definition. May be null for submitRun as this jobType schedule struct Schedule by which the job should execute and whether or not it is paused libraries array LEGACY \u0026ndash; Libraries defined in the job \u0026ndash; Nested within tasks as of API 2.1 manual_override_params struct When task is executed manually and the default parameters were manually overridden the overridden parameters will be captured here repair_details array Details of the repair run including any references to previous repairs timeout_seconds string Job-level timeout seconds. Databricks supports timeout seconds at both the job level and the task level. Task level timeout_seconds can be found nested within tasks retry_on_timeout boolean LEGACY \u0026ndash; whether or not to retry if a job run times out \u0026ndash; Nested within tasks as of API 2.1 max_retries long LEGACY \u0026ndash; Max retries for legacy jobs \u0026ndash; Nested within tasks as of API 2.1 min_retry_interval_millis long LEGACY \u0026ndash; Minimal interval in milliseconds between the start of the failed run and the subsequent retry run. The default behavior is that unsuccessful runs are immediately retried. \u0026ndash; Nested within tasks as of API 2.1 max_concurrent_runs long Job-level \u0026ndash; maximum concurrent executions of the job run_as_user_name string The user email of the principal configured to execute the job parent_run_id long The upstream run_id of the run that called current run using dbutils.notebook.run \u0026ndash; DO NOT confuse this with multitask_parent_run_id, these are different workflow_context string The workflow context (as a json string) provided when using Notebook Workflows (i.e. dbutils.notebook.run) task_detail_legacy struct LEGACY \u0026ndash; The details of the task logic for legacy jobs such as notebook_task, spark_python_task, etc. These must be separated from the task level details as the structures have been altered in many cases submitRun_details struct When task_type == submitRun, full job and run definition provided in the submitRun API Call. Since no existing job definition is present for a submitRun \u0026ndash; all the details of the run submission are captured here created_by string Email account that created the job last_edited_by string Email account that made the previous edit \u0026ndash; defaults to created by if no edits made request_detail struct All request details of the lifecycle and their results are captured here including submission, cancellation, completions, and execution start time_detail struct All events in the run lifecycle timestamps are captured here in the event deeper timestamp analysis is required JobRunCostPotentialFact SAMPLE\nDatabricks has moved to \u0026ldquo;multi-task jobs\u0026rdquo; and each run now refers to the run of a task not a job. Please reference jobRuns table for more detail\nUnsupported Scenarios Costs for runs of that execute SQL/DBT/DLT tasks KEY \u0026ndash; organization_id + run_id + startEpochMS\nIncremental Columns \u0026ndash; startEpochMS\nPartition Columns \u0026ndash; organization_id\nWrite Mode \u0026ndash; Merge\nThis fact table defines the job, the cluster, the cost, the potential, and utilization (if cluster logging is enabled) of a cluster associated with a specific Databricks Job Run.\nDimensionality Note that this fact table is not normalized by time but rather by job run and cluster state. Costs are not derived from job runs but from clusters thus the state[s] of the cluster are what\u0026rsquo;s pertinent when tying to cost. This is extremely important in the case of long running jobs, such as streaming.\nSCENARIO: Imagine a streaming job with 12 concurrent runs on an existing cluster that run for 20 days at the end of which the driver dies for some reason causing all runs fail and begin retrying but failing. When the 20 days end, the cost will be captured solely on that date and even more importantly, not only will all 20 days be captured at that date but the cost associated will be cluster runtime for 20 days * number of runs. Overwatch will automatically smooth the costs across the concurrent runs but not the days running since this fact table is not based by on an equidistant time axis.\nPotential: Total core_milliseconds for which the cluster COULD execute spark tasks. This derivation only includes the worker nodes in a state ready to receive spark tasks (i.e. Running). Nodes being added or running init scripts are not ready for spark jobs thus those core milliseconds are omitted from the total potential. Cost: Derived from the instanceDetails table and DBU configured contract price (see Configuration for more details). The compute costs in instanceDetails table are taken from the \u0026ldquo;Compute_Contract_Price\u0026rdquo; values associated with the instance type in instanceDetails. Utilization: Utilization is a function of core milliseconds used during spark task execution divided by the total amount of core milliseconds available given the cluster size and state. (i.e. spark_task_runtime_H / worker_potential_core_H) Cluster State: The state[s] of a cluster during a run. As the cluster scales and morphs to accommodate the run\u0026rsquo;s needs, the state changes. The number of state changes are recorded in this table as \u0026ldquo;run_cluster_states\u0026rdquo;. Run State: Advanced Topic for data engineers and developers. This topic is discussed in considerable detail in the Advanced Topics section. Given a cluster state, the run state is a state of all runs on a cluster at a given moment in time. This is the measure used to calculate shared costs across concurrent runs. A run state cannot pass the boundaries of a cluster state, a run that continues across cluster-state lines will result in a new run state. Column Type Description organization_id string Canonical workspace id workspace_name string Customer defined name of the workspace or workspace_id (default) job_id long Canonical ID of job job_name string Name of the runName if run is named, otherwise it will be job name job_trigger_type string One of \u0026ldquo;cron\u0026rdquo; (automated scheduled), \u0026ldquo;manual\u0026rdquo;, \u0026ldquo;repair\u0026rdquo; terminal_state string State of the task run at the time of Overwatch pipeline execution run_id long The lowest level of the run_id (i.e. legacy jobs may not have a task_run_id, in this case, it will be the job_run_id). run_name string The name of the run if the run is named (i.e. in submitRun) otherwise this is set == taskKey multitask_parent_run_id long If the task belongs to a multi-task job the job_run_id will be populated here, otherwise it will be null job_run_id long The run id of the job, not the task task_run_id long The run id of the task except for legacy and repair_id long If the task or job was repaired, the repair id will be present here and the details of the repair will be in repair_details task_key string The name of the task is actually a key and must be unique within a job, this field specifies the task that was executed in this task_run_id task_type string Type of task to be executed \u0026ndash; this should mirror the \u0026ldquo;type\u0026rdquo; selected in the \u0026ldquo;type\u0026rdquo; drop down in the job definition. May be null for submitRun as this jobType task_runtime struct The runtime of the task from start to termination. Databricks does not publish task_launch_time task_execution_runtime struct Until Databricks publishes task_launch_time this will equal task_runtime cluster_type string Type of type cluster used in the execution, one of \u0026ldquo;automated\u0026rdquo;, \u0026ldquo;interactive, null \u0026ndash; will be null for DLT pipelines and/or in situations where the type is not provided from Databricks. In the future you can expect \u0026ldquo;SQL Warehouse\u0026rdquo; and other types of compute to show up here. cluster_id string The cluster ID of the compute used to execute the task run. If task executed on a SQL Warehouse, the warehouse_id will be populated here. cluster_name string The name of the compute asset used to execute the task run cluster_tags map Tags present on the compute that executed the run parent_run_id long The upstream run_id of the run that called current run using dbutils.notebook.run \u0026ndash; DO NOT confuse this with multitask_parent_run_id, these are different running_days array Array (or list) of dates (not strings) across which the job run executed. This simplifies day-level cost attribution, among other metrics, when trying to smooth costs for long-running / streaming jobs avg_cluster_share double Average share of the cluster the run had available assuming fair scheduling. This DOES NOT account for activity outside of jobs (i.e. interactive notebooks running alongside job runs), this measure only splits out the share among concurrent job runs. Measure is only calculated for interactive clusters, automated clusters assume 100% run allocation. For more granular utilization detail, enable cluster logging and utilize \u0026ldquo;job_run_cluster_util\u0026rdquo; column which derives utilization at the spark task level. avg_overlapping_runs double Number of concurrent runs shared by the cluster on average throughout the run max_overlapping_runs long Highest number of concurrent runs on the cluster during the run run_cluster_states long Count of cluster state transitions during the job run driver_node_type_id string Driver Node type for the compute asset (not supported for Warehouses yet) node_type_id string Worker Node type for the compute asset (not supported for Warehouses yet) worker_potential_core_H double cluster core hours capable of executing spark tasks, \u0026ldquo;potential\u0026rdquo; dbu_rate double Effective DBU rate at time of job run used for calculations based on configured contract price in instanceDetails at the time of the Overwatch Pipeline Run driver_compute_cost double Compute costs associated with driver runtime driver_dbu_cost double DBU costs associated with driver runtime worker_compute_cost double Compute costs associated with worker runtime worker_dbu_cost double DBU costs associated with cumulative runtime of all worker nodes total_driver_cost double Driver costs including DBUs and compute total_worker_cost double Worker costs including DBUs and compute total_compute_cost double All compute costs for Driver and Workers total_dbu_cost double All dbu costs for Driver and Workers total_cost double Total cost from Compute and DBUs for all nodes (including Driver) spark_task_runtimeMS long Spark core execution time in milliseconds (i.e. task was operating/locking on core). Cluster logging must be enabled spark_task_runtime_H double Spark core execution time in Hours (i.e. task was operating/locking on core). Cluster logging must be enabled job_run_cluster_util double Cluster utilization: spark task execution time / cluster potential. True measure by core of utilization. Cluster logging must be enabled. created_by string Email account that created the job last_edited_by string Email account that made the previous edit \u0026ndash; defaults to created by if no edits made sqlQueryHistory SAMPLE\nKEY \u0026ndash; organization_id + warehouse_id + query_id + query_start_time_ms\nIncremental Columns \u0026ndash; query_start_time_ms\nPartition Columns \u0026ndash; organization_id\nWrite Mode \u0026ndash; Merge\nColumn Type Description organization_id string Canonical workspace id workspace_name string Customizable human-legible name of the workspace, should be globally unique within the organization warehouse_id string ID of the SQL warehouse. query_id string ID of the query executed in the warehouse query_end_time_ms long Query execution end time user_name string User name who created the query user_id long Id of the user who created the query executed_as_user_id long Id of the user who executed the query executed_as_user_name string User name who executed the query duration long Duration of the query execution error_message string Error message for failed queries execution_end_time_ms long Query execution end time in ms query_start_time_ms long Query start time in ms query_text text Query text which is executed in the warehouse rows_produced long Number of rows returned as query output spark_ui_url string URL of the Spark UI statement_type string Statement type of the query being executed, e.g - Select, Update etc status string Current status of the query being executed, e.g - FINISHED, RUNNING etc compilation_time_ms long Time spent loading metadata and optimizing the query, in milliseconds. execution_time_ms long ime spent executing the query, in milliseconds. network_sent_bytes long Size of data transferred over network in bytes photon_total_time_ms long Total execution time for all individual Photon query engine tasks in the query, in milliseconds. pruned_bytes long Size of data pruned in bytes pruned_files_count long Total number of files pruned read_bytes long Size of data red in bytes read_cache_bytes long Size of data cached during reading in bytes read_files_count long Total number of files in read read_partitions_count long Total number of partitions used while reading read_remote_bytes long Shuffle fetches from remote executor result_fetch_time_ms long Time spent fetching the query results after the execution finished, in milliseconds. result_from_cache long Flag to check whether result is fetched from cache rows_produced_count long Total number of rows produced after fetching the data rows_read_count string Total number of rows in the output after fetcing the data spill_to_disk_bytes long Data spilled to disk in bytes task_total_time_ms long Sum of execution time for all of the querys tasks, in milliseconds. total_time_ms long Total execution time of the query from the clients point of view, in milliseconds. This is equivalent to duration write_remote_bytes long Shuffle writes to the remote executor Notebook SAMPLE\nKEY \u0026ndash; organization_id + notebook_id + request_id + action + unixTimeMS\nIncremental Columns \u0026ndash; unixTimeMS\nPartition Columns \u0026ndash; organization_id\nWrite Mode \u0026ndash; Append\nColumn Type Description notebook_id string Canonical notebook id notebook_name string Name of notebook at time of action requested notebook_path string Path of notebook at time of action requested cluster_id string Canonical workspace cluster id action string action recorded timestamp timestamp timestamp the action took place old_name string When action is \u0026ldquo;renameNotebook\u0026rdquo; this holds notebook name before rename old_path string When action is \u0026ldquo;moveNotebook\u0026rdquo; this holds notebook path before move new_name string When action is \u0026ldquo;renameNotebook\u0026rdquo; this holds notebook name after rename new_path string When action is \u0026ldquo;moveNotebook\u0026rdquo; this holds notebook path after move parent_path string When action is \u0026ldquo;renameNotebook\u0026rdquo; notebook containing, workspace path is recorded here user_email string Email of the user requesting the action request_id string Canonical request_id response struct HTTP response including errorMessage, result, and statusCode InstancePool KEY \u0026ndash; organization_id + instance_pool_id + timestamp\nIncremental Columns \u0026ndash; timestamp\nPartition Columns \u0026ndash; organization_id\nWrite Mode \u0026ndash; Merge\nColumn Type Description instance_pool_id string Canonical notebook id instance_pool_name string Name of notebook at time of action requested actionName string action recorded timestamp long timestamp the action took place node_type_id string Type of node in the pool idle_instance_autotermination_minutes long Minutes after which a node shall be terminated if unused min_idle_instances long Minimum number of hot instances in the pool max_capacity long Maximum number of nodes allowed in the pool preloaded_spark_versions string Spark versions preloaded on nodes in the pool Account Tables Not exposed in the consumer database. These tables contain more sensitive information and by default are not exposed in the consumer database but held back in the ETL database. This is done purposely to simplify security when/if desired. If desired, this can be exposed in consumer database with a simple vew definition exposing the columns desired.\nFor deeper insights regarding audit, please reference auditLogSchema. This is simplified through the use of the ETL_DB.audit_log_bronze and filter where serviceName == accounts for example. Additionally, you may filter down to specific actions using \u0026ldquo;actionName\u0026rdquo;. An example query is provided below:\nspark.table(\u0026#34;overwatch.audit_log_bronze\u0026#34;) .filter(\u0026#39;serviceName === \u0026#34;accounts\u0026#34; \u0026amp;\u0026amp; \u0026#39;actionName === \u0026#34;createGroup\u0026#34;) .selectExpr(\u0026#34;*\u0026#34;, \u0026#34;requestParams.*\u0026#34;).drop(\u0026#34;requestParams\u0026#34;) Slow changing dimension of user entity through time. Also used as reference map from user_email to user_id\nColumn Type Description organization_id string Canonical workspace id user_id string Canonical user id for which the action was requested (within the workspace) (target) user_email string User\u0026rsquo;s email for which the action was requested (target) action string Action requested to be performed added_from_ip_address string Source IP of the request added_by string Authenticated user that made the request user_agent string request origin such as browser, terraform, api, etc. AccountMod SAMPLE\nKEY \u0026ndash; organization_id + acton + mod_unixTimeMS + request_id\nIncremental Columns \u0026ndash; mod_unixTimeMS\nPartition Columns \u0026ndash; organization_id\nWrite Mode \u0026ndash; Append\nTODO\nAccountLogin SAMPLE\nKEY \u0026ndash; organization_id + login_type + login_unixTimeMS + from_ip_address\nIncremental Columns \u0026ndash; login_unixTimeMS\nPartition Columns \u0026ndash; organization_id\nWrite Mode \u0026ndash; Append\nNot exposed in the consumer database. This table contains more sensitive information and by default is not exposed in the consumer database but held back in the etl datbase. This is done purposely to simplify security when/if desired. If desired, this can be exposed in consumer database with a simple vew definition exposing the columns desired.\nColumn Type Description user_id string Canonical user id (within the workspace) user_email string User\u0026rsquo;s email login_type string Type of login such as web, ssh, token ssh_username string username used to login via SSH groups_user_name string ?? To research ?? account_admin_userID string ?? To research ?? login_from_ip_address struct Details about the source login and target logged into user_agent string request origin such as browser, terraform, api, etc. The following sections are related to Spark. Everything that can be seend/found in the SparkUI is visibel in the spark tables below. A reasonable understanding of the Spark hierarchy is necessary to make this section simpler. Please reference Spark Hierarchy For More Details for more details.\nSparkExecution SAMPLE\nKEY \u0026ndash; organization_id + spark_context_id + execution_id + date + unixTimeMS\nIncremental Columns \u0026ndash; date + unixTimeMS\nPartition Columns \u0026ndash; organization_id\nWrite Mode \u0026ndash; Merge\nColumn Type Description organization_id string Canonical workspace id spark_context_id string Canonical context ID \u0026ndash; One Spark Context per Cluster cluster_id string Canonical workspace cluster id execution_id long Spark Execution ID description string Description provided by spark details string Execution StackTrace sql_execution_runtime struct Complete runtime detail breakdown SparkExecutor SAMPLE\nKEY \u0026ndash; organization_id + spark_context_id + executor_id + date + unixTimeMS\nIncremental Columns \u0026ndash; date + unixTimeMS\nPartition Columns \u0026ndash; organization_id\nWrite Mode \u0026ndash; Merge\nColumn Type Description organization_id string Canonical workspace id spark_context_id string Canonical context ID \u0026ndash; One Spark Context per Cluster cluster_id string Canonical workspace cluster id executor_id int Executor ID executor_info string Executor Detail removed_reason string Reason executor was removed executor_alivetime struct Complete lifetime detail breakdown SparkJob SAMPLE\nKEY \u0026ndash; organization_id + spark_context_id + job_id + unixTimeMS\nIncremental Columns \u0026ndash; date + unixTimeMS\nPartition Columns \u0026ndash; organization_id + date\nZ-Order Columns \u0026ndash; cluster_id\nWrite Mode \u0026ndash; Merge\nColumn Type Description organization_id string Canonical workspace id spark_context_id string Canonical context ID \u0026ndash; One Spark Context per Cluster cluster_id string Canonical workspace cluster id job_id string Spark Job ID job_group_id string Spark Job Group ID \u0026ndash; NOTE very powerful for many reasons. See SparkEvents execution_id string Spark Execution ID stage_ids array[long] Array of all Spark Stage IDs nested within this Spark Job notebook_id string Canonical Databricks Workspace Notebook ID notebook_path string Databricks Notebook Path user_email string email of user that owned the request, for Databricks jobs this will be the job owner db_job_id string Databricks Job Id executing the Spark Job db_id_in_job string \u0026ldquo;id_in_job\u0026rdquo; such as \u0026ldquo;Run 10\u0026rdquo; without \u0026ldquo;Run \u0026quot; prefix. This is a critical join column when working looking up Databricks Jobs metadata job_runtime string Complete job runtime detail breakdown job_result struct Job Result and Exception if present SparkStage SAMPLE\nKEY \u0026ndash; organization_id + spark_context_id + stage_id + stage_attempt_id + unixTimeMS\nIncremental Columns \u0026ndash; date + unixTimeMS\nPartition Columns \u0026ndash; organization_id + date\nZ-Order Columns \u0026ndash; cluster_id\nWrite Mode \u0026ndash; Merge\nColumn Type Description organization_id string Canonical workspace id spark_context_id string Canonical context ID \u0026ndash; One Spark Context per Cluster cluster_id string Canonical workspace cluster id stage_id string Spark Stage ID stage_attempt_id string Spark Stage Attempt ID stage_runtime string Complete stage runtime detail stage_info string Lineage of all accumulables for the Spark Stage SparkTask SAMPLE\nKEY \u0026ndash; organization_id + spark_context_id + task_id + task_attempt_id + stage_id + stage_attempt_id + host + unixTimeMS\nIncremental Columns \u0026ndash; date + unixTimeMS\nPartition Columns \u0026ndash; organization_id + date\nZ-Order Columns \u0026ndash; cluster_id\nWrite Mode \u0026ndash; Merge\nUSE THE PARTITION COLUMN (date) and Indexed Column (cluster_id) in all joins and filters where possible. This table can get extremely large, select samples or smaller date ranges and reduce joins and columns selected to improve performance.\nColumn Type Description organization_id string Canonical workspace id workspace_name string Customizable human-legible name of the workspace, should be globally unique within the organization spark_context_id string Canonical context ID \u0026ndash; One Spark Context per Cluster cluster_id string Canonical workspace cluster id task_id string Spark Task ID task_attempt_id string Spark Task Attempt ID stage_id string Spark Stage ID stage_attempt_id string Spark Stage Attempt ID executor_id string Spark Executor ID host string Internal IP address of node task_runtime string Complete task runtime detail task_metrics string Lowest level compute metrics provided by spark such as spill bytes, read/write bytes, shuffle info, GC time, Serialization, etc. task_info string Lineage of all accumulables for the Spark Task task_type string Spark task Type (i.e. ResultTask, ShuffleMapTask, etc) task_end_reason string Task end status, state, and details plus stake trace when error SparkStream KEY \u0026ndash; organization_id + spark_context_id + cluster_id + stream_id + stream_run_id + stream_batch_id + stream_timestamp\nIncremental Columns \u0026ndash; date + stream_timestamp\nPartition Columns \u0026ndash; organization_id + date\nZ-Order Columns \u0026ndash; cluster_id\nWrite Mode \u0026ndash; Merge\nColumn Type Description spark_context_id string Canonical context ID \u0026ndash; One Spark Context per Cluster cluster_id string Canonical workspace cluster id stream_id string GUID ID of the spark stream stream_name string Name of stream if named stream_run_id string GUID ID of the spark stream run stream_batch_id long GUID ID of the spark stream run batch stream_timestamp long Unix time (millis) the stream reported its batch complete metrics streamSegment string Type of event from the event listener such as \u0026lsquo;Progressed\u0026rsquo; streaming_metrics dynamic struct All metrics available for the stream batch run execution_ids array Array of execution_ids in the spark_context. Can explode and tie back to sparkExecution and other spark tables Warehouse SAMPLE\nKEY \u0026ndash; organization_id + warehouse_id + unixTimeMS\nIncremental Columns \u0026ndash; unixTimeMS\nPartition Columns \u0026ndash; organization_id\nWrite Mode \u0026ndash; Append\nColumn Type Description organization_id string Canonical workspace id workspace_name string Customizable human-legible name of the workspace, should be globally unique within the organization warehouse_id string Canonical workspace warehouse id warehouse_name string User-defined name of the warehouse service_name string Name of the service corresponding to DBSQL warehouse action_name string create, edit, or snapImpute  depicts the type of action for the warehouse  **snapImpute is used on first run to initialize the state of the cluster even if it wasnt created/edited since audit logs began user_email string Email of the user requesting the action cluster_size string Size of the clusters allocated for this warehouse. min_num_clusters long Minimum number of available clusters that will be maintained for this SQL warehouse. max_num_clusters long Maximum number of clusters that the autoscaler will create to handle concurrent queries. auto_stop_mins long The amount of time in minutes that a SQL warehouse must be idle (i.e., no RUNNING queries) before it is automatically stopped. spot_instance_policy string Configurations whether the warehouse should use spot instances. enable_photon boolean Configures whether the warehouse should use Photon optimized clusters. channel struct This column contains channel details. Some examples - CHANNEL_NAME_UNSPECIFIED, CHANNEL_NAME_PREVIEW, CHANNEL_NAME_CURRENT, CHANNEL_NAME_PREVIOUS, CHANNEL_NAME_CUSTOM enable_serverless_compute boolean Flag indicating whether the warehouse should use serverless compute. warehouse_type string Warehouse type: PRO or CLASSIC warehouse_state string State of the warehouse size string Size of the clusters allocated for this warehouse. creator_id long warehouse creator id tags map A set of key-value pairs that will be tagged on all resources (e.g., AWS instances and EBS volumes) associated with this SQL warehouse. num_clusters long current number of clusters running for the service num_active_sessions long current number of active sessions for the warehouse jdbc_url string the jdbc connection string for this warehouse created_by string warehouse creator name Common Meta Fields Column Type Description organization_id string Workspace / Organization ID on which the cluster was instantiated cluster_id string Canonical workspace cluster id unixTimeMS long unix time epoch as a long in milliseconds timestamp string unixTimeMS as a timestamp type in milliseconds date string unixTimeMS as a date type created_by string last_edited_by string last user to edit the state of the entity last_edited_ts string timestamp at which the entitiy\u0026rsquo;s sated was last edited deleted_by string user that deleted the entity deleted_ts string timestamp at which the entity was deleted event_log_start string Spark Event Log BEGIN file name / path event_log_end string Spark Event Log END file name / path Pipeline_SnapTS string Snapshot timestmap of Overwatch run that added the record Overwatch_RunID string Overwatch canonical ID that resulted in the record load ETL Tables The following are the list of potential tables, the module with which it\u0026rsquo;s created and the layer in which it lives. This list consists of only the ETL tables created to facilitate and deliver the consumption layer The gold and consumption layers are the only layers that maintain column name uniformity and naming convention across all tables. Users should always reference Consumption and Gold layers unless the data necessary has not been curated.\nBronze Table Scope Layer Description audit_log_bronze audit bronze Raw audit log data full schema audit_log_raw_events audit bronze (azure) Intermediate staging table responsible for coordinating intermediate events from azure Event Hub cluster_events_bronze clusterEvents bronze Raw landing of dataframe derived from JSON response from cluster events api call. Note: cluster events expire after 30 days of last termination. (reference) clusters_snapshot_bronze clusters bronze API snapshot of existing clusters defined in Databricks workspace at the time of the Overwatch run. Snapshot is taken on each run jobs_snapshot_bronze jobs bronze API snapshot of existing jobs defined in Databricks workspace at the time of the Overwatch run. Snapshot is taken on each run pools_snapshot_bronze pools bronze API snapshot of existing pools defined in Databricks workspace at the time of the Overwatch run. Snapshot is taken on each run spark_events_bronze sparkEvents bronze Raw landing of the master sparkEvents schema and data for all cluster logs. Cluster log locations are defined by cluster specs and all locations will be scanned for new files not yet captured by Overwatch. Overwatch uses an implicit schema generation here, as such, a lack of real-world can cause unforeseen issues. spark_events_processedfiles sparkEvents bronze Table that keeps track of all previously processed cluster log files (spark event logs) to minimize future file scanning and improve performance. This table can be used to reprocess and/or find specific eventLog files. warehouses_snapshot_bronze DBSQL bronze API snapshot of existing warehouse defined in Databricks workspace at the time of the Overwatch run. Snapshot is taken on each run pipeline_report NA tracking Tracking table used to identify state and status of each Overwatch Pipeline run. This table is also used to control the start and end points of each run. Altering the timestamps and status of this table will change the ETL start/end points. Silver Table Scope Layer Description account_login_silver accounts silver Login events account_mods_silver accounts silver Account modification events cluster_spec_silver clusters silver Slow changing dimension used to track all clusters through time including edits but excluding state change. cluster_state_detail_silver clusterEvents silver State detail for each cluster event enriched with cost information job_status_silver jobs silver Slow changing dimension used to track all jobs specifications through time jobrun_silver jobs silver Historical run of every job since Overwatch began capturing the audit_log_data notebook_silver notebooks silver Slow changing dimension used to track all notebook changes as it morphs through time along with which user instigated the change. This does not include specific change details of the commands within a notebook just metadata changes regarding the notebook. pools_silver pools silver Slow changing dimension used to track all changes to instance pools spark_executions_silver sparkEvents silver All spark event data relevant to spark executions spark_executors_silver sparkEvents silver All spark event data relevant to spark executors spark_jobs_silver sparkEvents silver All spark event data relevant to spark jobs spark_stages_silver sparkEvents silver All spark event data relevant to spark stages spark_tasks_silver sparkEvents silver All spark event data relevant to spark tasks sql_query_history_silver DBSQL silver History of all the sql queries executed through SQL warehouses warehouse_spec_silver DBSQL silver State detail for each warehouse event Gold Table Scope Layer Description account_login_gold accounts gold Login events account_mods_gold accounts gold Account modification events cluster_gold clusters gold Slow-changing dimension with all cluster creates and edits through time. These events DO NOT INCLUDE automated cluster resize events or cluster state changes. Automated cluster resize and cluster state changes will be in clusterstatefact_gold. If user changes min/max nodes or node count (non-autoscaling) the event will be registered here AND clusterstatefact_gold. clusterStateFact_gold clusterEvents gold All cluster event changes along with the time spent in each state and the core hours in each state. This table should be used to find cluster anomalies and/or calculate compute/DBU costs of some given scope. job_gold jobs gold Slow-changing dimension of all changes to a job definition through time jobrun_gold jobs gold Dimensional data for each job run in the databricks workspace notebook_gold notebooks gold Slow changing dimension used to track all notebook changes as it morphs through time along with which user instigated the change. This does not include specific change details of the commands within a notebook just metadata changes regarding the notebook. instancepool_gold pools gold Slow changing dimension used to track all changes to instance pools sparkexecution_gold sparkEvents gold All spark event data relevant to spark executions sparkexecutor_gold sparkEvents gold All spark event data relevant to spark executors sparkjob_gold sparkEvents gold All spark event data relevant to spark jobs sparkstage_gold sparkEvents gold All spark event data relevant to spark stages sparktask_gold sparkEvents gold All spark event data relevant to spark tasks sparkstream_gold sparkEvents gold All spark event data relevant to spark streams sql_query_history_gold DBSQL gold History of all the sql queries executed through SQL warehouses warehouse_gold DBSQL gold Slow-changing dimension with all warehouse creates and edits through time. "
},
{
	"uri": "http://localhost:1313/overwatch/contributing/",
	"title": "",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://localhost:1313/overwatch/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://localhost:1313/overwatch/deployoverwatch/configureoverwatch/configdetailsbyversion/",
	"title": "Configuration Details By Version",
	"tags": [],
	"description": "",
	"content": "Overwatch Deployment Configuration By Version 0.8.x.x Configuration 0.7.2.x Configuration 0.7.1.x Configuration Column description 0.8.x.x Column Type IsRequired Description workspace_name String True Name of the workspace. workspace_id String True Id of the workspace. MUST BE VALUE AFTER THE o= in the URL bar. To ensure you get the right value, run the following on the target workspace. Initializer.getOrgId workspace_url String True URL of the workspace. Should be in format of https://*.com or https://*.net. Don\u0026rsquo;t include anything after the .com or .net suffix api_url String True API URL for the Workspace (execute in scala dbutils.notebook.getContext().apiUrl.get ON THE TARGET WORKSPACE NOT DEPLOYMENT WORKSPACE to get the API URL for the workspace. NOTE: Workspace_URL and API_URL can be different for a workspace but may be the same even for multiple workspaces). You can also use the workspace_url here. cloud String True Cloud provider (Azure/AWS/GCP). primordial_date String True The date from which Overwatch will capture the details. The format should be yyyy-MM-dd ex: 2022-05-20 == May 20 2022. **IMPORTANT NOTE: ** You should only set the primordial date in the initial run of Overwatch, and never change it again, as Overwatch will progress the dates using it\u0026rsquo;s own calculations and checkpoints. storage_prefix String True CASE SENSITIVE - Lower Case The location in which Overwatch will store the data. You can think of this as the Overwatch working directory. dbfs:/mnt/path/\u0026hellip; or abfss://container@myStorageAccount.dfs.core.windows.net/\u0026hellip; or s3://myBucket/\u0026hellip; or gs://myBucket/\u0026hellip; etl_database_name String True The name of the ETL data base for Overwatch (i.e. overwatch_etl or custom) consumer_database_name String True The name of the Consumer database for Overwatch. (i.e. overwatch or custom) secret_scope String True Name of the secret scope. This must be created on the workspace which the Overwatch job will execute. secret_key_dbpat String True This will contain the PAT token of the workspace. The key should be present in the secret_scope and should start with the letters dapi. auditlogprefix_source_path String True For all clouds use keyword system to fetch data from System Tables (system.access.audit) See System Table Configuration Details for details. If you are not using System Tables, you can enter the location of the auditlogs (AWS/GCP Only). The contents under this directory must have the folders with the date partitions like date=2022-12-0 . interactive_dbu_price Double True Contract (or list) Price for interactive DBUs. The provided template has the list prices by default. automated_dbu_price Double True Contract (or list) Price for automated DBUs. The provided template has the list prices by default. sql_compute_dbu_price Double True Contract (or list) Price for DBSQL DBUs. This should be the closest average price across your DBSQL Skus (classic / Pro / Serverless) for now. See Custom Costs for more details. The provided template has the DBSQL Classic list prices by default. jobs_light_dbu_price Double True Contract (or list) Price for interactive DBUs. The provided template has the list prices by default. max_days Integer True This is the max incrementals days that will be loaded. Usually only relevant for historical loading and rebuilds. Recommendation == 30 excluded_scopes String False Scopes that should not be excluded from the pipelines. Since this is a CSV, it\u0026rsquo;s critical that these are colon delimited. Leave blank if you\u0026rsquo;d like to load all overwatch scopes. active Boolean True Whether or not the workspace should be validated / deployed. proxy_host String False Proxy url for the workspace. proxy_port String False Proxy port for the workspace proxy_user_name String False Proxy user name for the workspace. proxy_password_scope String False Scope which contains the proxy password key. proxy_password_key String False Key which contains proxy password. success_batch_size Integer False API Tunable - Indicates the size of the buffer on filling of which the result will be written to a temp location. This is used to tune performance in certain circumstances. Leave default except for special circumstances. Default == 200 error_batch_size Integer False API Tunable - Indicates the size of the error writer buffer containing API call errors. This is used to tune performance in certain circumstances. Leave default except for special circumstances. Default == 500 enable_unsafe_SSL Boolean False API Tunable - Enables unsafe SSL. Default == False thread_pool_size Integer False API Tunable - Max number of API calls Overwatch is allowed to make in parallel. Default == 4. Increase for faster bronze but if workspace is busy, risks API endpoint saturation. Overwatch will detect saturation and back-off when detected but for safety never go over 8 without testing. api_waiting_time Long False API Tunable - Overwatch makes async api calls in parallel, api_waiting_time signifies the max wait time in case of no response received from the api call. Default = 300000(5 minutes) mount_mapping_path String False Path to local CSV holding details of all mounts on remote workspaces (only necessary for remote workspaces with \u0026gt;50 mounts) click here for more details temp_dir_path String False Custom temporary working directory, directory gets cleaned up after each run. sql_endpoint String False Use http path from warehouse connection details. **IMPORTANT NOTE: ** This column only needs to be added for multi account deployment, see Multi Account System Table Integration for more details Azure Event Hub Specific Configurations When configuring the Azure EH configurations users can use EITHER a shared access key OR AAD SP as of 072x to authenticate to the EH. Below are the required configurations for each auth method. One of the options for Azure deployments must be used as EH is required for Azure.\nShared Access Key Requirements Review Authorizing Access Via SAS Policy for more details.\nColumn Type IsRequired Description eh_name String True (AZURE) Event hub name (Azure Only) The event hub will contain the audit logs of the workspace eh_scope_key String True (AZURE) Name of the key in the \u0026lt;secret_scope\u0026gt; that holds the connection string to the Event Hub WITH THE SHARED ACCESS KEY IN IT \u0026ndash; See EH Configuration for details AAD Requirements\nReview Authorizing Access Via AAD SPN for more details.\nEnsure the dependent library for AAD Auth is attached com.microsoft.azure:msal4j:1.10.1\nColumn Type IsRequired Description eh_name String True (AZURE) Event hub name The event hub will contain the audit logs of the workspace eh_conn_string String True (AZURE) Event hub connection string without shared access key. ex: \u0026ldquo;Endpoint=sb://evhub-ns.servicebus.windows.net\u0026rdquo; aad_tenant_id String True (AZURE) Tenant ID for Service principle. aad_client_id String True (AZURE) Client ID for Service principle. aad_client_secret_key String True (AZURE) Name of the key in the \u0026lt;secret_scope\u0026gt; that holds the SPN secret for the Service principle. aad_authority_endpoint String True (AZURE) Endpoint of the authority. Default value is \u0026ldquo;https://login.microsoftonline.com/\u0026quot; Column description 0.7.2.x Column Type IsRequired Description workspace_name String True Name of the workspace. workspace_id String True Id of the workspace. workspace_url String True URL of the workspace. api_url String True API URL for the Workspace (execute in scala dbutils.notebook.getContext().apiUrl.get ON THE TARGET WORKSPACE NOT DEPLOYMENT WORKSPACE to get the API URL for the workspace. NOTE: Workspace_URL and API_URL can be different for a workspace but may be the same even for multiple workspaces). cloud String True Cloud provider (Azure or AWS). primordial_date String True The date from which Overwatch will capture the details. The format should be yyyy-MM-dd ex: 2022-05-20 == May 20 2022 storage_prefix String True The location on which Overwatch will store the data. You can think of this as the Overwatch working directory. dbfs:/mnt/path/\u0026hellip; or abfss://container@myStorageAccount.dfs.core.windows.net/\u0026hellip; or s3://myBucket/\u0026hellip; or gs://myBucket/\u0026hellip; etl_database_name String True The name of the ETL data base for Overwatch (i.e. overwatch_etl or custom) consumer_database_name String True The name of the Consumer database for Overwatch. (i.e. overwatch or custom) secret_scope String True Name of the secret scope. This must be created on the workspace which the Overwatch job will execute. secret_key_dbpat String True This will contain the PAT token of the workspace. The key should be present in the secret_scope and should start with dapi. auditlogprefix_source_path String True (AWS/GCP) Location of auditlog (AWS/GCP Only). The contents under this directory must have the folders with the date partitions like date=2022-12-01 interactive_dbu_price Double True Contract (or list) Price for interactive DBUs. The provided template has the list prices by default. automated_dbu_price Double True Contract (or list) Price for automated DBUs. The provided template has the list prices by default. sql_compute_dbu_price Double True Contract (or list) Price for DBSQL DBUs. This should be the closest average price across your DBSQL Skus (classic / Pro / Serverless) for now. See Custom Costs for more details. The provided template has the DBSQL Classic list prices by default. jobs_light_dbu_price Double True Contract (or list) Price for interactive DBUs. The provided template has the list prices by default. max_days Integer True This is the max incrementals days that will be loaded. Usually only relevant for historical loading and rebuilds. Recommendation == 30 excluded_scopes String False Scopes that should not be excluded from the pipelines. Since this is a CSV, it\u0026rsquo;s critical that these are colon delimited. Leave blank if you\u0026rsquo;d like to load all overwatch scopes. active Boolean True Whether or not the workspace should be validated / deployed. proxy_host String False Proxy url for the workspace. proxy_port String False Proxy port for the workspace proxy_user_name String False Proxy user name for the workspace. proxy_password_scope String False Scope which contains the proxy password key. proxy_password_key String False Key which contains proxy password. success_batch_size Integer False API Tunable - Indicates the size of the buffer on filling of which the result will be written to a temp location. This is used to tune performance in certain circumstances. Leave default except for special circumstances. Default == 200 error_batch_size Integer False API Tunable - Indicates the size of the error writer buffer containing API call errors. This is used to tune performance in certain circumstances. Leave default except for special circumstances. Default == 500 enable_unsafe_SSL Boolean False API Tunable - Enables unsafe SSL. Default == False thread_pool_size Integer False API Tunable - Max number of API calls Overwatch is allowed to make in parallel. Default == 4. Increase for faster bronze but if workspace is busy, risks API endpoint saturation. Overwatch will detect saturation and back-off when detected but for safety never go over 8 without testing. api_waiting_time Long False API Tunable - Overwatch makes async api calls in parallel, api_waiting_time signifies the max wait time in case of no response received from the api call. Default = 300000(5 minutes) mount_mapping_path String False Path to local CSV holding details of all mounts on remote workspaces (only necessary for remote workspaces with \u0026gt;50 mounts) click here for more details temp_dir_path String False Custom temporary working directory, directory gets cleaned up after each run. Azure Event Hub Specific Configurations When configuring the Azure EH configurations users can use EITHER a shared access key OR AAD SP as of 072x to authenticate to the EH. Below are the required configurations for each auth method. One of the options for Azure deployments must be used as EH is required for Azure.\nShared Access Key Requirements\nColumn Type IsRequired Description eh_name String True (AZURE) Event hub name (Azure Only) The event hub will contain the audit logs of the workspace eh_scope_key String True (AZURE) Name of the key in the \u0026lt;secret_scope\u0026gt; that holds the connection string to the Event Hub WITH THE SHARED ACCESS KEY IN IT \u0026ndash; See EH Configuration for details AAD Requirements\nReview Authorizing Access Via AAD SPN for more details.\nEnsure the dependent library for AAD Auth is attached com.microsoft.azure:msal4j:1.10.1\nColumn Type IsRequired Description eh_name String True (AZURE) Event hub name The event hub will contain the audit logs of the workspace eh_conn_string String True (AZURE) Event hub connection string without shared access key. ex: \u0026ldquo;Endpoint=sb://evhub-ns.servicebus.windows.net\u0026rdquo; aad_tenant_id String True (AZURE) Tenant ID for Service principle. aad_client_id String True (AZURE) Client ID for Service principle. aad_client_secret_key String True (AZURE) Client Secret Key for Service principle. aad_authority_endpoint String True (AZURE) Endpoint of the authority. Default value is \u0026ldquo;https://login.microsoftonline.com/\u0026quot; Column description 0.7.1.x Column Type IsRequired Description workspace_name String True Name of the workspace. workspace_id String True Id of the workspace. workspace_url String True URL of the workspace. api_url String True API URL for the Workspace (execute in scala dbutils.notebook.getContext().apiUrl.get ON THE TARGET WORKSPACE NOT DEPLOYMENT WORKSPACE to get the API URL for the workspace. NOTE: Workspace_URL and API_URL can be different for a workspace but may be the same even for multiple workspaces). cloud String True Cloud provider (Azure or AWS). primordial_date String True The date from which Overwatch will capture the details. The format should be yyyy-MM-dd ex: 2022-05-20 == May 20 2022 etl_storage_prefix String True The location on which Overwatch will store the data. You can think of this as the Overwatch working directory. dbfs:/mnt/path/\u0026hellip; or abfss://container@myStorageAccount.dfs.core.windows.net/\u0026hellip; or s3://myBucket/\u0026hellip; or gs://myBucket/\u0026hellip; etl_database_name String True The name of the ETL data base for Overwatch (i.e. overwatch_etl or custom) consumer_database_name String True The name of the Consumer database for Overwatch. (i.e. overwatch or custom) secret_scope String True Name of the secret scope. This must be created on the workspace which the Overwatch job will execute. secret_key_dbpat String True This will contain the PAT token of the workspace. The key should be present in the secret_scope and should start with dapi. auditlogprefix_source_aws String True (AWS/GCP) Location of auditlog (AWS/GCP Only). The contents under this directory must have the folders with the date partitions like date=2022-12-01 eh_name String True (AZURE) Event hub name (Azure Only) The event hub will contain the audit logs of the workspace eh_scope_key String True for NON AAD Connection(AZURE) (Azure Only) Key that holds the connection string to the Event Hub \u0026ndash; See EH Configuration for details interactive_dbu_price Double True Contract (or list) Price for interactive DBUs. The provided template has the list prices by default. automated_dbu_price Double True Contract (or list) Price for automated DBUs. The provided template has the list prices by default. sql_compute_dbu_price Double True Contract (or list) Price for DBSQL DBUs. This should be the closest average price across your DBSQL Skus (classic / Pro / Serverless) for now. See Custom Costs for more details. The provided template has the DBSQL Classic list prices by default. jobs_light_dbu_price Double True Contract (or list) Price for interactive DBUs. The provided template has the list prices by default. max_days Integer True This is the max incrementals days that will be loaded. Usually only relevant for historical loading and rebuilds. Recommendation == 30 excluded_scopes String False Scopes that should not be excluded from the pipelines. Since this is a CSV, it\u0026rsquo;s critical that these are colon delimited. Leave blank if you\u0026rsquo;d like to load all overwatch scopes. active Boolean True Whether or not the workspace should be validated / deployed. proxy_host String False Proxy url for the workspace. proxy_port String False Proxy port for the workspace proxy_user_name String False Proxy user name for the workspace. proxy_password_scope String False Scope which contains the proxy password key. proxy_password_key String False Key which contains proxy password. success_batch_size Integer False API Tunable - Indicates the size of the buffer on filling of which the result will be written to a temp location. This is used to tune performance in certain circumstances. Leave default except for special circumstances. Default == 200 error_batch_size Integer False API Tunable - Indicates the size of the error writer buffer containing API call errors. This is used to tune performance in certain circumstances. Leave default except for special circumstances. Default == 500 enable_unsafe_SSL Boolean False API Tunable - Enables unsafe SSL. Default == False thread_pool_size Integer False API Tunable - Max number of API calls Overwatch is allowed to make in parallel. Default == 4. Increase for faster bronze but if workspace is busy, risks API endpoint saturation. Overwatch will detect saturation and back-off when detected but for safety never go over 8 without testing. api_waiting_time Long False API Tunable - Overwatch makes async api calls in parallel, api_waiting_time signifies the max wait time in case of no response received from the api call. Default = 300000(5 minutes) mount_mapping_path String False Path to local CSV holding details of all mounts on remote workspaces (only necessary for remote workspaces with \u0026gt;50 mounts) click here for more details temp_dir_path String False Custom temporary working directory, directory gets cleaned up after each run. "
},
{
	"uri": "http://localhost:1313/overwatch/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]